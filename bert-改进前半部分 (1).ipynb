{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Administrator\\\\Desktop\\\\数据挖掘\\\\数据挖掘比赛\\\\ccf-隐私-deepshare\\\\ch5'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 信息和标签增强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import jieba\n",
    "import jieba.posseg as pseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_line(data_path):\n",
    "    with open(data_path,'r',encoding=\"utf-8\") as f:\n",
    "        line = ''.join(f.readlines())\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_label(label_path):\n",
    "    return pd.read_csv(label_path,encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = get_line('./data/train_data/0.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = get_df_label('./data/train_label/0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'艺术是相同的，音乐美术体育三样都是艺术。，三样艺术都是靠感觉的。感觉好玩起来就很轻松，所以叫做玩艺术。没感觉找不到北的干脆别玩了！，香港电影国语配音名家周思平，代表作有TVB《上海滩》周润发等'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>Pos_b</th>\n",
       "      <th>Pos_e</th>\n",
       "      <th>Privacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>position</td>\n",
       "      <td>66</td>\n",
       "      <td>75</td>\n",
       "      <td>香港电影国语配音名家</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>name</td>\n",
       "      <td>76</td>\n",
       "      <td>78</td>\n",
       "      <td>周思平</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>movie</td>\n",
       "      <td>87</td>\n",
       "      <td>91</td>\n",
       "      <td>《上海滩》</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>name</td>\n",
       "      <td>92</td>\n",
       "      <td>94</td>\n",
       "      <td>周润发</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>organization</td>\n",
       "      <td>84</td>\n",
       "      <td>86</td>\n",
       "      <td>TVB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID      Category  Pos_b  Pos_e     Privacy\n",
       "0   0      position     66     75  香港电影国语配音名家\n",
       "1   0          name     76     78         周思平\n",
       "2   0         movie     87     91       《上海滩》\n",
       "3   0          name     92     94         周润发\n",
       "4   0  organization     84     86         TVB"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=pseg.cut(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "艺术 n\n",
      "是 v\n",
      "相同 d\n",
      "的 uj\n",
      "， x\n",
      "音乐 n\n",
      "美术 n\n",
      "体育 vn\n",
      "三样 m\n",
      "都 d\n",
      "是 v\n",
      "艺术 n\n",
      "。 x\n",
      "， x\n",
      "三样 m\n",
      "艺术 n\n",
      "都 d\n",
      "是 v\n",
      "靠 v\n",
      "感觉 n\n",
      "的 uj\n",
      "。 x\n",
      "感觉 n\n",
      "好玩 v\n",
      "起来 v\n",
      "就 d\n",
      "很 d\n",
      "轻松 a\n",
      "， x\n",
      "所以 c\n",
      "叫做 v\n",
      "玩 v\n",
      "艺术 n\n",
      "。 x\n",
      "没 v\n",
      "感觉 n\n",
      "找 v\n",
      "不到 v\n",
      "北 f\n",
      "的 uj\n",
      "干脆 d\n",
      "别 d\n",
      "玩 v\n",
      "了 ul\n",
      "！ x\n",
      "， x\n",
      "香港电影 n\n",
      "国语 nz\n",
      "配音 n\n",
      "名家 n\n",
      "周思平 nr\n",
      "， x\n",
      "代表作 n\n",
      "有 v\n",
      "TVB eng\n",
      "《 x\n",
      "上海滩 ns\n",
      "》 x\n",
      "周润发 nr\n",
      "等 u\n"
     ]
    }
   ],
   "source": [
    "for word, flag in words:\n",
    "    print(word, flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_label(label_path):\n",
    "    return pd.read_csv(label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>Pos_b</th>\n",
       "      <th>Pos_e</th>\n",
       "      <th>Privacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>position</td>\n",
       "      <td>66</td>\n",
       "      <td>75</td>\n",
       "      <td>香港电影国语配音名家</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>name</td>\n",
       "      <td>76</td>\n",
       "      <td>78</td>\n",
       "      <td>周思平</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>movie</td>\n",
       "      <td>87</td>\n",
       "      <td>91</td>\n",
       "      <td>《上海滩》</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>name</td>\n",
       "      <td>92</td>\n",
       "      <td>94</td>\n",
       "      <td>周润发</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>organization</td>\n",
       "      <td>84</td>\n",
       "      <td>86</td>\n",
       "      <td>TVB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID      Category  Pos_b  Pos_e     Privacy\n",
       "0   0      position     66     75  香港电影国语配音名家\n",
       "1   0          name     76     78         周思平\n",
       "2   0         movie     87     91       《上海滩》\n",
       "3   0          name     92     94         周润发\n",
       "4   0  organization     84     86         TVB"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label=get_df_label(\"./data/train_label/0.csv\")\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将数据分别打标成 3种标注体系 BIES BI IE\n",
    "def get_list_data_ner(data_path):\n",
    "    with open(data_path,'r',encoding=\"utf-8\") as f:\n",
    "        line = ''.join(f.readlines())\n",
    "        words = pseg.cut(line)\n",
    "        list_data = []\n",
    "        list_ner = []\n",
    "        list_BIES = []\n",
    "        list_BI = []\n",
    "        list_IE = []\n",
    "        for word, flag in words:\n",
    "            list_word = list(word)\n",
    "            len_word = len(list_word)\n",
    "            list_data += list_word\n",
    "            list_ner += [flag] * len_word\n",
    "            if len_word == 1:\n",
    "                list_BIES += ['S']\n",
    "                list_BI += ['B']\n",
    "                list_IE += ['E']\n",
    "            elif len_word == 2:\n",
    "                list_BIES += ['B','E']\n",
    "                list_BI += ['B','I']\n",
    "                list_IE += ['I','E']\n",
    "            else:\n",
    "                list_BIES += ['B'] + ['I'] * (len_word-2) + ['E']\n",
    "                list_BI += ['B'] + ['I'] * (len_word-1)\n",
    "                list_IE += ['I'] * (len_word-1) + ['E']\n",
    "                list_data = list(line)\n",
    "    list_data_info = list_data,list_ner,list_BIES,list_BI,list_IE\n",
    "    return list_data_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "艺 n B B I\n",
      "术 n E I E\n"
     ]
    }
   ],
   "source": [
    "list_data_info=get_list_data_ner('./data/train_data/0.txt')\n",
    "print(list_data_info[0][0],list_data_info[1][0],list_data_info[2][0],list_data_info[3][0],list_data_info[4][0])\n",
    "print(list_data_info[0][1],list_data_info[1][1],list_data_info[2][1],list_data_info[3][1],list_data_info[4][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将3种标注体系 BIES BI IE的数据标签和实体标签进行结合，拼接\n",
    "def get_label_ner(list_data_info, df_label):\n",
    "    list_data,list_ner,list_BIES,list_BI,list_IE = list_data_info\n",
    "    list_BIOES_label = ['label_O'] * len(list_data)\n",
    "    list_BIO_label = ['label_O'] * len(list_data)\n",
    "    list_IEO_label = ['label_O'] * len(list_data)\n",
    "    list_IO_label = ['label_O'] * len(list_data)\n",
    "    for index,d in df_label.iterrows():\n",
    "        ID = d['ID']\n",
    "        Category = d['Category']\n",
    "        Pos_b = d['Pos_b']\n",
    "        Pos_e = d['Pos_e']\n",
    "        \n",
    "        # 异常数据处理\n",
    "        if 2162 == ID:\n",
    "            Category == '前明骏女孩组合队长\"'\n",
    "            Pos_e = Pos_e - 1\n",
    "\n",
    "        if Pos_b == Pos_e:\n",
    "            list_BIOES_label[Pos_b] = 'label_S_' + Category\n",
    "            \n",
    "            list_BIO_label[Pos_b] = 'label_B_' + Category\n",
    "            \n",
    "            list_IEO_label[Pos_b] = 'label_I_' + Category\n",
    "            \n",
    "            list_IO_label[Pos_b] = 'label_I_' + Category\n",
    "        elif Pos_e - Pos_b == 1:\n",
    "            list_BIOES_label[Pos_b] = 'label_B_' + Category\n",
    "            list_BIOES_label[Pos_e] = 'label_E_' + Category\n",
    "            \n",
    "            list_BIO_label[Pos_b] = 'label_B_' + Category\n",
    "            list_BIO_label[Pos_e] = 'label_I_' + Category\n",
    "            \n",
    "            list_IEO_label[Pos_b] = 'label_I_' + Category\n",
    "            list_IEO_label[Pos_e] = 'label_E_' + Category\n",
    "            \n",
    "            list_IO_label[Pos_b] = 'label_I_' + Category\n",
    "            list_IO_label[Pos_e] = 'label_I_' + Category   \n",
    "        else:\n",
    "            # 异常数据处理\n",
    "            try:\n",
    "                list_BIOES_label[Pos_b] = 'label_B_' + Category\n",
    "                list_BIOES_label[Pos_e] = 'label_E_' + Category \n",
    "                for pos_i in range(Pos_b+1,Pos_e):\n",
    "                    list_BIOES_label[pos_i] = 'label_I_' + Category \n",
    "                    \n",
    "                list_BIO_label[Pos_b] = 'label_B_' + Category\n",
    "                for pos_i in range(Pos_b+1,Pos_e+1):\n",
    "                    list_BIO_label[pos_i] = 'label_I_' + Category \n",
    "                    \n",
    "                list_IEO_label[Pos_e] = 'label_E_' + Category \n",
    "                for pos_i in range(Pos_b,Pos_e):\n",
    "                    list_IEO_label[pos_i] = 'label_I_' + Category \n",
    "                    \n",
    "                for pos_i in range(Pos_b,Pos_e+1):\n",
    "                    list_IO_label[pos_i] = 'label_I_' + Category \n",
    "            except:\n",
    "                print(ID)\n",
    "    \n",
    "    data_label_info = [(ID,data,label_BIOES,label_BIO,label_IEO,label_IO,\\\n",
    "                           data_ner,data_BIES,data_BI,data_IE) for data,label_BIOES,label_BIO,label_IEO,label_IO,\\\n",
    "                           data_ner,data_BIES,data_BI,data_IE in zip(list_data,\n",
    "                                                                     list_BIOES_label,\n",
    "                                                                     list_BIO_label,\n",
    "                                                                     list_IEO_label,\n",
    "                                                                     list_IO_label,\n",
    "                                                                     list_ner,\n",
    "                                                                     list_BIES,\n",
    "                                                                     list_BI,\n",
    "                                                                     list_IE)]\n",
    "    return data_label_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0, '艺', 'label_O', 'label_O', 'label_O', 'label_O', 'n', 'B', 'B', 'I'),\n",
       " (0,\n",
       "  '《',\n",
       "  'label_B_movie',\n",
       "  'label_B_movie',\n",
       "  'label_I_movie',\n",
       "  'label_I_movie',\n",
       "  'x',\n",
       "  'S',\n",
       "  'B',\n",
       "  'E'))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_label_info=get_label_ner(list_data_info,label)\n",
    "data_label_info[0],data_label_info[87]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_label(path_dir_data,path_dir_label):\n",
    "    \n",
    "    # 训练数据集\n",
    "    list_data_label_info = []\n",
    "    # 获取训练集 ID\n",
    "    list_data_ID = [file_name.split('.')[0] for file_name in os.listdir(path_dir_data) if '.txt' in file_name]\n",
    "    list_label_ID = [file_name.split('.')[0] for file_name in os.listdir(path_dir_label) if '.csv' in file_name]\n",
    "    \n",
    "    # 循环训练集 ID\n",
    "    for ID in tqdm(set(list_data_ID) & set(list_label_ID)):\n",
    "        # 获取词和标签地址\n",
    "        data_path = os.path.join(path_dir_data,ID+'.txt')\n",
    "        label_path = os.path.join(path_dir_label,ID+'.csv')\n",
    "        \n",
    "        # 获取词和标签数据\n",
    "        list_data_info = get_list_data_ner(data_path)\n",
    "        df_label = get_df_label(label_path)\n",
    "        \n",
    "        # BIOES 进行标注\n",
    "        data_label_info = get_label_ner(list_data_info, df_label)\n",
    "        \n",
    "        # 加入训练数据集\n",
    "        list_data_label_info += [data_label_info]\n",
    "        \n",
    "    return list_data_label_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dir_data = './data/train_data/'\n",
    "path_dir_label = './data/train_label/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 13262/13262 [04:39<00:00, 47.50it/s]\n"
     ]
    }
   ],
   "source": [
    "list_data_label_info = get_data_label(path_dir_data,path_dir_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3355, '他', 'label_O', 'label_O', 'label_O', 'label_O', 'r', 'S', 'B', 'E'),\n",
       " (3355, '告', 'label_O', 'label_O', 'label_O', 'label_O', 'v', 'B', 'B', 'I'),\n",
       " (3355, '诉', 'label_O', 'label_O', 'label_O', 'label_O', 'v', 'E', 'I', 'E'),\n",
       " (3355,\n",
       "  '记',\n",
       "  'label_B_position',\n",
       "  'label_B_position',\n",
       "  'label_I_position',\n",
       "  'label_I_position',\n",
       "  'n',\n",
       "  'B',\n",
       "  'B',\n",
       "  'I'),\n",
       " (3355,\n",
       "  '者',\n",
       "  'label_E_position',\n",
       "  'label_I_position',\n",
       "  'label_E_position',\n",
       "  'label_I_position',\n",
       "  'n',\n",
       "  'E',\n",
       "  'I',\n",
       "  'E'),\n",
       " (3355, '，', 'label_O', 'label_O', 'label_O', 'label_O', 'x', 'S', 'B', 'E'),\n",
       " (3355, '自', 'label_O', 'label_O', 'label_O', 'label_O', 'r', 'B', 'B', 'I'),\n",
       " (3355, '己', 'label_O', 'label_O', 'label_O', 'label_O', 'r', 'E', 'I', 'E'),\n",
       " (3355, '在', 'label_O', 'label_O', 'label_O', 'label_O', 'p', 'S', 'B', 'E'),\n",
       " (3355,\n",
       "  '中',\n",
       "  'label_B_address',\n",
       "  'label_B_address',\n",
       "  'label_I_address',\n",
       "  'label_I_address',\n",
       "  'ns',\n",
       "  'B',\n",
       "  'B',\n",
       "  'I'),\n",
       " (3355,\n",
       "  '山',\n",
       "  'label_I_address',\n",
       "  'label_I_address',\n",
       "  'label_I_address',\n",
       "  'label_I_address',\n",
       "  'ns',\n",
       "  'I',\n",
       "  'I',\n",
       "  'I'),\n",
       " (3355,\n",
       "  '公',\n",
       "  'label_I_address',\n",
       "  'label_I_address',\n",
       "  'label_I_address',\n",
       "  'label_I_address',\n",
       "  'ns',\n",
       "  'I',\n",
       "  'I',\n",
       "  'I'),\n",
       " (3355,\n",
       "  '园',\n",
       "  'label_E_address',\n",
       "  'label_I_address',\n",
       "  'label_E_address',\n",
       "  'label_I_address',\n",
       "  'ns',\n",
       "  'E',\n",
       "  'I',\n",
       "  'E'),\n",
       " (3355, '上', 'label_O', 'label_O', 'label_O', 'label_O', 'v', 'B', 'B', 'I'),\n",
       " (3355, '班', 'label_O', 'label_O', 'label_O', 'label_O', 'v', 'E', 'I', 'E'),\n",
       " (3355, '，', 'label_O', 'label_O', 'label_O', 'label_O', 'x', 'S', 'B', 'E'),\n",
       " (3355, '特', 'label_O', 'label_O', 'label_O', 'label_O', 'd', 'B', 'B', 'I'),\n",
       " (3355, '意', 'label_O', 'label_O', 'label_O', 'label_O', 'd', 'E', 'I', 'E'),\n",
       " (3355, '赶', 'label_O', 'label_O', 'label_O', 'label_O', 'v', 'S', 'B', 'E'),\n",
       " (3355, '在', 'label_O', 'label_O', 'label_O', 'label_O', 'p', 'S', 'B', 'E'),\n",
       " (3355, '上', 'label_O', 'label_O', 'label_O', 'label_O', 'v', 'B', 'B', 'I'),\n",
       " (3355, '班', 'label_O', 'label_O', 'label_O', 'label_O', 'v', 'E', 'I', 'E'),\n",
       " (3355, '前', 'label_O', 'label_O', 'label_O', 'label_O', 'v', 'B', 'B', 'I'),\n",
       " (3355, '到', 'label_O', 'label_O', 'label_O', 'label_O', 'v', 'E', 'I', 'E'),\n",
       " (3355, '银', 'label_O', 'label_O', 'label_O', 'label_O', 'n', 'B', 'B', 'I'),\n",
       " (3355, '行', 'label_O', 'label_O', 'label_O', 'label_O', 'n', 'E', 'I', 'E'),\n",
       " (3355, '来', 'label_O', 'label_O', 'label_O', 'label_O', 'v', 'S', 'B', 'E'),\n",
       " (3355, '办', 'label_O', 'label_O', 'label_O', 'label_O', 'n', 'B', 'B', 'I'),\n",
       " (3355, '理', 'label_O', 'label_O', 'label_O', 'label_O', 'n', 'E', 'I', 'E'),\n",
       " (3355, '放', 'label_O', 'label_O', 'label_O', 'label_O', 'v', 'B', 'B', 'I'),\n",
       " (3355, '贷', 'label_O', 'label_O', 'label_O', 'label_O', 'v', 'E', 'I', 'E'),\n",
       " (3355, '业', 'label_O', 'label_O', 'label_O', 'label_O', 'n', 'B', 'B', 'I'),\n",
       " (3355, '务', 'label_O', 'label_O', 'label_O', 'label_O', 'n', 'E', 'I', 'E'),\n",
       " (3355, '。', 'label_O', 'label_O', 'label_O', 'label_O', 'x', 'S', 'B', 'E'),\n",
       " (3355, '说', 'label_O', 'label_O', 'label_O', 'label_O', 'v', 'B', 'B', 'I'),\n",
       " (3355, '起', 'label_O', 'label_O', 'label_O', 'label_O', 'v', 'E', 'I', 'E'),\n",
       " (3355, '七', 'label_O', 'label_O', 'label_O', 'label_O', 'm', 'B', 'B', 'I'),\n",
       " (3355, '折', 'label_O', 'label_O', 'label_O', 'label_O', 'm', 'E', 'I', 'E'),\n",
       " (3355, '优', 'label_O', 'label_O', 'label_O', 'label_O', 'vn', 'B', 'B', 'I'),\n",
       " (3355, '惠', 'label_O', 'label_O', 'label_O', 'label_O', 'vn', 'E', 'I', 'E'),\n",
       " (3355, '，', 'label_O', 'label_O', 'label_O', 'label_O', 'x', 'S', 'B', 'E'),\n",
       " (3355, '王', 'label_O', 'label_O', 'label_O', 'label_O', 'nr', 'B', 'B', 'I'),\n",
       " (3355, '先', 'label_O', 'label_O', 'label_O', 'label_O', 'nr', 'I', 'I', 'I'),\n",
       " (3355, '生', 'label_O', 'label_O', 'label_O', 'label_O', 'nr', 'E', 'I', 'E'),\n",
       " (3355, '一', 'label_O', 'label_O', 'label_O', 'label_O', 'm', 'B', 'B', 'I'),\n",
       " (3355, '肚', 'label_O', 'label_O', 'label_O', 'label_O', 'm', 'I', 'I', 'I'),\n",
       " (3355, '子', 'label_O', 'label_O', 'label_O', 'label_O', 'm', 'E', 'I', 'E'),\n",
       " (3355, '苦', 'label_O', 'label_O', 'label_O', 'label_O', 'n', 'B', 'B', 'I'),\n",
       " (3355, '水', 'label_O', 'label_O', 'label_O', 'label_O', 'n', 'E', 'I', 'E'),\n",
       " (3355, '：', 'label_O', 'label_O', 'label_O', 'label_O', 'x', 'S', 'B', 'E')]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_data_label_info[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 增强数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(path_dir_data):\n",
    "    # 获取的数据\n",
    "    list_ID = []\n",
    "    list_data = []\n",
    "    \n",
    "    # 获取ID\n",
    "    list_data_ID = [file_name.split('.')[0] for file_name in os.listdir(path_dir_data) if '.txt' in file_name]\n",
    "    # 循环ID\n",
    "    for ID in tqdm(set(list_data_ID)):\n",
    "        # 获取词\n",
    "        data_path = os.path.join(path_dir_data,ID+'.txt')\n",
    "        \n",
    "        with open(data_path,'r',encoding=\"utf-8\") as f:\n",
    "            line = ''.join(f.readlines())\n",
    "        # 获取词\n",
    "        list_ID += [ID]\n",
    "        list_data += [line]\n",
    "    # dataframe\n",
    "    df_data = pd.DataFrame()\n",
    "    df_data['ID'] = list_ID\n",
    "    df_data['data'] = list_data\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dir_data_train = './data/train_data/'\n",
    "path_dir_data_test = './data/test_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 13262/13262 [00:01<00:00, 7160.38it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train = get_dataframe(path_dir_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 3956/3956 [00:26<00:00, 151.22it/s]\n"
     ]
    }
   ],
   "source": [
    "df_test = get_dataframe(path_dir_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3355</td>\n",
       "      <td>他告诉记者，自己在中山公园上班，特意赶在上班前到银行来办理放贷业务。说起七折优惠，王先生一肚...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2835</td>\n",
       "      <td>大师”，知名星际主持人，常年解说SPL，MSL，OSL，PLU举办的比赛，</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4641</td>\n",
       "      <td>均为正正负形态，切沃vs罗马一场客胜盈亏最低，赢球概率最大，不过目前来看，3场都可以作博胆。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1451</td>\n",
       "      <td>对两强1胜1平使奥尼尔尝到了甜头，也不再一味坚持他用惯了的433，正是上个月2：1主场力克皇...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6649</td>\n",
       "      <td>京开高速方便畅达到达金融街、三里河、丽泽商务区；。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13257</th>\n",
       "      <td>5505</td>\n",
       "      <td>最好建议小雨本人去长沙就近的派出所报警。公安回应已开展工作29日，@南昌铁路公安局回应</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13258</th>\n",
       "      <td>6892</td>\n",
       "      <td>数据显示，昨天上海黄金交易所黄金au99.95、黄金au99.99、黄金au100g、黄金a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13259</th>\n",
       "      <td>7125</td>\n",
       "      <td>日前，静安法院一审判决黄春败诉。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13260</th>\n",
       "      <td>8184</td>\n",
       "      <td>saidintibazonkiza，前锋，布隆迪）、比科林（jhonnyvan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13261</th>\n",
       "      <td>4140</td>\n",
       "      <td>2009年北京楼市进入平稳发展阶段，多个传统热点板块如cbd、百子湾、朝阳公园、中关村、</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13262 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                               data\n",
       "0      3355  他告诉记者，自己在中山公园上班，特意赶在上班前到银行来办理放贷业务。说起七折优惠，王先生一肚...\n",
       "1      2835              大师”，知名星际主持人，常年解说SPL，MSL，OSL，PLU举办的比赛，\n",
       "2      4641     均为正正负形态，切沃vs罗马一场客胜盈亏最低，赢球概率最大，不过目前来看，3场都可以作博胆。\n",
       "3      1451  对两强1胜1平使奥尼尔尝到了甜头，也不再一味坚持他用惯了的433，正是上个月2：1主场力克皇...\n",
       "4      6649                          京开高速方便畅达到达金融街、三里河、丽泽商务区；。\n",
       "...     ...                                                ...\n",
       "13257  5505        最好建议小雨本人去长沙就近的派出所报警。公安回应已开展工作29日，@南昌铁路公安局回应\n",
       "13258  6892  数据显示，昨天上海黄金交易所黄金au99.95、黄金au99.99、黄金au100g、黄金a...\n",
       "13259  7125                                   日前，静安法院一审判决黄春败诉。\n",
       "13260  8184            saidintibazonkiza，前锋，布隆迪）、比科林（jhonnyvan\n",
       "13261  4140       2009年北京楼市进入平稳发展阶段，多个传统热点板块如cbd、百子湾、朝阳公园、中关村、\n",
       "\n",
       "[13262 rows x 2 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1288</td>\n",
       "      <td>中国陶瓷艺术大师，广东省工艺美术协会会员，石湾公仔协会副会长，鸿之陶美术工艺厂负责人。工作事...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3355</td>\n",
       "      <td>标语书法家、车窗摄影家、低级政治家、无股资本家、搭车旅行家、茅庐建筑师、乡村规划师、贫农园艺...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>242</td>\n",
       "      <td>韩国文化部突然将星际争霸2化为18禁游戏。具体原因：要知道目前凯尔特人还保持着欧冠赛场最长的...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3184</td>\n",
       "      <td>香港动漫画联会会长、漫画家。成名作品&lt;龙虎门&gt;&lt;天子传奇&gt;＜神兵玄奇＞，总销量超过四亿本。动...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1568</td>\n",
       "      <td>《21世纪》：近期，您指出，市场风险、房地产市场风险和信用风险是宏观监管需要关注的重要方面。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3951</th>\n",
       "      <td>2756</td>\n",
       "      <td>无缘大慈同体大悲。愿我们所有的人都能生慈悲心断恶修善吃素放生念佛忏悔愿大家诸事圆满。阿弥陀佛...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3952</th>\n",
       "      <td>573</td>\n",
       "      <td>你是秋天里的风，我却是一片叶。当你来到我的身边，我跟随着你。而你不会停留，吹向远方，一直向前...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3953</th>\n",
       "      <td>3408</td>\n",
       "      <td>风神A60，大气浑厚外观，MR系列2.0排量发动机与CVT无极变速器，2700mm超长轴距与...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3954</th>\n",
       "      <td>2009</td>\n",
       "      <td>两支排名靠后的球队交手，更看好主场作战的马洛卡，毕竟主队连续两个赛季的主场都击败了维尔瓦，k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3955</th>\n",
       "      <td>2064</td>\n",
       "      <td>根据王女士提供的电话，记者联系到该公司“张经理”，他表示，“无论是谁，只要本人携带身份证，昨...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3956 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID                                               data\n",
       "0     1288  中国陶瓷艺术大师，广东省工艺美术协会会员，石湾公仔协会副会长，鸿之陶美术工艺厂负责人。工作事...\n",
       "1     3355  标语书法家、车窗摄影家、低级政治家、无股资本家、搭车旅行家、茅庐建筑师、乡村规划师、贫农园艺...\n",
       "2      242  韩国文化部突然将星际争霸2化为18禁游戏。具体原因：要知道目前凯尔特人还保持着欧冠赛场最长的...\n",
       "3     3184  香港动漫画联会会长、漫画家。成名作品<龙虎门><天子传奇>＜神兵玄奇＞，总销量超过四亿本。动...\n",
       "4     1568     《21世纪》：近期，您指出，市场风险、房地产市场风险和信用风险是宏观监管需要关注的重要方面。\n",
       "...    ...                                                ...\n",
       "3951  2756  无缘大慈同体大悲。愿我们所有的人都能生慈悲心断恶修善吃素放生念佛忏悔愿大家诸事圆满。阿弥陀佛...\n",
       "3952   573  你是秋天里的风，我却是一片叶。当你来到我的身边，我跟随着你。而你不会停留，吹向远方，一直向前...\n",
       "3953  3408  风神A60，大气浑厚外观，MR系列2.0排量发动机与CVT无极变速器，2700mm超长轴距与...\n",
       "3954  2009  两支排名靠后的球队交手，更看好主场作战的马洛卡，毕竟主队连续两个赛季的主场都击败了维尔瓦，k...\n",
       "3955  2064  根据王女士提供的电话，记者联系到该公司“张经理”，他表示，“无论是谁，只要本人携带身份证，昨...\n",
       "\n",
       "[3956 rows x 2 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(path_dir_data):\n",
    "    # 获取的数据\n",
    "    list_df_label = []\n",
    "    \n",
    "    # 获取ID\n",
    "    list_label_ID = [file_name.split('.')[0] for file_name in os.listdir(path_dir_label) if '.csv' in file_name]\n",
    "    # 循环ID\n",
    "    for ID in tqdm(set(list_label_ID)):\n",
    "        \n",
    "        # 获取标签\n",
    "        label_path = os.path.join(path_dir_label,ID+'.csv')\n",
    "        \n",
    "        df_label = get_df_label(label_path)\n",
    "        \n",
    "        list_df_label += [df_label]\n",
    "        \n",
    "\n",
    "    # dataframe\n",
    "    df_label = pd.concat(list_df_label)\n",
    "    return df_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 13262/13262 [00:24<00:00, 538.97it/s]\n"
     ]
    }
   ],
   "source": [
    "df_label_train = get_label('./data/train_label/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>Pos_b</th>\n",
       "      <th>Pos_e</th>\n",
       "      <th>Privacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3355</td>\n",
       "      <td>address</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>中山公园</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3355</td>\n",
       "      <td>position</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>记者</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2835</td>\n",
       "      <td>game</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>星际</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2835</td>\n",
       "      <td>organization</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>SPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2835</td>\n",
       "      <td>position</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>主持人</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID      Category  Pos_b  Pos_e Privacy\n",
       "0  3355       address      9     12    中山公园\n",
       "1  3355      position      3      4      记者\n",
       "0  2835          game      6      7      星际\n",
       "1  2835  organization     16     18     SPL\n",
       "2  2835      position      8     10     主持人"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_label_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'QQ',\n",
       " 'address',\n",
       " 'book',\n",
       " 'company',\n",
       " 'email',\n",
       " 'game',\n",
       " 'government',\n",
       " 'mobile',\n",
       " 'movie',\n",
       " 'name',\n",
       " 'organization',\n",
       " 'position',\n",
       " 'scene',\n",
       " 'vx'}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df_label_train.Category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Privacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>QQ</td>\n",
       "      <td>1827967769 18618193311 371451138 783594553 533...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>address</td>\n",
       "      <td>中山公园 金融街 台北 oldlauncestonseaport 越南 堪萨斯城 摩天大厦 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>book</td>\n",
       "      <td>《老子·五十八章》 《红楼梦》 《梅冈城故事》 抗日之铁血兵王 晨报 乾隆御制诗 《故城时光...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>company</td>\n",
       "      <td>光大银行深圳分行信用卡中心 中国银联 中资银行 航天科技中心 EA 中华油脂网 金鹰网 中行...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>email</td>\n",
       "      <td>yinentertainment@gmail.com guanshaozeng007@163...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>game</td>\n",
       "      <td>星际 星际争霸 《魔兽世界》 《古墓丽影：传奇》 WAR3 DotA 魔兽争霸3 StarC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>government</td>\n",
       "      <td>石家庄市政府 马达加斯加政府 最高法 美国海军陆战队 国务院 美国海军 俄空军 广州国土房管...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mobile</td>\n",
       "      <td>+85223352105 18911738839 886926929796 +8526687...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>movie</td>\n",
       "      <td>《好家伙坏家伙怪家伙》 30DaysofNight 《射雕英雄传》 《姨妈的后现代生活》 《...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>name</td>\n",
       "      <td>奥尼尔 老子 董玉辉 易纲 奈史密斯 郭清保 史蒂夫奈尔斯 翁美玲 CCM. Grigoro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>organization</td>\n",
       "      <td>SPL 切沃 拉涅利 皇马 米兰 阿尔梅 ac米兰 意甲 桑普多利亚 伦敦德比 WCG 纽卡...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>position</td>\n",
       "      <td>记者 主持人 副秘书长 市长 主编 漫画家 选手 社长 副总经理 飞行员 导演 画家 记者 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>scene</td>\n",
       "      <td>苏州博物馆 太空体验基地 瓜岛 西本愿寺 故宫 泸沽湖 维多利亚花园 阿里山森铁 太湖 念青...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>vx</td>\n",
       "      <td>1827967769 waveapp songxiaobo montager rachel-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Category                                            Privacy\n",
       "0             QQ  1827967769 18618193311 371451138 783594553 533...\n",
       "1        address  中山公园 金融街 台北 oldlauncestonseaport 越南 堪萨斯城 摩天大厦 ...\n",
       "2           book  《老子·五十八章》 《红楼梦》 《梅冈城故事》 抗日之铁血兵王 晨报 乾隆御制诗 《故城时光...\n",
       "3        company  光大银行深圳分行信用卡中心 中国银联 中资银行 航天科技中心 EA 中华油脂网 金鹰网 中行...\n",
       "4          email  yinentertainment@gmail.com guanshaozeng007@163...\n",
       "5           game  星际 星际争霸 《魔兽世界》 《古墓丽影：传奇》 WAR3 DotA 魔兽争霸3 StarC...\n",
       "6     government  石家庄市政府 马达加斯加政府 最高法 美国海军陆战队 国务院 美国海军 俄空军 广州国土房管...\n",
       "7         mobile  +85223352105 18911738839 886926929796 +8526687...\n",
       "8          movie  《好家伙坏家伙怪家伙》 30DaysofNight 《射雕英雄传》 《姨妈的后现代生活》 《...\n",
       "9           name  奥尼尔 老子 董玉辉 易纲 奈史密斯 郭清保 史蒂夫奈尔斯 翁美玲 CCM. Grigoro...\n",
       "10  organization  SPL 切沃 拉涅利 皇马 米兰 阿尔梅 ac米兰 意甲 桑普多利亚 伦敦德比 WCG 纽卡...\n",
       "11      position  记者 主持人 副秘书长 市长 主编 漫画家 选手 社长 副总经理 飞行员 导演 画家 记者 ...\n",
       "12         scene  苏州博物馆 太空体验基地 瓜岛 西本愿寺 故宫 泸沽湖 维多利亚花园 阿里山森铁 太湖 念青...\n",
       "13            vx  1827967769 waveapp songxiaobo montager rachel-..."
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_label_train_Category = df_label_train.groupby(['Category'])['Privacy'].agg(lambda x: ' '.join([str(i) for i in list(x)])).reset_index()\n",
    "df_label_train_Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label_train_Category['Category_num'] = df_label_train_Category['Privacy'].map(lambda x:len(x.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Privacy</th>\n",
       "      <th>Category_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>vx</td>\n",
       "      <td>1827967769 waveapp songxiaobo montager rachel-...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>QQ</td>\n",
       "      <td>1827967769 18618193311 371451138 783594553 533...</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>email</td>\n",
       "      <td>yinentertainment@gmail.com guanshaozeng007@163...</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mobile</td>\n",
       "      <td>+85223352105 18911738839 886926929796 +8526687...</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>book</td>\n",
       "      <td>《老子·五十八章》 《红楼梦》 《梅冈城故事》 抗日之铁血兵王 晨报 乾隆御制诗 《故城时光...</td>\n",
       "      <td>1642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>scene</td>\n",
       "      <td>苏州博物馆 太空体验基地 瓜岛 西本愿寺 故宫 泸沽湖 维多利亚花园 阿里山森铁 太湖 念青...</td>\n",
       "      <td>1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>government</td>\n",
       "      <td>石家庄市政府 马达加斯加政府 最高法 美国海军陆战队 国务院 美国海军 俄空军 广州国土房管...</td>\n",
       "      <td>2648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>movie</td>\n",
       "      <td>《好家伙坏家伙怪家伙》 30DaysofNight 《射雕英雄传》 《姨妈的后现代生活》 《...</td>\n",
       "      <td>2680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>game</td>\n",
       "      <td>星际 星际争霸 《魔兽世界》 《古墓丽影：传奇》 WAR3 DotA 魔兽争霸3 StarC...</td>\n",
       "      <td>3468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>address</td>\n",
       "      <td>中山公园 金融街 台北 oldlauncestonseaport 越南 堪萨斯城 摩天大厦 ...</td>\n",
       "      <td>4146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>organization</td>\n",
       "      <td>SPL 切沃 拉涅利 皇马 米兰 阿尔梅 ac米兰 意甲 桑普多利亚 伦敦德比 WCG 纽卡...</td>\n",
       "      <td>4223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>company</td>\n",
       "      <td>光大银行深圳分行信用卡中心 中国银联 中资银行 航天科技中心 EA 中华油脂网 金鹰网 中行...</td>\n",
       "      <td>4365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>position</td>\n",
       "      <td>记者 主持人 副秘书长 市长 主编 漫画家 选手 社长 副总经理 飞行员 导演 画家 记者 ...</td>\n",
       "      <td>6043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>name</td>\n",
       "      <td>奥尼尔 老子 董玉辉 易纲 奈史密斯 郭清保 史蒂夫奈尔斯 翁美玲 CCM. Grigoro...</td>\n",
       "      <td>6269</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Category                                            Privacy  \\\n",
       "13            vx  1827967769 waveapp songxiaobo montager rachel-...   \n",
       "0             QQ  1827967769 18618193311 371451138 783594553 533...   \n",
       "4          email  yinentertainment@gmail.com guanshaozeng007@163...   \n",
       "7         mobile  +85223352105 18911738839 886926929796 +8526687...   \n",
       "2           book  《老子·五十八章》 《红楼梦》 《梅冈城故事》 抗日之铁血兵王 晨报 乾隆御制诗 《故城时光...   \n",
       "12         scene  苏州博物馆 太空体验基地 瓜岛 西本愿寺 故宫 泸沽湖 维多利亚花园 阿里山森铁 太湖 念青...   \n",
       "6     government  石家庄市政府 马达加斯加政府 最高法 美国海军陆战队 国务院 美国海军 俄空军 广州国土房管...   \n",
       "8          movie  《好家伙坏家伙怪家伙》 30DaysofNight 《射雕英雄传》 《姨妈的后现代生活》 《...   \n",
       "5           game  星际 星际争霸 《魔兽世界》 《古墓丽影：传奇》 WAR3 DotA 魔兽争霸3 StarC...   \n",
       "1        address  中山公园 金融街 台北 oldlauncestonseaport 越南 堪萨斯城 摩天大厦 ...   \n",
       "10  organization  SPL 切沃 拉涅利 皇马 米兰 阿尔梅 ac米兰 意甲 桑普多利亚 伦敦德比 WCG 纽卡...   \n",
       "3        company  光大银行深圳分行信用卡中心 中国银联 中资银行 航天科技中心 EA 中华油脂网 金鹰网 中行...   \n",
       "11      position  记者 主持人 副秘书长 市长 主编 漫画家 选手 社长 副总经理 飞行员 导演 画家 记者 ...   \n",
       "9           name  奥尼尔 老子 董玉辉 易纲 奈史密斯 郭清保 史蒂夫奈尔斯 翁美玲 CCM. Grigoro...   \n",
       "\n",
       "    Category_num  \n",
       "13            19  \n",
       "0             28  \n",
       "4            259  \n",
       "7            277  \n",
       "2           1642  \n",
       "12          1936  \n",
       "6           2648  \n",
       "8           2680  \n",
       "5           3468  \n",
       "1           4146  \n",
       "10          4223  \n",
       "3           4365  \n",
       "11          6043  \n",
       "9           6269  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_label_train_Category.sort_values(['Category_num'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QQ 数据产生"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1827967769 18618193311 371451138 783594553 53344829 130238288 114281231 2308713823 2450006972 415392980 326597719 232297193 89359735 1787817738 14705257 496049279 229215757 490401933 527622886 160308912 826929452 776979804 33370722 1941600020 924969718 449801320 574785760 1143175993'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_QQ = df_label_train_Category[df_label_train_Category['Category']=='QQ']['Privacy'].to_list()[0]\n",
    "txt_QQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({10: 6, 11: 1, 9: 17, 8: 4})"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([len(i) for i in txt_QQ.split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qq():\n",
    "    qq_len = random.randint(8,11)\n",
    "    qq = ''\n",
    "    for i in range(qq_len):\n",
    "        qq += str(random.randint(0,9))\n",
    "    return qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'74391500'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_qq()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据增强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' 不是内部或外部命令，也不是可运行的程序\n",
      "或批处理文件。\n"
     ]
    }
   ],
   "source": [
    "!ls ../data/train/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_label(label_path):\n",
    "    return pd.read_csv(label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_data(data_path):\n",
    "    with open(data_path,'r',encoding=\"utf-8\") as f:\n",
    "        line = ''.join(f.readlines())\n",
    "        list_data = list(line)\n",
    "    return list_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label = get_df_label('./data/train_label/0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data = get_list_data('./data/train_data/0.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_BIOES(list_data, df_label):\n",
    "    list_label = ['O'] * len(list_data)\n",
    "    for index,d in df_label.iterrows():\n",
    "        ID = d['ID']\n",
    "        Category = d['Category']\n",
    "        Pos_b = d['Pos_b']\n",
    "        Pos_e = d['Pos_e']\n",
    "        \n",
    "        # 异常数据处理\n",
    "        if 2162 == ID:\n",
    "            Category == '前明骏女孩组合队长\"'\n",
    "            Pos_e = Pos_e - 1\n",
    "\n",
    "        if Pos_b == Pos_e:\n",
    "            list_label[Pos_b] = 'S_' + Category\n",
    "        elif Pos_b - Pos_e == 1:\n",
    "            list_label[Pos_b] = 'B_' + Category\n",
    "            list_label[Pos_e] = 'E_' + Category\n",
    "        else:\n",
    "            # 异常数据处理\n",
    "            try:\n",
    "                list_label[Pos_b] = 'B_' + Category\n",
    "                list_label[Pos_e] = 'E_' + Category \n",
    "                for pos_i in range(Pos_b+1,Pos_e):\n",
    "                    list_label[pos_i] = 'I_' + Category \n",
    "            except:\n",
    "                print(ID)\n",
    "    return [(ID,data,label) for data,label in zip(list_data,list_label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, '艺', 'O')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BIOES = get_BIOES(list_data, df_label)\n",
    "BIOES[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 方法一 插入 产生数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qq():\n",
    "    qq_len = random.randint(8,10)\n",
    "    qq = ''\n",
    "    for i in range(qq_len):\n",
    "        qq += str(random.randint(0,9))\n",
    "    return qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_BIOES_qq(qq,ID):\n",
    "    list_qq = list(qq)\n",
    "    Pos_b, Pos_e = 0, len(list_qq)-1\n",
    "    \n",
    "    list_label = ['O'] * len(list_qq)\n",
    "    if Pos_b == Pos_e:\n",
    "        list_label[Pos_b] = 'S_' + 'QQ'\n",
    "    elif Pos_e - Pos_b == 1:\n",
    "        list_label[Pos_b] = 'B_' + 'QQ'\n",
    "        list_label[Pos_e] = 'E_' + 'QQ'\n",
    "    else:\n",
    "        list_label[Pos_b] = 'B_' + 'QQ'\n",
    "        list_label[Pos_e] = 'E_' + 'QQ'\n",
    "        for pos_i in range(Pos_b+1,Pos_e):\n",
    "            list_label[pos_i] = 'I_' + 'QQ'\n",
    "    return [(ID,data,label) for data,label in zip(list_qq,list_label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(16, '8', 'B_QQ'),\n",
       " (16, '0', 'I_QQ'),\n",
       " (16, '6', 'I_QQ'),\n",
       " (16, '3', 'I_QQ'),\n",
       " (16, '6', 'I_QQ'),\n",
       " (16, '0', 'I_QQ'),\n",
       " (16, '8', 'I_QQ'),\n",
       " (16, '3', 'E_QQ')]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_BIOES_qq(get_qq(),16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_data(list_data, df_label):\n",
    "    #ID,data,label\n",
    "    #(0, '艺', 'O')\n",
    "    BIOES = get_BIOES(list_data, df_label)\n",
    "    ID = BIOES[0][0]\n",
    "    #ID,data,label\n",
    "    #(16, '3', 'E_QQ')\n",
    "    qq_BIOES = get_BIOES_qq(get_qq(),ID)\n",
    "    print(qq_BIOES)\n",
    "    # 随机插入 O 的位置\n",
    "    len_BIOES = len(BIOES)\n",
    "    r_loc = random.randint(0,len(BIOES))\n",
    "    if r_loc == 0:\n",
    "        augument_BIOES = qq_BIOES + BIOES\n",
    "    elif r_loc == len_BIOES:\n",
    "        augument_BIOES = BIOES + qq_BIOES\n",
    "    else:\n",
    "        augument_BIOES = BIOES\n",
    "        for r_loc_i in range(r_loc, len_BIOES):\n",
    "            if BIOES[r_loc_i][-1] == 'O':\n",
    "                augument_BIOES = BIOES[:r_loc_i] + qq_BIOES + BIOES[r_loc_i:]  \n",
    "                break\n",
    "    return augument_BIOES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '1', 'B_QQ'), (0, '2', 'I_QQ'), (0, '4', 'I_QQ'), (0, '1', 'I_QQ'), (0, '5', 'I_QQ'), (0, '8', 'I_QQ'), (0, '6', 'I_QQ'), (0, '8', 'I_QQ'), (0, '3', 'I_QQ'), (0, '4', 'E_QQ')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, '艺', 'O'),\n",
       " (0, '术', 'O'),\n",
       " (0, '是', 'O'),\n",
       " (0, '相', 'O'),\n",
       " (0, '同', 'O'),\n",
       " (0, '的', 'O'),\n",
       " (0, '，', 'O'),\n",
       " (0, '音', 'O'),\n",
       " (0, '乐', 'O'),\n",
       " (0, '美', 'O'),\n",
       " (0, '术', 'O'),\n",
       " (0, '体', 'O'),\n",
       " (0, '育', 'O'),\n",
       " (0, '三', 'O'),\n",
       " (0, '样', 'O'),\n",
       " (0, '都', 'O'),\n",
       " (0, '是', 'O'),\n",
       " (0, '艺', 'O'),\n",
       " (0, '术', 'O'),\n",
       " (0, '。', 'O'),\n",
       " (0, '，', 'O'),\n",
       " (0, '三', 'O'),\n",
       " (0, '样', 'O'),\n",
       " (0, '艺', 'O'),\n",
       " (0, '术', 'O'),\n",
       " (0, '都', 'O'),\n",
       " (0, '是', 'O'),\n",
       " (0, '靠', 'O'),\n",
       " (0, '感', 'O'),\n",
       " (0, '觉', 'O'),\n",
       " (0, '的', 'O'),\n",
       " (0, '。', 'O'),\n",
       " (0, '感', 'O'),\n",
       " (0, '觉', 'O'),\n",
       " (0, '好', 'O'),\n",
       " (0, '玩', 'O'),\n",
       " (0, '1', 'B_QQ'),\n",
       " (0, '2', 'I_QQ'),\n",
       " (0, '4', 'I_QQ'),\n",
       " (0, '1', 'I_QQ'),\n",
       " (0, '5', 'I_QQ'),\n",
       " (0, '8', 'I_QQ'),\n",
       " (0, '6', 'I_QQ'),\n",
       " (0, '8', 'I_QQ'),\n",
       " (0, '3', 'I_QQ'),\n",
       " (0, '4', 'E_QQ'),\n",
       " (0, '起', 'O'),\n",
       " (0, '来', 'O'),\n",
       " (0, '就', 'O'),\n",
       " (0, '很', 'O'),\n",
       " (0, '轻', 'O'),\n",
       " (0, '松', 'O'),\n",
       " (0, '，', 'O'),\n",
       " (0, '所', 'O'),\n",
       " (0, '以', 'O'),\n",
       " (0, '叫', 'O'),\n",
       " (0, '做', 'O'),\n",
       " (0, '玩', 'O'),\n",
       " (0, '艺', 'O'),\n",
       " (0, '术', 'O'),\n",
       " (0, '。', 'O'),\n",
       " (0, '没', 'O'),\n",
       " (0, '感', 'O'),\n",
       " (0, '觉', 'O'),\n",
       " (0, '找', 'O'),\n",
       " (0, '不', 'O'),\n",
       " (0, '到', 'O'),\n",
       " (0, '北', 'O'),\n",
       " (0, '的', 'O'),\n",
       " (0, '干', 'O'),\n",
       " (0, '脆', 'O'),\n",
       " (0, '别', 'O'),\n",
       " (0, '玩', 'O'),\n",
       " (0, '了', 'O'),\n",
       " (0, '！', 'O'),\n",
       " (0, '，', 'O'),\n",
       " (0, '香', 'B_position'),\n",
       " (0, '港', 'I_position'),\n",
       " (0, '电', 'I_position'),\n",
       " (0, '影', 'I_position'),\n",
       " (0, '国', 'I_position'),\n",
       " (0, '语', 'I_position'),\n",
       " (0, '配', 'I_position'),\n",
       " (0, '音', 'I_position'),\n",
       " (0, '名', 'I_position'),\n",
       " (0, '家', 'E_position'),\n",
       " (0, '周', 'B_name'),\n",
       " (0, '思', 'I_name'),\n",
       " (0, '平', 'E_name'),\n",
       " (0, '，', 'O'),\n",
       " (0, '代', 'O'),\n",
       " (0, '表', 'O'),\n",
       " (0, '作', 'O'),\n",
       " (0, '有', 'O'),\n",
       " (0, 'T', 'B_organization'),\n",
       " (0, 'V', 'I_organization'),\n",
       " (0, 'B', 'E_organization'),\n",
       " (0, '《', 'B_movie'),\n",
       " (0, '上', 'I_movie'),\n",
       " (0, '海', 'I_movie'),\n",
       " (0, '滩', 'I_movie'),\n",
       " (0, '》', 'E_movie'),\n",
       " (0, '周', 'B_name'),\n",
       " (0, '润', 'I_name'),\n",
       " (0, '发', 'E_name'),\n",
       " (0, '等', 'O')]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_new_data(list_data, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 工具包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nlpcda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from nlpcda import Randomword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load :C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nlpcda\\data\\company.txt done\n",
      "随机实体替换>>>>>>\n",
      "这是个实体：58同城；今天是2020年3月8日11:40，天气晴朗，天气很不错，空气很好，不差；这个nlpcad包，用于方便一键数据增强，可有效增强NLP模型的泛化性能、减少波动、抵抗对抗攻击\n",
      "这是个实体：中国通信服务；今天是2020年3月8日11:40，天气晴朗，天气很不错，空气很好，不差；这个nlpcad包，用于方便一键数据增强，可有效增强NLP模型的泛化性能、减少波动、抵抗对抗攻击\n",
      "这是个实体：瑞丰光电；今天是2020年3月8日11:40，天气晴朗，天气很不错，空气很好，不差；这个nlpcad包，用于方便一键数据增强，可有效增强NLP模型的泛化性能、减少波动、抵抗对抗攻击\n"
     ]
    }
   ],
   "source": [
    "test_str = '''这是个实体：58同城；今天是2020年3月8日11:40，天气晴朗，天气很不错，空气很好，不差；这个nlpcad包，用于方便一键数据增强，可有效增强NLP模型的泛化性能、减少波动、抵抗对抗攻击'''\n",
    "\n",
    "smw = Randomword(create_num=4,change_rate=5)\n",
    "rs1 = smw.replace(test_str)\n",
    "\n",
    "print('随机实体替换>>>>>>')\n",
    "for s in rs1:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "Randomword?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "国投集团控股\r\n",
      "银河娱乐\r\n",
      "金界控股\r\n",
      "均安控股\r\n",
      "泛海酒店\r\n",
      "德祥地产\r\n",
      "亚洲能源物流\r\n",
      "搜房网\r\n",
      "中国电信\r\n",
      "弘阳地产\r\n",
      "昱辉阳光\r\n",
      "领航医药生物科技\r\n",
      "宇华教育\r\n",
      "龙翔集团\r\n",
      "国际家居零售\r\n",
      "钧濠集团\r\n",
      "港铁公司\r\n",
      "KAKIKO GROUP\r\n",
      "红黄蓝\r\n",
      "万科企业\r\n",
      "宏安地产\r\n",
      "科兴生物\r\n",
      "南旋控股\r\n",
      "中信证券\r\n",
      "迪臣发展国际\r\n",
      "创业集团控股\r\n",
      "凌锐控股\r\n",
      "BOC 2014 PREF\r\n",
      "云能国际\r\n",
      "中国石油化工股份\r\n",
      "进升集团控股\r\n",
      "汇汉控股\r\n",
      "东瀛游\r\n",
      "联合能源集团\r\n",
      "数码通电讯\r\n",
      "元征科技\r\n",
      "药明生物\r\n",
      "HKE HOLDINGS\r\n",
      "宏华集团\r\n",
      "德基科技控股\r\n",
      "趣头条\r\n",
      "康宁医院\r\n",
      "天保能源\r\n",
      "58同城\r\n",
      "中国新城市\r\n",
      "中国兴业控股\r\n",
      "华宝国际\r\n",
      "福森药业\r\n",
      "嘉进投资国际\r\n",
      "远东控股国际\r\n",
      "国瑞置业\r\n",
      "惠生工程\r\n",
      "冠力国际\r\n",
      "九兴控股\r\n",
      "中国服饰控股\r\n",
      "远东环球\r\n",
      "龙辉国际控股\r\n",
      "中国联通(香港)\r\n",
      "同方友友\r\n",
      "中国香精香料\r\n",
      "仁瑞投资\r\n",
      "雨润食品\r\n",
      "中国绿岛科技\r\n",
      "中国汽车物流\r\n",
      "华昱高速\r\n",
      "恒益控股\r\n",
      "STYLAND HOLD\r\n",
      "中国人寿\r\n",
      "旭明光电\r\n",
      "和信贷\r\n",
      "麦盛资本\r\n",
      "九洲大药房\r\n",
      "华显光电\r\n",
      "盈利时\r\n",
      "正业国际\r\n",
      "东方报业集团\r\n",
      "云锋金融\r\n",
      "中国水业集团\r\n",
      "环球实业科技\r\n",
      "中油洁能控股\r\n",
      "东方海外国际\r\n",
      "绿色能源科技集团\r\n",
      "复锐医疗科技\r\n",
      "CEC INT'L HOLD\r\n",
      "新鸿基地产\r\n",
      "昆仑国际金融\r\n",
      "信佳国际\r\n",
      "英利\r\n",
      "威华达控股\r\n",
      "高力集团\r\n",
      "汉国置业\r\n",
      "盛京银行\r\n",
      "青岛啤酒股份\r\n",
      "中国石油股份\r\n",
      "正恒国际控股\r\n",
      "同方康泰\r\n",
      "三盛控股\r\n",
      "先丰服务集团\r\n",
      "现恒建筑\r\n",
      "中石化冠德\r\n",
      "中环球船务\r\n",
      "优品３６０\r\n",
      "天安\r\n",
      "思城控股\r\n",
      "中国铝罐\r\n",
      "富通科技\r\n",
      "工商银行\r\n",
      "正干金融控股\r\n",
      "华能电力\r\n",
      "珠江船务\r\n",
      "同得仕（集团）\r\n",
      "金嗓子\r\n",
      "寺库\r\n",
      "百富环球\r\n",
      "阿特斯\r\n",
      "融信中国\r\n",
      "联亚集团\r\n",
      "中国山东高速金融\r\n",
      "乙德投资控股\r\n",
      "广泰国际控股\r\n",
      "复星旅游文化\r\n",
      "星美文化旅游\r\n",
      "华润燃气\r\n",
      "汇付天下\r\n",
      "景福集团\r\n",
      "富阳\r\n",
      "中国北大荒\r\n",
      "中国南方航空股份\r\n",
      "兴利（香港）控股\r\n",
      "佳兆业健康\r\n",
      "信阳毛尖\r\n",
      "杉杉品牌\r\n",
      "达内科技\r\n",
      "沿海家园\r\n",
      "比速科技\r\n",
      "北控清洁能源集团\r\n",
      "精优药业\r\n",
      "中国财险\r\n",
      "丰德丽控股\r\n",
      "百济神州\r\n",
      "中国儿童护理\r\n",
      "桐成控股\r\n",
      "美亨实业\r\n",
      "大生农业金融\r\n",
      "五矿地产\r\n",
      "联合地产（香港）\r\n",
      "结好金融\r\n",
      "中移动\r\n",
      "迪信通\r\n",
      "洛阳玻璃股份\r\n",
      "ＢＢＩ生命科学\r\n",
      "浦林成山\r\n",
      "HERALD HOLD\r\n",
      "北京控股\r\n",
      "新鸿基公司\r\n",
      "大成生化科技\r\n",
      "前海健康\r\n",
      "建生国际\r\n",
      "合和公路基建—Ｒ\r\n",
      "旅业国际\r\n",
      "陆氏集团（越南）\r\n",
      "上为\r\n",
      "金源米业\r\n",
      "创科实业\r\n",
      "同佳健康\r\n",
      "民银资本\r\n",
      "大唐西市\r\n",
      "成实外教育\r\n",
      "艾伯科技\r\n",
      "国盛投资（新）\r\n",
      "大洋集团\r\n",
      "兴华港口\r\n",
      "中国诚通发展集团\r\n",
      "流利说\r\n",
      "晨讯科技\r\n",
      "华大酒店\r\n",
      "小牛电动\r\n",
      "中国高速传动\r\n",
      "滇池水务\r\n",
      "宝宝树集团\r\n",
      "百世集团\r\n",
      "MONGOL MINING\r\n",
      "闽信集团\r\n",
      "怡邦行控股\r\n",
      "华领医药－Ｂ\r\n",
      "新东方\r\n",
      "安莉芳控股\r\n",
      "隽泰控股\r\n",
      "西藏水资源\r\n",
      "珠海控股投资\r\n",
      "瑞思\r\n",
      "中国数码信息\r\n",
      "卡森国际\r\n",
      "顺豪控股\r\n",
      "毅德国际\r\n",
      "蒙牛乳业\r\n",
      "沧海控股\r\n",
      "博耳电力\r\n",
      "中信资源\r\n",
      "重庆钢铁股份\r\n",
      "太平洋网络\r\n",
      "陈唱国际\r\n",
      "华联国际\r\n",
      "中国能源建设\r\n",
      "佳宁娜\r\n",
      "宏辉集团\r\n",
      "第一拖拉机股份\r\n",
      "丝路物流控股\r\n",
      "易车网\r\n",
      "中国擎天软件\r\n",
      "南南资源\r\n",
      "中国优材\r\n",
      "第九城市\r\n",
      "和信超媒体\r\n",
      "永嘉集团\r\n",
      "研祥智能\r\n",
      "天宝集团\r\n",
      "百仕达控股\r\n",
      "国际资源\r\n",
      "四川能投发展\r\n",
      "庄士中国\r\n",
      "卡姆丹克太阳能\r\n",
      "廖创兴企业\r\n",
      "正兴集团\r\n",
      "新华汇富金融\r\n",
      "亚太资源\r\n",
      "亚美能源\r\n",
      "CNT GROUP\r\n",
      "智美体育\r\n",
      "品牌中国\r\n",
      "金仑控股有限公司\r\n",
      "伊泰煤炭\r\n",
      "长兴国际\r\n",
      "信星集团\r\n",
      "中广核新能源\r\n",
      "中电控股\r\n",
      "万咖壹联\r\n",
      "中国中铁\r\n",
      "汉广厦房地产\r\n",
      "互太纺织\r\n",
      "拼多多\r\n",
      "中盈盛达融资担保\r\n",
      "招商银行\r\n",
      "中国国家文化产业\r\n",
      "OKURA HOLDINGS\r\n",
      "金利来集团\r\n",
      "森美控股\r\n",
      "十方控股\r\n",
      "前进控股集团\r\n",
      "昂纳科技集团\r\n",
      "合和公路基建\r\n",
      "中国汇融\r\n",
      "现代美容\r\n",
      "首都创投\r\n",
      "民商创科\r\n",
      "高富集团控股\r\n",
      "雅视光学\r\n",
      "SINCEREWATCH HK\r\n",
      "弘达金融控股\r\n",
      "中国绿色农业\r\n",
      "中国赛特\r\n",
      "阿尔法企业\r\n",
      "绿叶制药\r\n",
      "中国绿地博大绿泽\r\n",
      "东方航空\r\n",
      "泰邦生物\r\n",
      "普汇中金国际\r\n",
      "东北电气\r\n",
      "泛亚环保\r\n",
      "SINO HOTELS\r\n",
      "越南制造加工出口\r\n",
      "庄园牧场\r\n",
      "中国白银集团\r\n",
      "中国金属利用\r\n",
      "能源国际投资\r\n",
      "保发集团\r\n",
      "威胜控股\r\n",
      "大众金融控股\r\n",
      "昌兴国际\r\n",
      "中国创新投资\r\n",
      "昊天发展集团\r\n",
      "亚积邦租赁\r\n",
      "宏利金融—Ｓ\r\n",
      "长城一带一路\r\n",
      "UT斯达康\r\n",
      "普益财富\r\n",
      "远东酒店实业\r\n",
      "新海能源\r\n",
      "易生活控股\r\n",
      "万洲国际\r\n",
      "富维薄膜\r\n",
      "大昌微线集团\r\n",
      "珠光控股\r\n",
      "亨鑫科技\r\n",
      "中国疏浚环保\r\n",
      "慧聪集团\r\n",
      "万隆控股集团\r\n",
      "泰和诚医疗\r\n",
      "大中华地产控股\r\n",
      "中国城市基础设施\r\n",
      "建业地产\r\n",
      "中国银行\r\n",
      "天长集团\r\n",
      "川控股\r\n",
      "谭木匠\r\n",
      "第一上海\r\n",
      "中信银行\r\n",
      "安踏体育\r\n",
      "申基国际\r\n",
      "松龄护老集团\r\n",
      "中国自动化\r\n",
      "畅游\r\n",
      "丽丰控股\r\n",
      "青岛银行\r\n",
      "美团点评\r\n",
      "亚洲联网科技\r\n",
      "中国建材\r\n",
      "中国罕王\r\n",
      "招商局置地\r\n",
      "奥威控股\r\n",
      "第一太平\r\n",
      "中海石油化学\r\n",
      "江苏创新\r\n",
      "佳明集团控股\r\n",
      "凯知乐国际\r\n",
      "管道工程\r\n",
      "东方电气\r\n",
      "矽品\r\n",
      "年年卡\r\n",
      "中国通商集团\r\n",
      "琥珀能源\r\n",
      "奥星制药\r\n",
      "中国银河\r\n",
      "中电光谷\r\n",
      "中广核电力\r\n",
      "恒诚建筑\r\n",
      "雅高控股\r\n",
      "亚太资源股权\r\n",
      "中航国际控股\r\n",
      "联华超市\r\n",
      "中国顺客隆\r\n",
      "国际精密\r\n",
      "万国国际矿业\r\n",
      "香港食品投资\r\n",
      "南京熊猫电子股份\r\n",
      "创毅控股\r\n",
      "冠华国际控股\r\n",
      "中国卫生集团\r\n",
      "华邦金融\r\n",
      "SIS INT'L\r\n",
      "老虎证券\r\n",
      "品钛\r\n",
      "新华通讯频媒\r\n",
      "BOSSINI INT'L\r\n",
      "新浪\r\n",
      "弘和仁爱医疗\r\n",
      "泰邦集团\r\n",
      "大成糖业\r\n",
      "华厦置业\r\n",
      "中国农林低碳\r\n",
      "中远海控\r\n",
      "中海油\r\n",
      "中国铁建\r\n",
      "利邦\r\n",
      "顺泰控股\r\n",
      "顺腾国际控股\r\n",
      "常茂生物\r\n",
      "民生银行\r\n",
      "华能国际电力股份\r\n",
      "中车时代电气\r\n",
      "泛海集团\r\n",
      "KK文化\r\n",
      "山东墨龙\r\n",
      "中国智能集团\r\n",
      "泛华金融\r\n",
      "蓝汛\r\n",
      "金轮天地控股\r\n",
      "华人置业\r\n",
      "中国天然资源\r\n",
      "中国航天万源\r\n",
      "珠江钢管\r\n",
      "信义光能\r\n",
      "工盖有限公司\r\n",
      "华晨中国\r\n",
      "Ａ８新媒体\r\n",
      "冠忠巴士集团\r\n",
      "合一投资\r\n",
      "上海证大\r\n",
      "粤海制革\r\n",
      "好孩子国际\r\n",
      "恒生银行\r\n",
      "天虹纺织\r\n",
      "汉港控股\r\n",
      "中汽系统\r\n",
      "中国太保\r\n",
      "元力控股\r\n",
      "京投交通科技\r\n",
      "凤凰卫视\r\n",
      "兖州煤业股份\r\n",
      "贝森金融\r\n",
      "唐宫中国\r\n",
      "先施\r\n",
      "资本策略地产\r\n",
      "伟俊集团控股\r\n",
      "英达公路再生科技\r\n",
      "雅士利国际\r\n",
      "鹰君\r\n",
      "万景控股\r\n",
      "兴达国际\r\n",
      "HYPEBEAST\r\n",
      "博润\r\n",
      "天洁环境\r\n",
      "珂莱蒂尔\r\n",
      "英裘控股\r\n",
      "新源万恒控股\r\n",
      "黄河实业\r\n",
      "华融金控\r\n",
      "天鸽互动\r\n",
      "环球信贷集团\r\n",
      "海通证券\r\n",
      "第一视频\r\n",
      "中策集团\r\n",
      "友达光电\r\n",
      "天德化工\r\n",
      "保利达资产\r\n",
      "立基工程控股\r\n",
      "天津港发展\r\n",
      "万邦投资\r\n",
      "九龙仓置业\r\n",
      "航美传媒\r\n",
      "光大永年\r\n",
      "香港医思医疗集团\r\n",
      "茂业国际\r\n",
      "守益控股\r\n",
      "简普科技\r\n",
      "彩虹新能源\r\n",
      "津上机床中国\r\n",
      "天津发展\r\n",
      "童园国际\r\n",
      "嘉年华国际\r\n",
      "鼎亿集团投资\r\n",
      "中国东方航空股份\r\n",
      "中国飞机租赁\r\n",
      "海港企业\r\n",
      "嘉士利集团\r\n",
      "精熙国际\r\n",
      "宏基资本\r\n",
      "华禧控股\r\n",
      "金卫医疗\r\n",
      "奇景光电\r\n",
      "连达科技控股\r\n",
      "JF中国基金\r\n",
      "基石药业－Ｂ\r\n",
      "建联集团\r\n",
      "金融界\r\n",
      "荣丰联合控股\r\n",
      "东江环保\r\n",
      "恒达科技控股\r\n",
      "浙商银行\r\n",
      "中国平安\r\n",
      "齐鲁高速\r\n",
      "槟杰科达\r\n",
      "美美证券\r\n",
      "奥克斯国际\r\n",
      "中国中药\r\n",
      "爱得威建设集团\r\n",
      "依波路\r\n",
      "冠捷科技\r\n",
      "人人公司\r\n",
      "四洲集团\r\n",
      "中国宝丰国际\r\n",
      "安宁控股\r\n",
      "成都普天电缆股份\r\n",
      "迅捷环球控股\r\n",
      "歌礼制药－Ｂ\r\n",
      "康臣药业\r\n",
      "凯普松国际\r\n",
      "耀莱集团\r\n",
      "华电福新\r\n",
      "彭顺国际\r\n",
      "蔚来\r\n",
      "IDT INT'L\r\n",
      "中国核能科技\r\n",
      "富途\r\n",
      "李氏大药厂\r\n",
      "隆基泰和智慧能源\r\n",
      "兰亭集势\r\n",
      "中生联合\r\n",
      "中国新城镇\r\n",
      "百勤油服\r\n",
      "绿色动力环保\r\n",
      "阿里巴巴\r\n",
      "CWT INT'L\r\n",
      "四川成渝高速公路\r\n",
      "爱康医疗\r\n",
      "郑州银行\r\n",
      "凹凸科技\r\n",
      "首创环境\r\n",
      "有线宽频\r\n",
      "万华媒体\r\n",
      "中播控股\r\n",
      "英皇国际\r\n",
      "建业实业\r\n",
      "瑞港集团\r\n",
      "苍南仪表\r\n",
      "灵宝黄金\r\n",
      "S.A.S. DRAGON\r\n",
      "新兴印刷\r\n",
      "卡宾\r\n",
      "天利控股集团\r\n",
      "汇力资源\r\n",
      "奥邦建筑\r\n",
      "中天国际\r\n",
      "中国天瑞水泥\r\n",
      "唯品会\r\n",
      "盛源控股\r\n",
      "中国通信服务\r\n",
      "中国新经济投资\r\n",
      "ASIA SATELLITE\r\n",
      "力世纪\r\n",
      "瑞风新能源\r\n",
      "大全新能源\r\n",
      "中铝国际\r\n",
      "HSSP INTL\r\n",
      "瑞丰动力\r\n",
      "铭霖控股\r\n",
      "莱尔斯丹\r\n",
      "国开国际投资\r\n",
      "PERFECTECH INTL\r\n",
      "阿里影业\r\n",
      "新矿资源\r\n",
      "三和建筑集团\r\n",
      "长江生命科技\r\n",
      "时富投资\r\n",
      "中渝置地\r\n",
      "胜狮货柜\r\n",
      "建溢集团\r\n",
      "都市丽人\r\n",
      "中国集成控股\r\n",
      "现代传播\r\n",
      "鑫达集团\r\n",
      "亚洲实业集团\r\n",
      "英皇文化产业\r\n",
      "中国稀土\r\n",
      "中国资源交通\r\n",
      "佳兆业物业\r\n",
      "REF HOLDINGS\r\n",
      "兴纺控股\r\n",
      "天臣控股\r\n",
      "碧瑶绿色集团\r\n",
      "中电华大科技\r\n",
      "中发展控股\r\n",
      "中誉集团\r\n",
      "蘑菇街\r\n",
      "TS WONDERS\r\n",
      "中化化肥\r\n",
      "国银租赁\r\n",
      "FUTURE BRIGHT\r\n",
      "惠记集团\r\n",
      "美建集团\r\n",
      "中国育儿网络\r\n",
      "专业旅运\r\n",
      "农业银行\r\n",
      "趣店\r\n",
      "雅迪控股\r\n",
      "中视金桥\r\n",
      "映美控股\r\n",
      "大中华集团\r\n",
      "雅仕维\r\n",
      "国际天食\r\n",
      "中国多金属\r\n",
      "海湾资源\r\n",
      "璋利国际\r\n",
      "新世纪集团\r\n",
      "恒基地产\r\n",
      "开元酒店\r\n",
      "WING ON CO\r\n",
      "银杏教育\r\n",
      "STYLAND W1911\r\n",
      "国药控股\r\n",
      "赛晶电力电子\r\n",
      "开明投资\r\n",
      "新城发展控股\r\n",
      "HTSC\r\n",
      "畅捷通\r\n",
      "海隆控股\r\n",
      "正道集团\r\n",
      "碧生源\r\n",
      "鹏程亚洲\r\n",
      "中科生物\r\n",
      "中国三迪\r\n",
      "复星国际\r\n",
      "皓天财经集团\r\n",
      "中建富通\r\n",
      "南方航空\r\n",
      "熊猫绿能\r\n",
      "芯智控股\r\n",
      "ASIA COMM HOLD\r\n",
      "中国唐商\r\n",
      "博雅互动\r\n",
      "中粮肉食\r\n",
      "仁天科技控股\r\n",
      "中国再生能源投资\r\n",
      "万成集团股份\r\n",
      "瀚华金控\r\n",
      "滨海投资\r\n",
      "嘉瑞国际\r\n",
      "中远海运港口\r\n",
      "海昌海洋公园\r\n",
      "华信金融投资\r\n",
      "新华保险\r\n",
      "企展控股\r\n",
      "中国智能交通\r\n",
      "友佳国际\r\n",
      "中国医疗网络\r\n",
      "厦门港务\r\n",
      "乐居\r\n",
      "中国动力控股\r\n",
      "大成食品\r\n",
      "滨江服务\r\n",
      "珀丽酒店\r\n",
      "NATIONAL ELEC H\r\n",
      "通力电子\r\n",
      "顺豪物业\r\n",
      "保集健康\r\n",
      "中国信息技术\r\n",
      "安徽皖通高速公路\r\n",
      "恩达集团控股\r\n",
      "金榜集团\r\n",
      "中银香港\r\n",
      "五龙电动车\r\n",
      "胜捷企业\r\n",
      "星星地产\r\n",
      "汛和集团\r\n",
      "天源集团\r\n",
      "达力环保\r\n",
      "神州控股\r\n",
      "香港通讯国际控股\r\n",
      "华油能源\r\n",
      "励晶太平洋\r\n",
      "大悦城地产\r\n",
      "力宝华润\r\n",
      "美瑞健康国际\r\n",
      "江南集团\r\n",
      "布莱克万矿业\r\n",
      "渝太地产\r\n",
      "海亮国际\r\n",
      "博奇环保\r\n",
      "广南（集团）\r\n",
      "长盈集团（控股）\r\n",
      "长江制衣\r\n",
      "必瘦站\r\n",
      "宏基集团控股\r\n",
      "浩泽净水\r\n",
      "西王特钢\r\n",
      "沪港联合\r\n",
      "理文化工\r\n",
      "威高国际\r\n",
      "平潭海洋\r\n",
      "I.T\r\n",
      "世纪阳光\r\n",
      "广泽国际发展\r\n",
      "永恒策略\r\n",
      "中华电信\r\n",
      "中智全球\r\n",
      "稻香控股\r\n",
      "交通银行\r\n",
      "华住酒店集团\r\n",
      "创联教育金融\r\n",
      "京东方精电\r\n",
      "鹰力投资\r\n",
      "东英金融\r\n",
      "爱奇艺\r\n",
      "博骏教育\r\n",
      "天大药业\r\n",
      "ＹＵＳＥＩ\r\n",
      "魏桥纺织\r\n",
      "上谕集团\r\n",
      "冠中地产（新）\r\n",
      "百盛集团\r\n",
      "润利海事\r\n",
      "金利丰金融\r\n",
      "衍生集团\r\n",
      "新秀丽\r\n",
      "胜利管道\r\n",
      "敏捷控股\r\n",
      "小米集团－Ｗ\r\n",
      "高银金融\r\n",
      "恒和集团\r\n",
      "天成国际\r\n",
      "中通\r\n",
      "长城环亚控股\r\n",
      "粤丰环保\r\n",
      "伯明翰体育股权\r\n",
      "达进东方照明\r\n",
      "光丽科技\r\n",
      "粤运交通\r\n",
      "星凯控股\r\n",
      "粤海置地\r\n",
      "中国环保能源\r\n",
      "西王置业\r\n",
      "时计宝\r\n",
      "兴科蓉医药\r\n",
      "500彩票网\r\n",
      "德永佳集团\r\n",
      "中国互联网投资\r\n",
      "亚证地产\r\n",
      "星宏传媒\r\n",
      "权威金融\r\n",
      "飞思达科技\r\n",
      "赛伯乐国际控股\r\n",
      "毅兴行\r\n",
      "富临集团控股\r\n",
      "联合集团\r\n",
      "航天控股\r\n",
      "朗廷—ＳＳ\r\n",
      "高山企业\r\n",
      "汇盈控股\r\n",
      "亚洲电视控股\r\n",
      "有利集团\r\n",
      "汇丰控股\r\n",
      "中绿\r\n",
      "联泰控股\r\n",
      "幸福控股\r\n",
      "米技国际控股\r\n",
      "富智康集团\r\n",
      "大同机械\r\n",
      "中华汽车\r\n",
      "三爱健康集团\r\n",
      "中粮包装\r\n",
      "伟俊矿业集团\r\n",
      "东阳光药\r\n",
      "合和实业\r\n",
      "李宁\r\n",
      "卓珈控股\r\n",
      "３６１度\r\n",
      "富士高实业\r\n",
      "新世界百货中国\r\n",
      "金山能源\r\n",
      "百信国际\r\n",
      "兴证国际\r\n",
      "国双科技\r\n",
      "明辉国际\r\n",
      "亿和控股\r\n",
      "爱德新能源\r\n",
      "椰丰集团\r\n",
      "奥瑞金种业\r\n",
      "依利安达\r\n",
      "新焦点\r\n",
      "新威国际\r\n",
      "锦艺集团控股\r\n",
      "亨泰\r\n",
      "叶氏化工集团\r\n",
      "东光化工\r\n",
      "中国农产品交易\r\n",
      "天誉置业\r\n",
      "广州农商银行\r\n",
      "优源控股\r\n",
      "太古股份公司Ｂ\r\n",
      "中国贷款公司\r\n",
      "搜狐\r\n",
      "中国天弓控股\r\n",
      "潼关黄金\r\n",
      "莱蒙国际\r\n",
      "集一家居\r\n",
      "亨得利\r\n",
      "永利地产发展\r\n",
      "山东新华制药股份\r\n",
      "米兰站\r\n",
      "凯华集团\r\n",
      "尚德机构\r\n",
      "钜派\r\n",
      "FAST RETAIL-DRS\r\n",
      "日成控股\r\n",
      "紫金矿业\r\n",
      "卓悦控股\r\n",
      "天彩控股\r\n",
      "盛诺集团\r\n",
      "泰坦能源技术\r\n",
      "华夏动漫\r\n",
      "包浩斯国际\r\n",
      "佐丹奴国际\r\n",
      "钱唐控股\r\n",
      "LHN\r\n",
      "坚宝国际\r\n",
      "太睿国际控股\r\n",
      "恒基发展\r\n",
      "茂盛控股\r\n",
      "恒鼎实业\r\n",
      "理士国际\r\n",
      "联康生物科技集团\r\n",
      "中国环境资源\r\n",
      "腾讯控股\r\n",
      "中信股份\r\n",
      "亿胜生物科技\r\n",
      "进智公共交通\r\n",
      "玉柴国际\r\n",
      "永泰地产\r\n",
      "波司登\r\n",
      "中国公共采购\r\n",
      "中国移动\r\n",
      "上置集团\r\n",
      "云游控股\r\n",
      "弘业期货\r\n",
      "香港国际建投\r\n",
      "山东国信\r\n",
      "枫叶教育\r\n",
      "大健康国际\r\n",
      "日清食品\r\n",
      "荣威国际\r\n",
      "国际娱乐\r\n",
      "中国脐带血库\r\n",
      "大唐投资国际\r\n",
      "宝光实业\r\n",
      "平安证券集团控股\r\n",
      "开达集团\r\n",
      "中国心连心化肥\r\n",
      "华讯\r\n",
      "香港兴业国际\r\n",
      "鼎石资本\r\n",
      "青岛控股\r\n",
      "自动系统\r\n",
      "华彩控股\r\n",
      "致浩达控股\r\n",
      "威铖国际\r\n",
      "华商国际海洋控股\r\n",
      "太古股份公司Ａ\r\n",
      "达飞控股\r\n",
      "银建国际\r\n",
      "鳄鱼恤\r\n",
      "利宝阁集团\r\n",
      "中国金控\r\n",
      "狮子山集团\r\n",
      "永发置业\r\n",
      "新体育\r\n",
      "中国金融投资管理\r\n",
      "万达酒店发展\r\n",
      "新福港\r\n",
      "国锐地产\r\n",
      "金马能源\r\n",
      "MOS HOUSE\r\n",
      "正利控股\r\n",
      "吉利汽车\r\n",
      "彼岸控股\r\n",
      "首沣控股\r\n",
      "激成投资\r\n",
      "中国海洋石油\r\n",
      "中国上城\r\n",
      "泸州银行\r\n",
      "华侨城（亚洲）\r\n",
      "叙福楼集团\r\n",
      "雅各臣科研制药\r\n",
      "５１信用卡\r\n",
      "白花油\r\n",
      "富一国际控股\r\n",
      "TEAMWAY INTL GP\r\n",
      "盈健医疗\r\n",
      "扬科集团\r\n",
      "亚太丝路投资\r\n",
      "中网在线\r\n",
      "华润啤酒\r\n",
      "华荣能源\r\n",
      "华建控股\r\n",
      "永胜医疗\r\n",
      "万保刚集团\r\n",
      "拍拍贷\r\n",
      "久融控股\r\n",
      "域高国际控股\r\n",
      "中国太平\r\n",
      "俊知集团\r\n",
      "林达控股\r\n",
      "TERMBRAY IND\r\n",
      "齐屹科技\r\n",
      "普甜食品\r\n",
      "普华和顺\r\n",
      "创信国际\r\n",
      "兆邦基地产\r\n",
      "世纪集团国际\r\n",
      "俊裕地基\r\n",
      "皇朝家私\r\n",
      "聚美优品\r\n",
      "中国人民保险集团\r\n",
      "前程无忧\r\n",
      "基石控股\r\n",
      "1药网\r\n",
      "文化传信\r\n",
      "中国绿宝\r\n",
      "天德地产\r\n",
      "阳光能源\r\n",
      "天津银行\r\n",
      "TAI CHEUNG HOLD\r\n",
      "联合信息\r\n",
      "鑫网易商\r\n",
      "华润电力\r\n",
      "美丽中国控股\r\n",
      "STERLING GP\r\n",
      "众美联\r\n",
      "敏实集团\r\n",
      "天韵国际控股\r\n",
      "金风科技\r\n",
      "精英国际\r\n",
      "中国烯谷集团\r\n",
      "中国春来\r\n",
      "英皇娱乐酒店\r\n",
      "ＫＦＭ金德\r\n",
      "辰兴发展\r\n",
      "长城汽车\r\n",
      "石药集团\r\n",
      "能发伟业\r\n",
      "江南布衣\r\n",
      "首创钜大\r\n",
      "中国铁钛\r\n",
      "中国再保险\r\n",
      "新宇环保\r\n",
      "致丰工业电子\r\n",
      "SHOUGANG GRAND\r\n",
      "祈福生活服务\r\n",
      "科通芯城\r\n",
      "ＳＯＨＯ中国\r\n",
      "上海集优\r\n",
      "中国金石\r\n",
      "首控集团\r\n",
      "奥盛创新\r\n",
      "中国优通\r\n",
      "世纪互联\r\n",
      "香港信贷\r\n",
      "晶科能源\r\n",
      "中国星集团\r\n",
      "木薯资源\r\n",
      "东胜旅游\r\n",
      "天华阳光\r\n",
      "ASM PACIFIC\r\n",
      "天津津燃公用\r\n",
      "优库资源\r\n",
      "丽新国际\r\n",
      "乐游科技控股\r\n",
      "强泰环保\r\n",
      "东岳集团\r\n",
      "新丰泰集团\r\n",
      "安捷利实业\r\n",
      "恒嘉融资租赁\r\n",
      "远东发展\r\n",
      "锦兴国际控股\r\n",
      "易鑫集团\r\n",
      "如涵控股\r\n",
      "新时代能源\r\n",
      "宾仕国际\r\n",
      "百利保控股\r\n",
      "华润水泥控股\r\n",
      "华润置地\r\n",
      "正保教育\r\n",
      "迅雷\r\n",
      "银基集团\r\n",
      "研控科技\r\n",
      "河北建设\r\n",
      "万嘉集团\r\n",
      "博富临置业\r\n",
      "神州租车\r\n",
      "金达控股\r\n",
      "飞达控股\r\n",
      "小赢科技\r\n",
      "移动互联（中国）\r\n",
      "耀才证券金融\r\n",
      "博华太平洋\r\n",
      "太兴置业\r\n",
      "兴胜创建\r\n",
      "久泰邦达能源\r\n",
      "兴泸水务\r\n",
      "鸿宝资源\r\n",
      "和利时自动化\r\n",
      "剑虹集团控股\r\n",
      "盈丰科技\r\n",
      "位元堂\r\n",
      "其利工业集团\r\n",
      "信而富\r\n",
      "安德利果汁\r\n",
      "顺龙控股\r\n",
      "环科国际\r\n",
      "中国黄金国际\r\n",
      "澳门励骏\r\n",
      "中能控股\r\n",
      "中核国际\r\n",
      "金猫银猫\r\n",
      "梦东方\r\n",
      "永新视博\r\n",
      "ＩＤＧ能源投资\r\n",
      "中国中冶\r\n",
      "绿心集团\r\n",
      "展程控股\r\n",
      "大众公用\r\n",
      "朗诗绿色集团\r\n",
      "双运控股\r\n",
      "信达国际控股\r\n",
      "保诚\r\n",
      "维太移动\r\n",
      "巨星医疗控股\r\n",
      "青建国际\r\n",
      "巨腾国际\r\n",
      "微贷网\r\n",
      "爱康国宾\r\n",
      "鹰美\r\n",
      "蜡笔小新食品\r\n",
      "新奥能源\r\n",
      "珩湾科技\r\n",
      "同仁堂科技\r\n",
      "金诚控股\r\n",
      "华夏健康产业\r\n",
      "MATRIX HOLDINGS\r\n",
      "现代牧业\r\n",
      "龙升集团控股\r\n",
      "金朝阳集团\r\n",
      "瑞斯康集团\r\n",
      "九江银行\r\n",
      "延长石油国际\r\n",
      "港亚控股\r\n",
      "REGAL INT'L\r\n",
      "开源控股\r\n",
      "联合医务\r\n",
      "合丰集团\r\n",
      "大中华金融\r\n",
      "慧荣科技\r\n",
      "卡撒天娇\r\n",
      "江山控股\r\n",
      "恒大健康\r\n",
      "天工国际\r\n",
      "中国天然气\r\n",
      "招商局中国基金\r\n",
      "超盈国际控股\r\n",
      "香港建屋贷款\r\n",
      "大生地产\r\n",
      "星谦发展\r\n",
      "中港照相\r\n",
      "看通集团\r\n",
      "欢喜传媒\r\n",
      "王氏国际\r\n",
      "维达国际\r\n",
      "皇冠环球集团\r\n",
      "本间高尔夫\r\n",
      "实力建业\r\n",
      "中能国际控股\r\n",
      "比亚迪电子\r\n",
      "天津创业环保股份\r\n",
      "长和\r\n",
      "南方通信\r\n",
      "中国交通建设\r\n",
      "首都信息\r\n",
      "盈信控股\r\n",
      "银城国际控股\r\n",
      "哔哩哔哩\r\n",
      "亿都（国际控股）\r\n",
      "达力集团\r\n",
      "极光\r\n",
      "上石化\r\n",
      "大明国际\r\n",
      "百度\r\n",
      "福晟国际\r\n",
      "甘肃银行\r\n",
      "香港华人有限公司\r\n",
      "信源企业集团\r\n",
      "伟业控股\r\n",
      "恒都集团\r\n",
      "海福德集团\r\n",
      "宝业集团\r\n",
      "澳洲成峰高教\r\n",
      "成志控股\r\n",
      "建福集团\r\n",
      "众安房产\r\n",
      "中国置业投资\r\n",
      "澳狮环球\r\n",
      "朸浚国际\r\n",
      "训修实业\r\n",
      "香港生力啤\r\n",
      "猎豹移动\r\n",
      "东建国际\r\n",
      "洛阳钼业\r\n",
      "国美金融科技\r\n",
      "新沣集团\r\n",
      "百德国际\r\n",
      "景联集团\r\n",
      "格林酒店\r\n",
      "易纬集团\r\n",
      "大发地产\r\n",
      "天福\r\n",
      "新昌创展控股\r\n",
      "华鼎控股\r\n",
      "飞尚无烟煤\r\n",
      "南华金融\r\n",
      "维港环保科技\r\n",
      "亚投金融集团\r\n",
      "中金公司\r\n",
      "２１世纪教育\r\n",
      "道和环球\r\n",
      "融众金融\r\n",
      "汇聚科技\r\n",
      "欢聚时代\r\n",
      "未来世界金融\r\n",
      "新疆新鑫矿业\r\n",
      "金沙中国有限公司\r\n",
      "中民筑友智造科技\r\n",
      "亚信科技\r\n",
      "中国燃气\r\n",
      "中国透云\r\n",
      "中国中车\r\n",
      "利时集团控股\r\n",
      "上海实业环境\r\n",
      "瑞安建业\r\n",
      "融太集团\r\n",
      "金宝通\r\n",
      "锦胜集团（控股）\r\n",
      "融信资源\r\n",
      "海升果汁\r\n",
      "千百度\r\n",
      "利民实业\r\n",
      "五谷磨房\r\n",
      "华米\r\n",
      "FAIRWOOD HOLD\r\n",
      "鲜驰达控股\r\n",
      "华众车载\r\n",
      "百奥家庭互动\r\n",
      "无忧英语\r\n",
      "中国有色矿业\r\n",
      "实德环球\r\n",
      "香港电视\r\n",
      "360金融\r\n",
      "京西国际\r\n",
      "潍柴动力\r\n",
      "美高梅中国\r\n",
      "中盈集团控股\r\n",
      "帝国集团环球控股\r\n",
      "爪哇控股\r\n",
      "利福中国\r\n",
      "美高域\r\n",
      "晶澳太阳能\r\n",
      "阳光１００中国\r\n",
      "力丰（集团）\r\n",
      "创兴银行\r\n",
      "顺诚\r\n",
      "碧桂园\r\n",
      "QPL INT'L\r\n",
      "澳至尊\r\n",
      "万科置业海外\r\n",
      "意马国际\r\n",
      "中国金洋\r\n",
      "KEE\r\n",
      "中国钱包\r\n",
      "勇利投资\r\n",
      "坛金矿业\r\n",
      "中国生物制药\r\n",
      "K2 F&B\r\n",
      "合富辉煌\r\n",
      "核心经济投资\r\n",
      "永义国际\r\n",
      "中国全通\r\n",
      "香港交易所\r\n",
      "国际商业结算\r\n",
      "合景泰富集团\r\n",
      "鸿兴印刷集团\r\n",
      "港华燃气\r\n",
      "ATA公司\r\n",
      "进阶发展\r\n",
      "美亚娱乐资讯\r\n",
      "润东汽车\r\n",
      "顺风清洁能源\r\n",
      "金山工业\r\n",
      "百福控股\r\n",
      "东江集团控股\r\n",
      "瑞立集团\r\n",
      "中国陶瓷\r\n",
      "亚太卫星\r\n",
      "华谊腾讯娱乐\r\n",
      "美丽华酒店\r\n",
      "冠军科技集团\r\n",
      "环宇物流（亚洲）\r\n",
      "东方表行集团\r\n",
      "庆铃汽车股份\r\n",
      "希玛眼科\r\n",
      "美东汽车\r\n",
      "新世界发展\r\n",
      "弘海高新资源\r\n",
      "恒盛地产\r\n",
      "新晨动力\r\n",
      "虎牙\r\n",
      "保利协鑫能源\r\n",
      "北京燃气蓝天\r\n",
      "丰盛服务集团\r\n",
      "华虹半导体\r\n",
      "泰山石化\r\n",
      "中国秦发\r\n",
      "先机企业集团\r\n",
      "JOYCE BOUTIQUE\r\n",
      "冠轈控股\r\n",
      "永旺\r\n",
      "宝峰时尚\r\n",
      "卓越教育集团\r\n",
      "兴发铝业\r\n",
      "彩星玩具\r\n",
      "新华文轩\r\n",
      "胜龙国际\r\n",
      "康迪车业\r\n",
      "中达集团控股\r\n",
      "澳博控股\r\n",
      "中国派对文化\r\n",
      "新龙移动\r\n",
      "伟能集团\r\n",
      "易大宗\r\n",
      "广深铁路\r\n",
      "瑞鑫国际集团\r\n",
      "中国先锋医药\r\n",
      "永利澳门\r\n",
      "阅文集团\r\n",
      "宝威控股\r\n",
      "沈阳公用发展股份\r\n",
      "百济神州－Ｂ\r\n",
      "苏创燃气\r\n",
      "同方泰德\r\n",
      "盛洋投资\r\n",
      "友联租赁\r\n",
      "申万宏源香港\r\n",
      "神冠控股\r\n",
      "利兴发展\r\n",
      "电讯首科\r\n",
      "三宝科技\r\n",
      "中国恒石\r\n",
      "中奥到家\r\n",
      "东吴水泥\r\n",
      "泛亚国际\r\n",
      "SOLOMON SYSTECH\r\n",
      "中石油\r\n",
      "亚洲资源\r\n",
      "泰克飞石\r\n",
      "欣融国际\r\n",
      "威雅利\r\n",
      "建鹏控股\r\n",
      "合兴集团\r\n",
      "自然美\r\n",
      "盛力达科技\r\n",
      "华金国际资本\r\n",
      "PERSTA\r\n",
      "鑫苑置业\r\n",
      "中海重工\r\n",
      "寰宇娱乐文化\r\n",
      "中国中石控股\r\n",
      "福田实业\r\n",
      "滉达富控股\r\n",
      "中国大冶有色金属\r\n",
      "迪生创建\r\n",
      "哈尔滨电气\r\n",
      "霸王集团\r\n",
      "万城控股\r\n",
      "中星集团控股\r\n",
      "海亮教育\r\n",
      "意达利控股\r\n",
      "光宇国际集团科技\r\n",
      "电讯数码控股\r\n",
      "数字王国\r\n",
      "煜荣集团\r\n",
      "鼎立资本\r\n",
      "北青传媒\r\n",
      "星光集团\r\n",
      "巨涛海洋石油服务\r\n",
      "东方网库\r\n",
      "利基控股\r\n",
      "人和商业\r\n",
      "创建集团控股\r\n",
      "耀高控股\r\n",
      "中国恒天立信国际\r\n",
      "鼎丰集团控股\r\n",
      "香港小轮（集团）\r\n",
      "中国联通\r\n",
      "优信\r\n",
      "安贤园中国\r\n",
      "大森控股\r\n",
      "汽车之家\r\n",
      "保德国际发展\r\n",
      "郑煤机\r\n",
      "双桦控股\r\n",
      "新明中国\r\n",
      "星岛\r\n",
      "泛华保险\r\n",
      "综合环保集团\r\n",
      "广州基金国际控股\r\n",
      "众诚能源\r\n",
      "新世纪医疗\r\n",
      "信邦控股\r\n",
      "松景科技\r\n",
      "协鑫新能源\r\n",
      "翠华控股\r\n",
      "昆仑能源\r\n",
      "中国动向\r\n",
      "国华\r\n",
      "世界（集团）\r\n",
      "华章科技\r\n",
      "五菱汽车\r\n",
      "联电\r\n",
      "中国金融租赁\r\n",
      "升捷控股\r\n",
      "浦江中国\r\n",
      "宝胜国际\r\n",
      "中兴通讯\r\n",
      "创升控股\r\n",
      "华信地产财务\r\n",
      "联众\r\n",
      "友邦保险\r\n",
      "金活医药集团\r\n",
      "圣马丁国际\r\n",
      "建成控股\r\n",
      "中国圣牧\r\n",
      "银科控股\r\n",
      "中国宏泰发展\r\n",
      "培力控股\r\n",
      "精英汇集团\r\n",
      "云南水务\r\n",
      "世纪建业\r\n",
      "伟工控股\r\n",
      "霭华押业信贷\r\n",
      "恒隆地产\r\n",
      "亿达中国\r\n",
      "国电科环\r\n",
      "中国投融资\r\n",
      "指尖悦动\r\n",
      "当代置业\r\n",
      "满地科技股份\r\n",
      "全达电器集团控股\r\n",
      "国安国际\r\n",
      "阳光纸业\r\n",
      "青岛港\r\n",
      "华兴资本控股\r\n",
      "恒安国际\r\n",
      "克莉丝汀\r\n",
      "比亚迪股份\r\n",
      "远洋集团\r\n",
      "瑞慈医疗\r\n",
      "新天地产集团\r\n",
      "长实集团\r\n",
      "冠华国际控股股权\r\n",
      "哈尔滨银行\r\n",
      "美捷汇控股\r\n",
      "舜宇光学科技\r\n",
      "北方矿业\r\n",
      "网易\r\n",
      "中比能源\r\n",
      "益华控股\r\n",
      "中国富强金融\r\n",
      "共享集团\r\n",
      "云裳衣\r\n",
      "长寿花食品\r\n",
      "侨雄国际\r\n",
      "汇鑫小贷\r\n",
      "大酒店\r\n",
      "安保工程控股\r\n",
      "九台农商银行\r\n",
      "中国民生金融\r\n",
      "佐力小贷\r\n",
      "亚洲金融\r\n",
      "中国恒大\r\n",
      "申洲国际\r\n",
      "云智汇科技\r\n",
      "PALADIN\r\n",
      "德莱建业\r\n",
      "大连港\r\n",
      "中国金茂\r\n",
      "海鑫集团\r\n",
      "AEON CREDIT\r\n",
      "华新手袋国际控股\r\n",
      "英皇钟表珠宝\r\n",
      "交银国际\r\n",
      "民生国际\r\n",
      "谢瑞麟\r\n",
      "金力集团\r\n",
      "亚洲联合基建控股\r\n",
      "白马户外媒体\r\n",
      "朗生医药\r\n",
      "安域亚洲\r\n",
      "NIRAKU\r\n",
      "港通控股\r\n",
      "标准资源控股\r\n",
      "世纪金花\r\n",
      "港桥金融\r\n",
      "庄士机构国际\r\n",
      "南华集团控股\r\n",
      "微博\r\n",
      "莲和医疗\r\n",
      "同仁堂国药\r\n",
      "财讯传媒\r\n",
      "太阳城集团\r\n",
      "雷士照明\r\n",
      "云顶香港\r\n",
      "万裕科技\r\n",
      "经济日报集团\r\n",
      "泰凌医药\r\n",
      "北京京客隆\r\n",
      "川河集团\r\n",
      "粤海投资\r\n",
      "金茂酒店—ＳＳ\r\n",
      "海尔电器\r\n",
      "御佳控股\r\n",
      "金邦达宝嘉\r\n",
      "江西银行\r\n",
      "建业建荣\r\n",
      "济丰包装\r\n",
      "中国海外发展\r\n",
      "格林国际控股－新\r\n",
      "利标品牌\r\n",
      "富力地产\r\n",
      "科劲国际\r\n",
      "建设银行\r\n",
      "保华集团\r\n",
      "纷美包装\r\n",
      "远大中国\r\n",
      "中民控股\r\n",
      "通达宏泰\r\n",
      "携程网\r\n",
      "冠城钟表珠宝\r\n",
      "香港资源控股\r\n",
      "新华联资本\r\n",
      "皇中国际\r\n",
      "优越集团控股\r\n",
      "诺华家具\r\n",
      "鸿福堂\r\n",
      "泓盈控股\r\n",
      "紫光控股\r\n",
      "蒙古能源\r\n",
      "浪潮国际\r\n",
      "惠生国际\r\n",
      "黎氏企业\r\n",
      "原生态牧业\r\n",
      "橙天嘉禾\r\n",
      "毛记葵涌\r\n",
      "万事昌国际\r\n",
      "六福集团\r\n",
      "中集天达\r\n",
      "百本医护\r\n",
      "中创环球\r\n",
      "天瑞汽车内饰\r\n",
      "兖煤澳大利亚\r\n",
      "邵氏兄弟控股\r\n",
      "京维集团\r\n",
      "通天酒业\r\n",
      "乐信\r\n",
      "恒宇集团\r\n",
      "世界华文媒体\r\n",
      "中州证券\r\n",
      "远见控股\r\n",
      "金凰珠宝\r\n",
      "大昌行集团\r\n",
      "佳华百货控股\r\n",
      "大唐环境\r\n",
      "森信纸业集团\r\n",
      "周生生\r\n",
      "中怡国际\r\n",
      "天元医疗\r\n",
      "恒达集团控股\r\n",
      "时代集团控股\r\n",
      "慕诗国际\r\n",
      "中国艺术金融\r\n",
      "凯联国际酒店\r\n",
      "中国玻璃\r\n",
      "金山软件\r\n",
      "力量能源\r\n",
      "北京建设\r\n",
      "勒泰集团\r\n",
      "伟志控股\r\n",
      "搜狗\r\n",
      "北京汽车\r\n",
      "日月光半导体\r\n",
      "汇彩控股\r\n",
      "NANYANG HOLD\r\n",
      "元亨燃气\r\n",
      "南岸集团\r\n",
      "中芯国际\r\n",
      "永盛新材料\r\n",
      "仁恒实业控股\r\n",
      "康华医疗\r\n",
      "力劲科技\r\n",
      "国微技术\r\n",
      "迪诺斯环保\r\n",
      "达芙妮国际\r\n",
      "加达控股\r\n",
      "东京中央拍卖\r\n",
      "南方能源\r\n",
      "中国长远\r\n",
      "德林国际\r\n",
      "港大零售\r\n",
      "拉夏贝尔\r\n",
      "WKK INTL (HOLD)\r\n",
      "太和控股\r\n",
      "万辉化工\r\n",
      "卓能（集团）\r\n",
      "和谐汽车\r\n",
      "中国淀粉\r\n",
      "安全货仓\r\n",
      "保利文化\r\n",
      "裕华能源\r\n",
      "国联证券\r\n",
      "大同集团\r\n",
      "腾邦控股\r\n",
      "利福国际\r\n",
      "中原银行\r\n",
      "中国升海食品\r\n",
      "中国织材控股\r\n",
      "科联系统\r\n",
      "利亚零售\r\n",
      "嘉艺控股\r\n",
      "伯明翰体育\r\n",
      "海底捞\r\n",
      "现代牙科\r\n",
      "利记\r\n",
      "中国金融国际\r\n",
      "游莱互动\r\n",
      "天山发展控股\r\n",
      "龙记集团\r\n",
      "彩星集团\r\n",
      "ＴＯＭ集团\r\n",
      "力高集团\r\n",
      "新兴光学\r\n",
      "京城机电股份\r\n",
      "金粤控股\r\n",
      "民众金融科技\r\n",
      "广发证券\r\n",
      "新丝路文旅\r\n",
      "凤凰新媒体\r\n",
      "时间由你\r\n",
      "美联集团\r\n",
      "欢悦互娱\r\n",
      "中信大锰\r\n",
      "易居企业控股\r\n",
      "北京控股环境集团\r\n",
      "电能实业\r\n",
      "腾讯音乐\r\n",
      "徽商银行\r\n",
      "闽港控股\r\n",
      "捷荣国际控股\r\n",
      "旭日企业\r\n",
      "大自然家居\r\n",
      "高雅光学\r\n",
      "橡果国际\r\n",
      "远大医药\r\n",
      "光控精技\r\n",
      "普天通信集团\r\n",
      "AV CONCEPT HOLD\r\n",
      "昊天国际建投\r\n",
      "冠均国际控股\r\n",
      "中漆集团\r\n",
      "汉思能源\r\n",
      "猫眼娱乐\r\n",
      "玖源集团\r\n",
      "协众国际控股\r\n",
      "富道集团\r\n",
      "圣元国际\r\n",
      "联想集团\r\n",
      "华融投资股份\r\n",
      "中国华星\r\n",
      "恒投证券\r\n",
      "云米\r\n",
      "卜蜂国际\r\n",
      "稀镁科技\r\n",
      "世纪睿科\r\n",
      "万顺集团控股\r\n",
      "奥思集团\r\n",
      "世茂房地产\r\n",
      "海螺水泥\r\n",
      "龙资源\r\n",
      "中国通海金融\r\n",
      "启迪国际\r\n",
      "WANG ON GROUP\r\n",
      "志道国际\r\n",
      "九龙建业\r\n",
      "西证国际证券股权\r\n",
      "TST PROPERTIES\r\n",
      "新特能源\r\n",
      "华视传媒\r\n",
      "英恒科技\r\n",
      "米格国际控股\r\n",
      "南顺（香港）\r\n",
      "富元国际集团\r\n",
      "世纪城市国际\r\n",
      "方正控股\r\n",
      "一嗨租车\r\n",
      "宜人贷\r\n",
      "东银国际控股\r\n",
      "维信金科\r\n",
      "南海控股\r\n",
      "协同通信\r\n",
      "嘉利国际\r\n",
      "恒兴黄金\r\n",
      "汇银控股集团\r\n",
      "壹传媒\r\n",
      "澳能建设\r\n",
      "兴合控股\r\n",
      "首长宝佳\r\n",
      "第七大道\r\n",
      "丽新发展\r\n",
      "途牛\r\n",
      "中国铁塔\r\n",
      "德泰新能源集团\r\n",
      "亲亲食品\r\n",
      "SHANGHAI GROWTH\r\n",
      "大家乐集团\r\n",
      "绿科科技国际\r\n",
      "康利国际控股\r\n",
      "时富金融服务集团\r\n",
      "中国金融发展\r\n",
      "大湾区投资控股\r\n",
      "传递娱乐\r\n",
      "超大现代\r\n",
      "汇能集团\r\n",
      "荣晖国际\r\n",
      "顺兴集团控股\r\n",
      "思捷环球\r\n",
      "华隆金控\r\n",
      "城建设计\r\n",
      "金鹰商贸集团\r\n",
      "五龙动力\r\n",
      "中国国航\r\n",
      "YGM TRADING\r\n",
      "重庆机电\r\n",
      "阳光油砂\r\n",
      "香港建设（控股）\r\n",
      "百宏实业\r\n",
      "德信中国\r\n",
      "中国机械工程\r\n",
      "北控医疗健康\r\n",
      "圆通速递国际\r\n",
      "大唐新能源\r\n",
      "触宝\r\n",
      "YUGANG INT'L\r\n",
      "美联工商铺\r\n",
      "台积电\r\n",
      "协合新能源\r\n",
      "秦港股份\r\n",
      "高鹏矿业\r\n",
      "陌陌\r\n",
      "京东\r\n",
      "维珍妮\r\n",
      "建德国际控股\r\n",
      "威讯控股\r\n",
      "摩比发展\r\n",
      "创美药业\r\n",
      "鲈乡小贷\r\n",
      "融创中国\r\n",
      "中地乳业\r\n",
      "成都高速\r\n",
      "中国宝力科技\r\n",
      "笔克远东\r\n",
      "中国物流资产\r\n",
      "希尼亚\r\n",
      "大新银行集团\r\n",
      "晶苑国际\r\n",
      "震雄集团\r\n",
      "伟仕佳杰\r\n",
      "盈大地产\r\n",
      "MAYER HOLDINGS\r\n",
      "海峡石油化工\r\n",
      "亿仕登控股\r\n",
      "润中国际控股\r\n",
      "中石化\r\n",
      "诺亚财富\r\n",
      "蓝鼎国际\r\n",
      "义合控股\r\n",
      "誉宴集团\r\n",
      "悦达国际控股\r\n",
      "新沣集团一九零七\r\n",
      "九号运通\r\n",
      "康大食品\r\n",
      "达利国际\r\n",
      "力宝\r\n",
      "永丰集团控股\r\n",
      "彩客化学\r\n",
      "铁建装备\r\n",
      "TAI PING CARPET\r\n",
      "意科控股\r\n",
      "中国汽车内饰\r\n",
      "长安民生物流\r\n",
      "飞毛腿\r\n",
      "环球大通投资\r\n",
      "中国油气控股\r\n",
      "结好控股\r\n",
      "卜蜂莲花\r\n",
      "新工投资\r\n",
      "中国神华\r\n",
      "HPC HOLDINGS\r\n",
      "中华国际\r\n",
      "泰加保险\r\n",
      "ALCO HOLDINGS\r\n",
      "荣阳实业\r\n",
      "中国投资开发－新\r\n",
      "奥玛仕国际\r\n",
      "国浩集团\r\n",
      "荣智控股\r\n",
      "新奥混凝土\r\n",
      "高阳科技\r\n",
      "中远海运国际\r\n",
      "易易壹金融\r\n",
      "伟禄集团\r\n",
      "超威动力\r\n",
      "欧化\r\n",
      "和嘉控股\r\n",
      "贵联控股\r\n",
      "中泛控股\r\n",
      "巨匠建设\r\n",
      "亚伦国际\r\n",
      "中国三江化工\r\n",
      "飞鱼科技\r\n",
      "高丰集团控股\r\n",
      "华滋国际海洋\r\n",
      "奥星生命科技\r\n",
      "恒富控股\r\n",
      "瑞声科技\r\n",
      "春立医疗\r\n",
      "IBI GROUP HLDGS\r\n",
      "西证国际证券\r\n",
      "中国力鸿\r\n",
      "东瑞制药\r\n",
      "英皇证券\r\n",
      "华津国际控股\r\n",
      "载通\r\n",
      "绿领控股\r\n",
      "博实乐\r\n",
      "中国铝业\r\n",
      "中国基建投资\r\n",
      "天业节水\r\n",
      "正大企业国际\r\n",
      "新确科技\r\n",
      "金辉集团\r\n",
      "中国智慧能源\r\n",
      "裕田中国\r\n",
      "中昌国际控股\r\n",
      "中油燃气\r\n",
      "香港教育国际\r\n",
      "中广核矿业\r\n",
      "格林国际控股－旧\r\n",
      "新城市建设发展\r\n",
      "广汽集团\r\n",
      "黛丽斯国际\r\n",
      "中智药业\r\n",
      "北大资源\r\n",
      "澳科控股\r\n",
      "峨眉山A\r\n",
      "伟星股份\r\n",
      "百大集团\r\n",
      "洁美科技\r\n",
      "南玻A\r\n",
      "龙大肉食\r\n",
      "中能电气\r\n",
      "新五丰\r\n",
      "捷捷微电\r\n",
      "卫光生物\r\n",
      "锐科激光\r\n",
      "神奇制药\r\n",
      "联创互联\r\n",
      "上实发展\r\n",
      "亿利洁能\r\n",
      "江龙船艇\r\n",
      "新凤鸣\r\n",
      "开立医疗\r\n",
      "贵阳银行\r\n",
      "京新药业\r\n",
      "汉钟精机\r\n",
      "柘中股份\r\n",
      "威星智能\r\n",
      "美克家居\r\n",
      "神州数码\r\n",
      "通光线缆\r\n",
      "凯发电气\r\n",
      "泰尔股份\r\n",
      "阳光电源\r\n",
      "赣锋锂业\r\n",
      "厦门港务\r\n",
      "健友股份\r\n",
      "阿石创\r\n",
      "金浦钛业\r\n",
      "深信服\r\n",
      "启明星辰\r\n",
      "上海凤凰\r\n",
      "新城市\r\n",
      "易联众\r\n",
      "华林证券\r\n",
      "山河智能\r\n",
      "新华医疗\r\n",
      "芭田股份\r\n",
      "光库科技\r\n",
      "世纪鼎利\r\n",
      "永安药业\r\n",
      "向日葵\r\n",
      "国林环保\r\n",
      "金圆股份\r\n",
      "信邦制药\r\n",
      "同济科技\r\n",
      "汉威科技\r\n",
      "中远海控\r\n",
      "京能置业\r\n",
      "科华生物\r\n",
      "普路通\r\n",
      "我乐家居\r\n",
      "光一科技\r\n",
      "北部湾港\r\n",
      "三元股份\r\n",
      "香飘飘\r\n",
      "嘉事堂\r\n",
      "高争民爆\r\n",
      "华发股份\r\n",
      "得润电子\r\n",
      "珍宝岛\r\n",
      "景峰医药\r\n",
      "碳元科技\r\n",
      "奥飞娱乐\r\n",
      "光启技术\r\n",
      "天沃科技\r\n",
      "新澳股份\r\n",
      "大通燃气\r\n",
      "司尔特\r\n",
      "云意电气\r\n",
      "江苏国信\r\n",
      "杰恩设计\r\n",
      "太阳电缆\r\n",
      "三钢闽光\r\n",
      "川能动力\r\n",
      "达威股份\r\n",
      "众生药业\r\n",
      "天永智能\r\n",
      "京威股份\r\n",
      "四环生物\r\n",
      "中亚股份\r\n",
      "瑞泰科技\r\n",
      "富瑞特装\r\n",
      "利亚德\r\n",
      "万邦达\r\n",
      "绿康生化\r\n",
      "华数传媒\r\n",
      "金陵体育\r\n",
      "云赛智联\r\n",
      "宏昌电子\r\n",
      "金地集团\r\n",
      "长江通信\r\n",
      "宁波富达\r\n",
      "西部材料\r\n",
      "雪峰科技\r\n",
      "中恒集团\r\n",
      "甘咨询\r\n",
      "鲁商发展\r\n",
      "皖通科技\r\n",
      "东晶电子\r\n",
      "奋达科技\r\n",
      "招商公路\r\n",
      "中国神华\r\n",
      "中国武夷\r\n",
      "华兰生物\r\n",
      "华微电子\r\n",
      "英维克\r\n",
      "吉林森工\r\n",
      "博通股份\r\n",
      "中关村\r\n",
      "蓝黛传动\r\n",
      "深桑达A\r\n",
      "宏达新材\r\n",
      "数字政通\r\n",
      "国电电力\r\n",
      "创新医疗\r\n",
      "上海家化\r\n",
      "湖南黄金\r\n",
      "青龙管业\r\n",
      "新疆天业\r\n",
      "中工国际\r\n",
      "航天长峰\r\n",
      "宇晶股份\r\n",
      "科力尔\r\n",
      "大禹节水\r\n",
      "利通电子\r\n",
      "东方铁塔\r\n",
      "兴业银行\r\n",
      "吉艾科技\r\n",
      "华铁股份\r\n",
      "宁波精达\r\n",
      "白云山\r\n",
      "华锋股份\r\n",
      "龙源技术\r\n",
      "延江股份\r\n",
      "盐田港\r\n",
      "引力传媒\r\n",
      "金枫酒业\r\n",
      "豪迈科技\r\n",
      "唐人神\r\n",
      "精准信息\r\n",
      "曙光股份\r\n",
      "远光软件\r\n",
      "依米康\r\n",
      "中原内配\r\n",
      "通鼎互联\r\n",
      "掌趣科技\r\n",
      "宏达电子\r\n",
      "上海银行\r\n",
      "悦心健康\r\n",
      "滨海能源\r\n",
      "华银电力\r\n",
      "银江股份\r\n",
      "顺利办\r\n",
      "维宏股份\r\n",
      "红蜻蜓\r\n",
      "惠而浦\r\n",
      "杭电股份\r\n",
      "陕国投A\r\n",
      "搜于特\r\n",
      "宜通世纪\r\n",
      "日盈电子\r\n",
      "楚天科技\r\n",
      "华北高速\r\n",
      "华致酒行\r\n",
      "模塑科技\r\n",
      "利安隆\r\n",
      "中环装备\r\n",
      "兴业科技\r\n",
      "深科技\r\n",
      "深振业A\r\n",
      "电子城\r\n",
      "金一文化\r\n",
      "万里马\r\n",
      "中国应急\r\n",
      "东方中科\r\n",
      "中原高速\r\n",
      "新安股份\r\n",
      "科迪乳业\r\n",
      "梅泰诺\r\n",
      "江苏索普\r\n",
      "华孚时尚\r\n",
      "科华恒盛\r\n",
      "森霸传感\r\n",
      "科大讯飞\r\n",
      "中远海发\r\n",
      "今世缘\r\n",
      "鹏鹞环保\r\n",
      "元隆雅图\r\n",
      "亚太实业\r\n",
      "光明乳业\r\n",
      "长青集团\r\n",
      "瑞特股份\r\n",
      "伊之密\r\n",
      "太龙照明\r\n",
      "泰晶科技\r\n",
      "方盛制药\r\n",
      "腾龙股份\r\n",
      "沙河股份\r\n",
      "海南瑞泽\r\n",
      "*ST三维\r\n",
      "京东方A\r\n",
      "新城控股\r\n",
      "智动力\r\n",
      "粤宏远A\r\n",
      "二三四五\r\n",
      "航发控制\r\n",
      "新华都\r\n",
      "宁波港\r\n",
      "江苏雷利\r\n",
      "博腾股份\r\n",
      "粤传媒\r\n",
      "合康新能\r\n",
      "江铃汽车\r\n",
      "格尔软件\r\n",
      "厦门象屿\r\n",
      "新华制药\r\n",
      "特锐德\r\n",
      "鞍重股份\r\n",
      "名臣健康\r\n",
      "洪都航空\r\n",
      "漳泽电力\r\n",
      "来伊份\r\n",
      "日播时尚\r\n",
      "新晨科技\r\n",
      "王子新材\r\n",
      "双汇发展\r\n",
      "博济医药\r\n",
      "东港股份\r\n",
      "润达医疗\r\n",
      "明阳智能\r\n",
      "软控股份\r\n",
      "云南铜业\r\n",
      "兆日科技\r\n",
      "爱尔眼科\r\n",
      "ST中安\r\n",
      "天泽信息\r\n",
      "神开股份\r\n",
      "航天动力\r\n",
      "泰禾光电\r\n",
      "新日股份\r\n",
      "矩子科技\r\n",
      "新华传媒\r\n",
      "航天科技\r\n",
      "人福医药\r\n",
      "柳药股份\r\n",
      "广宇发展\r\n",
      "中威电子\r\n",
      "杉杉股份\r\n",
      "万东医疗\r\n",
      "厚普股份\r\n",
      "苏州银行\r\n",
      "中百集团\r\n",
      "中国船舶\r\n",
      "汇鸿集团\r\n",
      "益丰药房\r\n",
      "巨人网络\r\n",
      "沃特股份\r\n",
      "罗莱生活\r\n",
      "钱江生化\r\n",
      "中化国际\r\n",
      "达安基因\r\n",
      "安纳达\r\n",
      "一汽夏利\r\n",
      "安靠智电\r\n",
      "珈伟新能\r\n",
      "美的集团\r\n",
      "得利斯\r\n",
      "恒立实业\r\n",
      "文投控股\r\n",
      "北斗星通\r\n",
      "天地数码\r\n",
      "澄天伟业\r\n",
      "上海机场\r\n",
      "常山北明\r\n",
      "银信科技\r\n",
      "路桥建设\r\n",
      "罗平锌电\r\n",
      "神思电子\r\n",
      "金诚信\r\n",
      "飞力达\r\n",
      "值得买\r\n",
      "晨光生物\r\n",
      "康德莱\r\n",
      "国美通讯\r\n",
      "中国天楹\r\n",
      "圣邦股份\r\n",
      "光环新网\r\n",
      "朗科科技\r\n",
      "汉缆股份\r\n",
      "先进数通\r\n",
      "八菱科技\r\n",
      "万里扬\r\n",
      "耀皮玻璃\r\n",
      "长荣股份\r\n",
      "绝味食品\r\n",
      "创业黑马\r\n",
      "金隅集团\r\n",
      "福建水泥\r\n",
      "东方创业\r\n",
      "福建高速\r\n",
      "申通地铁\r\n",
      "恒信东方\r\n",
      "永艺股份\r\n",
      "金种子酒\r\n",
      "深圳华强\r\n",
      "仁东控股\r\n",
      "天赐材料\r\n",
      "丽珠集团\r\n",
      "豫园股份\r\n",
      "海联讯\r\n",
      "荣盛发展\r\n",
      "鸣志电器\r\n",
      "司太立\r\n",
      "西水股份\r\n",
      "蒙娜丽莎\r\n",
      "山推股份\r\n",
      "华润双鹤\r\n",
      "中元股份\r\n",
      "协鑫集成\r\n",
      "吉比特\r\n",
      "东南网架\r\n",
      "浙江永强\r\n",
      "烽火电子\r\n",
      "太安堂\r\n",
      "合纵科技\r\n",
      "蔚蓝生物\r\n",
      "正泰电器\r\n",
      "*ST华信\r\n",
      "数码科技\r\n",
      "高鸿股份\r\n",
      "华帝股份\r\n",
      "海能达\r\n",
      "爱司凯\r\n",
      "天圣制药\r\n",
      "朗科智能\r\n",
      "华升股份\r\n",
      "博深股份\r\n",
      "电工合金\r\n",
      "长春经开\r\n",
      "京天利\r\n",
      "欣锐科技\r\n",
      "承德钒钛\r\n",
      "京华激光\r\n",
      "黄山胶囊\r\n",
      "*ST成城\r\n",
      "扬农化工\r\n",
      "启明信息\r\n",
      "银河电子\r\n",
      "海正药业\r\n",
      "九华旅游\r\n",
      "首开股份\r\n",
      "青岛金王\r\n",
      "国立科技\r\n",
      "信雅达\r\n",
      "蓝帆医疗\r\n",
      "ST明科\r\n",
      "民德电子\r\n",
      "有友食品\r\n",
      "顾家家居\r\n",
      "上海电气\r\n",
      "苏宁易购\r\n",
      "方正证券\r\n",
      "泰永长征\r\n",
      "三峡水利\r\n",
      "英搏尔\r\n",
      "华宏科技\r\n",
      "迪威迅\r\n",
      "银之杰\r\n",
      "明阳电路\r\n",
      "国睿科技\r\n",
      "继峰股份\r\n",
      "凯莱英\r\n",
      "汇川技术\r\n",
      "国瓷材料\r\n",
      "钱江摩托\r\n",
      "华瑞股份\r\n",
      "中国外运\r\n",
      "浙江东日\r\n",
      "海航控股\r\n",
      "天舟文化\r\n",
      "GQY视讯\r\n",
      "日月股份\r\n",
      "南极电商\r\n",
      "远兴能源\r\n",
      "青松建化\r\n",
      "科陆电子\r\n",
      "中化岩土\r\n",
      "南京医药\r\n",
      "百隆东方\r\n",
      "初灵信息\r\n",
      "福能股份\r\n",
      "清水源\r\n",
      "首旅酒店\r\n",
      "世嘉科技\r\n",
      "原尚股份\r\n",
      "沃格光电\r\n",
      "新易盛\r\n",
      "隆利科技\r\n",
      "名家汇\r\n",
      "郑中设计\r\n",
      "南京熊猫\r\n",
      "普利制药\r\n",
      "新劲刚\r\n",
      "赛为智能\r\n",
      "万孚生物\r\n",
      "四通股份\r\n",
      "汉王科技\r\n",
      "岱美股份\r\n",
      "赛腾股份\r\n",
      "复旦复华\r\n",
      "三星医疗\r\n",
      "中曼石油\r\n",
      "宁波建工\r\n",
      "海信电器\r\n",
      "甘肃电投\r\n",
      "远东传动\r\n",
      "天神娱乐\r\n",
      "赛升药业\r\n",
      "生物股份\r\n",
      "徐工机械\r\n",
      "崇达技术\r\n",
      "重庆路桥\r\n",
      "奥马电器\r\n",
      "剑桥科技\r\n",
      "圣农发展\r\n",
      "远方信息\r\n",
      "川环科技\r\n",
      "金逸影视\r\n",
      "思特奇\r\n",
      "中国重工\r\n",
      "海量数据\r\n",
      "东方航空\r\n",
      "华平股份\r\n",
      "东百集团\r\n",
      "广西广电\r\n",
      "紫鑫药业\r\n",
      "朗新科技\r\n",
      "松芝股份\r\n",
      "开创国际\r\n",
      "安科生物\r\n",
      "合金投资\r\n",
      "桂林旅游\r\n",
      "东阿阿胶\r\n",
      "银禧科技\r\n",
      "合肥城建\r\n",
      "恒锋工具\r\n",
      "新开源\r\n",
      "驰宏锌锗\r\n",
      "紫金银行\r\n",
      "超图软件\r\n",
      "四川长虹\r\n",
      "中贝通信\r\n",
      "大族激光\r\n",
      "金财互联\r\n",
      "中兵红箭\r\n",
      "雅本化学\r\n",
      "耐威科技\r\n",
      "太极集团\r\n",
      "紫天科技\r\n",
      "杰赛科技\r\n",
      "集智股份\r\n",
      "长电科技\r\n",
      "豫金刚石\r\n",
      "中国汽研\r\n",
      "福耀玻璃\r\n",
      "中金环境\r\n",
      "集友股份\r\n",
      "中粮科技\r\n",
      "克来机电\r\n",
      "建设银行\r\n",
      "兰州黄河\r\n",
      "纳思达\r\n",
      "惠发股份\r\n",
      "迪瑞医疗\r\n",
      "龙马环卫\r\n",
      "德艺文创\r\n",
      "久其软件\r\n",
      "永安行\r\n",
      "珠海港\r\n",
      "兄弟科技\r\n",
      "震安科技\r\n",
      "松发股份\r\n",
      "龙净环保\r\n",
      "宝莱特\r\n",
      "白云机场\r\n",
      "沈阳机床\r\n",
      "高新兴\r\n",
      "汇源通信\r\n",
      "金运激光\r\n",
      "大同煤业\r\n",
      "美好置业\r\n",
      "赢时胜\r\n",
      "井神股份\r\n",
      "宇环数控\r\n",
      "吉祥航空\r\n",
      "陕西金叶\r\n",
      "电广传媒\r\n",
      "诚意药业\r\n",
      "雷迪克\r\n",
      "湖北广电\r\n",
      "中国联通\r\n",
      "乐视网\r\n",
      "中装建设\r\n",
      "星网锐捷\r\n",
      "数源科技\r\n",
      "中环股份\r\n",
      "海油工程\r\n",
      "伊力特\r\n",
      "腾信股份\r\n",
      "众应互联\r\n",
      "华映科技\r\n",
      "茂业通信\r\n",
      "智光电气\r\n",
      "芒果超媒\r\n",
      "宝泰隆\r\n",
      "太极股份\r\n",
      "安井食品\r\n",
      "国风塑业\r\n",
      "福瑞股份\r\n",
      "惠博普\r\n",
      "惠泉啤酒\r\n",
      "恒林股份\r\n",
      "中设股份\r\n",
      "景旺电子\r\n",
      "精达股份\r\n",
      "雷曼光电\r\n",
      "昊志机电\r\n",
      "中国出版\r\n",
      "深天地A\r\n",
      "五矿稀土\r\n",
      "迈克生物\r\n",
      "豫能控股\r\n",
      "益民集团\r\n",
      "人民同泰\r\n",
      "拓斯达\r\n",
      "至纯科技\r\n",
      "法尔胜\r\n",
      "栖霞建设\r\n",
      "仙乐健康\r\n",
      "华铁应急\r\n",
      "阳光城\r\n",
      "高新发展\r\n",
      "利尔化学\r\n",
      "万达电影\r\n",
      "*ST狮头\r\n",
      "思源电气\r\n",
      "扬帆新材\r\n",
      "卓翼科技\r\n",
      "上海贝岭\r\n",
      "御银股份\r\n",
      "博思软件\r\n",
      "康泰生物\r\n",
      "加加食品\r\n",
      "远望谷\r\n",
      "天华超净\r\n",
      "新疆交建\r\n",
      "集泰股份\r\n",
      "杭萧钢构\r\n",
      "赛隆药业\r\n",
      "*ST富控\r\n",
      "东方电热\r\n",
      "湖南投资\r\n",
      "城地股份\r\n",
      "幸福蓝海\r\n",
      "国科微\r\n",
      "环球印务\r\n",
      "证通电子\r\n",
      "天成自控\r\n",
      "西陇科学\r\n",
      "中国动力\r\n",
      "安利股份\r\n",
      "ST山水\r\n",
      "东阳光\r\n",
      "晓程科技\r\n",
      "中润资源\r\n",
      "ST岩石\r\n",
      "凯文教育\r\n",
      "富春环保\r\n",
      "雄韬股份\r\n",
      "福建金森\r\n",
      "浙商银行\r\n",
      "辉丰股份\r\n",
      "联络互动\r\n",
      "晶方科技\r\n",
      "亚士创能\r\n",
      "鲁信创投\r\n",
      "昂立教育\r\n",
      "智慧能源\r\n",
      "中国重汽\r\n",
      "永吉股份\r\n",
      "山东赫达\r\n",
      "东方雨虹\r\n",
      "三花智控\r\n",
      "出版传媒\r\n",
      "华谊集团\r\n",
      "罗牛山\r\n",
      "中再资环\r\n",
      "张裕A\r\n",
      "信立泰\r\n",
      "沃华医药\r\n",
      "航发科技\r\n",
      "金奥博\r\n",
      "羚锐制药\r\n",
      "北新路桥\r\n",
      "苏泊尔\r\n",
      "宏发股份\r\n",
      "海达股份\r\n",
      "白银有色\r\n",
      "信达地产\r\n",
      "中衡设计\r\n",
      "美联新材\r\n",
      "圣达生物\r\n",
      "超声电子\r\n",
      "万丰奥威\r\n",
      "澳柯玛\r\n",
      "艾华集团\r\n",
      "浙江美大\r\n",
      "内蒙一机\r\n",
      "正海生物\r\n",
      "东方通\r\n",
      "中富通\r\n",
      "保隆科技\r\n",
      "蓝思科技\r\n",
      "富满电子\r\n",
      "古越龙山\r\n",
      "奥特佳\r\n",
      "重庆港九\r\n",
      "秦安股份\r\n",
      "北方股份\r\n",
      "凯龙股份\r\n",
      "路畅科技\r\n",
      "中信银行\r\n",
      "厦门信达\r\n",
      "硅宝科技\r\n",
      "厦门空港\r\n",
      "大豪科技\r\n",
      "华昌达\r\n",
      "德尔股份\r\n",
      "迪安诊断\r\n",
      "佳禾智能\r\n",
      "诺邦股份\r\n",
      "新华文轩\r\n",
      "卧龙电驱\r\n",
      "安信信托\r\n",
      "金卡智能\r\n",
      "康尼机电\r\n",
      "华测检测\r\n",
      "兔宝宝\r\n",
      "中体产业\r\n",
      "恒铭达\r\n",
      "东软集团\r\n",
      "恒锋信息\r\n",
      "三安光电\r\n",
      "星网宇达\r\n",
      "焦作万方\r\n",
      "华大基因\r\n",
      "信威集团\r\n",
      "陆家嘴\r\n",
      "国际实业\r\n",
      "思美传媒\r\n",
      "开开实业\r\n",
      "苏交科\r\n",
      "山东墨龙\r\n",
      "麦克奥迪\r\n",
      "中颖电子\r\n",
      "蒙草生态\r\n",
      "红阳能源\r\n",
      "康芝药业\r\n",
      "东华测试\r\n",
      "乐山电力\r\n",
      "兆驰股份\r\n",
      "开能健康\r\n",
      "中国中铁\r\n",
      "华鹏飞\r\n",
      "*ST南风\r\n",
      "道道全\r\n",
      "金通灵\r\n",
      "龙江交通\r\n",
      "迅游科技\r\n",
      "德美化工\r\n",
      "利君股份\r\n",
      "丰华股份\r\n",
      "瀚叶股份\r\n",
      "*ST上普\r\n",
      "三诺生物\r\n",
      "亚星客车\r\n",
      "万里石\r\n",
      "迪贝电气\r\n",
      "交运股份\r\n",
      "大唐发电\r\n",
      "欣旺达\r\n",
      "中国卫星\r\n",
      "深中华A\r\n",
      "中材国际\r\n",
      "胜宏科技\r\n",
      "永泰能源\r\n",
      "海尔智家\r\n",
      "广电计量\r\n",
      "千方科技\r\n",
      "明泰铝业\r\n",
      "润建通信\r\n",
      "淳中科技\r\n",
      "菲林格尔\r\n",
      "金科文化\r\n",
      "杭州高新\r\n",
      "旺能环境\r\n",
      "雅克科技\r\n",
      "正业科技\r\n",
      "吉电股份\r\n",
      "潍柴动力\r\n",
      "科达利\r\n",
      "大恒科技\r\n",
      "华谊兄弟\r\n",
      "旷达科技\r\n",
      "中国一重\r\n",
      "唐源电气\r\n",
      "九典制药\r\n",
      "孚日股份\r\n",
      "百润股份\r\n",
      "蓝盾股份\r\n",
      "重庆钢铁\r\n",
      "海天味业\r\n",
      "华安证券\r\n",
      "海欣食品\r\n",
      "融捷股份\r\n",
      "雷柏科技\r\n",
      "爱普股份\r\n",
      "天玑科技\r\n",
      "京泉华\r\n",
      "汤臣倍健\r\n",
      "万盛股份\r\n",
      "美邦服饰\r\n",
      "獐子岛\r\n",
      "昆药集团\r\n",
      "卫宁健康\r\n",
      "国金证券\r\n",
      "宏创控股\r\n",
      "王府井\r\n",
      "伟星新材\r\n",
      "美亚柏科\r\n",
      "农业银行\r\n",
      "先锋电子\r\n",
      "三丰智能\r\n",
      "法拉电子\r\n",
      "溢多利\r\n",
      "海澜之家\r\n",
      "瑞达期货\r\n",
      "弘信电子\r\n",
      "中国人保\r\n",
      "文峰股份\r\n",
      "银泰黄金\r\n",
      "国信证券\r\n",
      "联创电子\r\n",
      "神州高铁\r\n",
      "国泰君安\r\n",
      "贵州轮胎\r\n",
      "万兴科技\r\n",
      "劲拓股份\r\n",
      "三六五网\r\n",
      "阳光照明\r\n",
      "纽威股份\r\n",
      "秦港股份\r\n",
      "万林物流\r\n",
      "正丹股份\r\n",
      "景嘉微\r\n",
      "吉翔股份\r\n",
      "三友化工\r\n",
      "精华制药\r\n",
      "西南证券\r\n",
      "保利地产\r\n",
      "海南橡胶\r\n",
      "宏川智慧\r\n",
      "克明面业\r\n",
      "一汽富维\r\n",
      "北大医药\r\n",
      "美锦能源\r\n",
      "中电环保\r\n",
      "海峡环保\r\n",
      "杭州银行\r\n",
      "威海广泰\r\n",
      "中通国脉\r\n",
      "丰元股份\r\n",
      "中超控股\r\n",
      "佩蒂股份\r\n",
      "第一创业\r\n",
      "容大感光\r\n",
      "轻纺城\r\n",
      "省广集团\r\n",
      "同德化工\r\n",
      "阿科力\r\n",
      "广济药业\r\n",
      "双环传动\r\n",
      "涪陵电力\r\n",
      "皖维高新\r\n",
      "中国太保\r\n",
      "传化智联\r\n",
      "智度股份\r\n",
      "涪陵榨菜\r\n",
      "美康生物\r\n",
      "顶点软件\r\n",
      "多氟多\r\n",
      "庄园牧场\r\n",
      "龙星化工\r\n",
      "万业企业\r\n",
      "联化科技\r\n",
      "银宝山新\r\n",
      "紫光国微\r\n",
      "穗恒运A\r\n",
      "金牛化工\r\n",
      "辉煌科技\r\n",
      "大连圣亚\r\n",
      "维信诺\r\n",
      "东北证券\r\n",
      "视觉中国\r\n",
      "中天能源\r\n",
      "兴发集团\r\n",
      "中京电子\r\n",
      "新金路\r\n",
      "可立克\r\n",
      "妙可蓝多\r\n",
      "拓维信息\r\n",
      "春秋电子\r\n",
      "荣科科技\r\n",
      "宁波海运\r\n",
      "北大荒\r\n",
      "粤水电\r\n",
      "荣华实业\r\n",
      "密尔克卫\r\n",
      "大连电瓷\r\n",
      "鸿合科技\r\n",
      "金力泰\r\n",
      "大北农\r\n",
      "机器人\r\n",
      "锋龙股份\r\n",
      "北矿科技\r\n",
      "中国铝业\r\n",
      "五矿资本\r\n",
      "西部牧业\r\n",
      "开元股份\r\n",
      "顺络电子\r\n",
      "中航重机\r\n",
      "首钢股份\r\n",
      "赢合科技\r\n",
      "中农立华\r\n",
      "德方纳米\r\n",
      "海鸥住工\r\n",
      "通化东宝\r\n",
      "欧菲光\r\n",
      "浙江龙盛\r\n",
      "国新能源\r\n",
      "奥翔药业\r\n",
      "美芝股份\r\n",
      "久吾高科\r\n",
      "当升科技\r\n",
      "天保基建\r\n",
      "立华股份\r\n",
      "江山欧派\r\n",
      "数字认证\r\n",
      "盛屯矿业\r\n",
      "祁连山\r\n",
      "思维列控\r\n",
      "双成药业\r\n",
      "维尔利\r\n",
      "桂发祥\r\n",
      "傲农生物\r\n",
      "松霖科技\r\n",
      "和晶科技\r\n",
      "盛天网络\r\n",
      "洲明科技\r\n",
      "星湖科技\r\n",
      "金刚玻璃\r\n",
      "大庆华科\r\n",
      "物产中大\r\n",
      "广深铁路\r\n",
      "九芝堂\r\n",
      "晨化股份\r\n",
      "海亮股份\r\n",
      "华统股份\r\n",
      "中南建设\r\n",
      "长缆科技\r\n",
      "誉衡药业\r\n",
      "国民技术\r\n",
      "国星光电\r\n",
      "盛洋科技\r\n",
      "东风股份\r\n",
      "易德龙\r\n",
      "嘉麟杰\r\n",
      "上海凯宝\r\n",
      "浙江东方\r\n",
      "赛摩电气\r\n",
      "隆盛科技\r\n",
      "安控科技\r\n",
      "恩华药业\r\n",
      "上海天洋\r\n",
      "蓝色光标\r\n",
      "中科海讯\r\n",
      "海通证券\r\n",
      "华西证券\r\n",
      "通海高科\r\n",
      "中国国旅\r\n",
      "中山公用\r\n",
      "君正集团\r\n",
      "宝馨科技\r\n",
      "大亚圣象\r\n",
      "大华股份\r\n",
      "文化长城\r\n",
      "青农商行\r\n",
      "华闻传媒\r\n",
      "三利谱\r\n",
      "申达股份\r\n",
      "全筑股份\r\n",
      "大悦城\r\n",
      "指南针\r\n",
      "天喻信息\r\n",
      "上海九百\r\n",
      "北辰实业\r\n",
      "恒银金融\r\n",
      "安记食品\r\n",
      "艾比森\r\n",
      "沃施股份\r\n",
      "华宝股份\r\n",
      "惠达卫浴\r\n",
      "同洲电子\r\n",
      "西藏天路\r\n",
      "航新科技\r\n",
      "奥维通信\r\n",
      "丰林集团\r\n",
      "网宿科技\r\n",
      "科斯伍德\r\n",
      "春秋航空\r\n",
      "乐心医疗\r\n",
      "中粮糖业\r\n",
      "特发信息\r\n",
      "振芯科技\r\n",
      "S佳通\r\n",
      "华夏银行\r\n",
      "海思科\r\n",
      "顺威股份\r\n",
      "五洋停车\r\n",
      "浙江仙通\r\n",
      "煌上煌\r\n",
      "中钢天源\r\n",
      "世龙实业\r\n",
      "雪榕生物\r\n",
      "飞鹿股份\r\n",
      "润欣科技\r\n",
      "万和电气\r\n",
      "天能重工\r\n",
      "长安汽车\r\n",
      "天邑股份\r\n",
      "海特高新\r\n",
      "欣泰退\r\n",
      "国电南自\r\n",
      "新乳业\r\n",
      "华锦股份\r\n",
      "博彦科技\r\n",
      "新开普\r\n",
      "*ST工新\r\n",
      "天音控股\r\n",
      "华森制药\r\n",
      "伟隆股份\r\n",
      "卫星石化\r\n",
      "科隆股份\r\n",
      "三环集团\r\n",
      "航民股份\r\n",
      "飞亚达A\r\n",
      "启迪环境\r\n",
      "康盛股份\r\n",
      "得邦照明\r\n",
      "中储股份\r\n",
      "珀莱雅\r\n",
      "西部证券\r\n",
      "天士力\r\n",
      "新经典\r\n",
      "建投能源\r\n",
      "湖南发展\r\n",
      "骆驼股份\r\n",
      "粤高速A\r\n",
      "天翔环境\r\n",
      "津滨发展\r\n",
      "红太阳\r\n",
      "易明医药\r\n",
      "五方光电\r\n",
      "中顺洁柔\r\n",
      "世纪天鸿\r\n",
      "华金资本\r\n",
      "深天马A\r\n",
      "韵达股份\r\n",
      "田中精机\r\n",
      "东软载波\r\n",
      "雪迪龙\r\n",
      "兴业证券\r\n",
      "帝欧家居\r\n",
      "顺网科技\r\n",
      "亚威股份\r\n",
      "南方轴承\r\n",
      "安道麦A\r\n",
      "兴瑞科技\r\n",
      "申华控股\r\n",
      "佳云科技\r\n",
      "京运通\r\n",
      "中源家居\r\n",
      "我武生物\r\n",
      "小康股份\r\n",
      "深南电路\r\n",
      "美都能源\r\n",
      "朗姿股份\r\n",
      "尚品宅配\r\n",
      "江河集团\r\n",
      "正裕工业\r\n",
      "国恒退\r\n",
      "广州酒家\r\n",
      "麦达数字\r\n",
      "天桥起重\r\n",
      "石英股份\r\n",
      "葛洲坝\r\n",
      "金发拉比\r\n",
      "湘邮科技\r\n",
      "中国海防\r\n",
      "长青股份\r\n",
      "杭州园林\r\n",
      "飞凯材料\r\n",
      "电科院\r\n",
      "维维股份\r\n",
      "慈铭体检\r\n",
      "南京港\r\n",
      "新疆众和\r\n",
      "中国中车\r\n",
      "凯众股份\r\n",
      "会畅通讯\r\n",
      "永新股份\r\n",
      "黑芝麻\r\n",
      "*ST上航\r\n",
      "鲁泰A\r\n",
      "三全食品\r\n",
      "麦格米特\r\n",
      "平治信息\r\n",
      "千金药业\r\n",
      "华北制药\r\n",
      "中色股份\r\n",
      "东方能源\r\n",
      "新宏泽\r\n",
      "金城医药\r\n",
      "连云港\r\n",
      "天顺风能\r\n",
      "中远海特\r\n",
      "威帝股份\r\n",
      "华丽家族\r\n",
      "华联综超\r\n",
      "雪莱特\r\n",
      "新筑股份\r\n",
      "晨曦航空\r\n",
      "中持股份\r\n",
      "博威合金\r\n",
      "科融环境\r\n",
      "国机汽车\r\n",
      "盈康生命\r\n",
      "华铭智能\r\n",
      "吉峰科技\r\n",
      "首航高科\r\n",
      "威华股份\r\n",
      "大立科技\r\n",
      "北方华创\r\n",
      "上海环境\r\n",
      "中国广核\r\n",
      "北京文化\r\n",
      "金龙汽车\r\n",
      "欢瑞世纪\r\n",
      "青岛中程\r\n",
      "科泰电源\r\n",
      "开滦股份\r\n",
      "乐凯胶片\r\n",
      "日辰股份\r\n",
      "中欣氟材\r\n",
      "圆通速递\r\n",
      "西部创业\r\n",
      "青松股份\r\n",
      "中路股份\r\n",
      "振华科技\r\n",
      "重庆啤酒\r\n",
      "中视传媒\r\n",
      "厦门国贸\r\n",
      "邮储银行\r\n",
      "茂化实华\r\n",
      "海默科技\r\n",
      "赛福天\r\n",
      "丹邦科技\r\n",
      "联得装备\r\n",
      "富奥股份\r\n",
      "弘宇股份\r\n",
      "天威视讯\r\n",
      "汇得科技\r\n",
      "天津普林\r\n",
      "蓝晓科技\r\n",
      "闽东电力\r\n",
      "中际旭创\r\n",
      "先河环保\r\n",
      "中信重工\r\n",
      "三湘印象\r\n",
      "际华集团\r\n",
      "长鹰信质\r\n",
      "佛燃股份\r\n",
      "太平鸟\r\n",
      "浙江鼎力\r\n",
      "步长制药\r\n",
      "包钢股份\r\n",
      "恒瑞医药\r\n",
      "启迪古汉\r\n",
      "铭普光磁\r\n",
      "海容冷链\r\n",
      "诺力股份\r\n",
      "新华网\r\n",
      "新文化\r\n",
      "星宇股份\r\n",
      "天夏智慧\r\n",
      "万润股份\r\n",
      "恒泰实达\r\n",
      "皖新传媒\r\n",
      "吉视传媒\r\n",
      "大龙地产\r\n",
      "科大国创\r\n",
      "川大智胜\r\n",
      "科大智能\r\n",
      "华仁药业\r\n",
      "华斯股份\r\n",
      "九州通\r\n",
      "新联电子\r\n",
      "全通教育\r\n",
      "国中水务\r\n",
      "东方明珠\r\n",
      "天润乳业\r\n",
      "弘高创意\r\n",
      "商业城\r\n",
      "宝钢包装\r\n",
      "恒大高新\r\n",
      "中科信息\r\n",
      "海信家电\r\n",
      "澄星股份\r\n",
      "皖通高速\r\n",
      "双星新材\r\n",
      "山东高速\r\n",
      "三联虹普\r\n",
      "强生控股\r\n",
      "拉卡拉\r\n",
      "金螳螂\r\n",
      "中炬高新\r\n",
      "京蓝科技\r\n",
      "博雅生物\r\n",
      "节能风电\r\n",
      "西菱动力\r\n",
      "中核钛白\r\n",
      "双鹭药业\r\n",
      "神州泰岳\r\n",
      "科力远\r\n",
      "蓝英装备\r\n",
      "千禾味业\r\n",
      "彤程新材\r\n",
      "深圳能源\r\n",
      "退市吉恩\r\n",
      "九洲药业\r\n",
      "中房股份\r\n",
      "中科创达\r\n",
      "爱乐达\r\n",
      "伊戈尔\r\n",
      "理邦仪器\r\n",
      "大参林\r\n",
      "同仁堂\r\n",
      "泉峰汽车\r\n",
      "华电重工\r\n",
      "微光股份\r\n",
      "中海达\r\n",
      "江特电机\r\n",
      "广生堂\r\n",
      "中远海科\r\n",
      "奥瑞德\r\n",
      "光弘科技\r\n",
      "宜昌交运\r\n",
      "上海电力\r\n",
      "共进股份\r\n",
      "现代制药\r\n",
      "久立特材\r\n",
      "蓝海华腾\r\n",
      "中原传媒\r\n",
      "摩登大道\r\n",
      "德生科技\r\n",
      "劲胜智能\r\n",
      "博天环境\r\n",
      "郑煤机\r\n",
      "北特科技\r\n",
      "正邦科技\r\n",
      "凌霄泵业\r\n",
      "成都银行\r\n",
      "九鼎新材\r\n",
      "智慧松德\r\n",
      "华电能源\r\n",
      "普利特\r\n",
      "黄山旅游\r\n",
      "华峰氨纶\r\n",
      "南华仪器\r\n",
      "冀中能源\r\n",
      "拓尔思\r\n",
      "鹏起科技\r\n",
      "美凯龙\r\n",
      "宁波水表\r\n",
      "古鳌科技\r\n",
      "山鼎设计\r\n",
      "弘亚数控\r\n",
      "喜临门\r\n",
      "南威软件\r\n",
      "东宏股份\r\n",
      "东方电子\r\n",
      "太辰光\r\n",
      "星期六\r\n",
      "莱克电气\r\n",
      "泰合健康\r\n",
      "广哈通信\r\n",
      "航发动力\r\n",
      "天壕环境\r\n",
      "东吴证券\r\n",
      "国投资本\r\n",
      "延安必康\r\n",
      "好太太\r\n",
      "佳士科技\r\n",
      "吉林高速\r\n",
      "同为股份\r\n",
      "龙韵股份\r\n",
      "四通新材\r\n",
      "文山电力\r\n",
      "新疆火炬\r\n",
      "金岭矿业\r\n",
      "超频三\r\n",
      "美年健康\r\n",
      "新潮能源\r\n",
      "光大银行\r\n",
      "中天科技\r\n",
      "永贵电器\r\n",
      "实达集团\r\n",
      "佳力图\r\n",
      "西王食品\r\n",
      "新纶科技\r\n",
      "游久游戏\r\n",
      "跨境通\r\n",
      "奥美医疗\r\n",
      "视源股份\r\n",
      "火炬电子\r\n",
      "老凤祥\r\n",
      "聚光科技\r\n",
      "跃岭股份\r\n",
      "天房发展\r\n",
      "大名城\r\n",
      "华天科技\r\n",
      "中国国贸\r\n",
      "中国建筑\r\n",
      "天晟新材\r\n",
      "舒泰神\r\n",
      "光大证券\r\n",
      "阳泉煤业\r\n",
      "亿通科技\r\n",
      "格林美\r\n",
      "嘉欣丝绸\r\n",
      "金山股份\r\n",
      "金域医学\r\n",
      "广东甘化\r\n",
      "长白山\r\n",
      "铁龙物流\r\n",
      "金安国纪\r\n",
      "和科达\r\n",
      "东湖高新\r\n",
      "力盛赛车\r\n",
      "康跃科技\r\n",
      "应流股份\r\n",
      "天津港\r\n",
      "浙大网新\r\n",
      "雪浪环境\r\n",
      "花园生物\r\n",
      "兆新股份\r\n",
      "伊利股份\r\n",
      "鲁西化工\r\n",
      "红旗连锁\r\n",
      "东方国信\r\n",
      "永兴材料\r\n",
      "朗迪集团\r\n",
      "中大力德\r\n",
      "沧州大化\r\n",
      "润邦股份\r\n",
      "今飞凯达\r\n",
      "海螺水泥\r\n",
      "泸天化\r\n",
      "福安药业\r\n",
      "汇通能源\r\n",
      "氯碱化工\r\n",
      "广电网络\r\n",
      "超讯通信\r\n",
      "ST运盛\r\n",
      "三只松鼠\r\n",
      "我爱我家\r\n",
      "鱼跃医疗\r\n",
      "正川股份\r\n",
      "精工钢构\r\n",
      "纵横通信\r\n",
      "东方电气\r\n",
      "网达软件\r\n",
      "工业富联\r\n",
      "冰川网络\r\n",
      "上海莱士\r\n",
      "海川智能\r\n",
      "创业慧康\r\n",
      "宁波韵升\r\n",
      "康力电梯\r\n",
      "居然之家\r\n",
      "中航机电\r\n",
      "派生科技\r\n",
      "纳尔股份\r\n",
      "华夏航空\r\n",
      "海普瑞\r\n",
      "振东制药\r\n",
      "贵广网络\r\n",
      "安车检测\r\n",
      "智莱科技\r\n",
      "绿色动力\r\n",
      "鹭燕医药\r\n",
      "永东股份\r\n",
      "华友钴业\r\n",
      "南华期货\r\n",
      "西安饮食\r\n",
      "棕榈股份\r\n",
      "国元证券\r\n",
      "科恒股份\r\n",
      "日科化学\r\n",
      "数知科技\r\n",
      "莱美药业\r\n",
      "华创阳安\r\n",
      "建发股份\r\n",
      "中新赛克\r\n",
      "海波重科\r\n",
      "天首发展\r\n",
      "亚星锚链\r\n",
      "大烨智能\r\n",
      "南方航空\r\n",
      "盛弘股份\r\n",
      "龙宇燃油\r\n",
      "汇金股份\r\n",
      "神农基因\r\n",
      "江苏吴中\r\n",
      "三雄极光\r\n",
      "恒实科技\r\n",
      "大众交通\r\n",
      "贝斯特\r\n",
      "秀强股份\r\n",
      "佛山照明\r\n",
      "安洁科技\r\n",
      "桂东电力\r\n",
      "双林股份\r\n",
      "梅安森\r\n",
      "深冷股份\r\n",
      "三力士\r\n",
      "乐歌股份\r\n",
      "雅化集团\r\n",
      "京能电力\r\n",
      "安泰科技\r\n",
      "奇信股份\r\n",
      "盘龙药业\r\n",
      "渝三峡A\r\n",
      "邦讯技术\r\n",
      "启迪设计\r\n",
      "光大嘉宝\r\n",
      "商赢环球\r\n",
      "国城矿业\r\n",
      "西安银行\r\n",
      "久之洋\r\n",
      "博深工具\r\n",
      "东兴证券\r\n",
      "飞乐音响\r\n",
      "聚力文化\r\n",
      "滨江集团\r\n",
      "壹网壹创\r\n",
      "大连港\r\n",
      "设研院\r\n",
      "四维图新\r\n",
      "仙琚制药\r\n",
      "中信国安\r\n",
      "麦趣尔\r\n",
      "捷顺科技\r\n",
      "美力科技\r\n",
      "名雕股份\r\n",
      "辰安科技\r\n",
      "蓝丰生化\r\n",
      "迈瑞医疗\r\n",
      "读者传媒\r\n",
      "老白干酒\r\n",
      "德尔未来\r\n",
      "东安动力\r\n",
      "*ST大唐\r\n",
      "茂业商业\r\n",
      "比亚迪\r\n",
      "陇神戎发\r\n",
      "通宝能源\r\n",
      "时代万恒\r\n",
      "榕基软件\r\n",
      "贵州茅台\r\n",
      "天奇股份\r\n",
      "家家悦\r\n",
      "南风股份\r\n",
      "寒锐钴业\r\n",
      "共达电声\r\n",
      "精测电子\r\n",
      "平潭发展\r\n",
      "惠伦晶体\r\n",
      "横店影视\r\n",
      "天味食品\r\n",
      "创业环保\r\n",
      "瀚蓝环境\r\n",
      "上海电影\r\n",
      "炼石航空\r\n",
      "海星股份\r\n",
      "凯普生物\r\n",
      "贝瑞基因\r\n",
      "创意信息\r\n",
      "派思股份\r\n",
      "紫江企业\r\n",
      "三川智慧\r\n",
      "中孚信息\r\n",
      "闰土股份\r\n",
      "众兴菌业\r\n",
      "埃斯顿\r\n",
      "海欣股份\r\n",
      "张家界\r\n",
      "云南白药\r\n",
      "台基股份\r\n",
      "河北宣工\r\n",
      "诚志股份\r\n",
      "大富科技\r\n",
      "三爱富\r\n",
      "保龄宝\r\n",
      "济川药业\r\n",
      "城投控股\r\n",
      "苏宁环球\r\n",
      "华宇软件\r\n",
      "博世科\r\n",
      "惠城环保\r\n",
      "青海春天\r\n",
      "中金岭南\r\n",
      "精艺股份\r\n",
      "南兴股份\r\n",
      "新乡化纤\r\n",
      "海洋王\r\n",
      "新兴铸管\r\n",
      "硕贝德\r\n",
      "亚泰集团\r\n",
      "金信诺\r\n",
      "科蓝软件\r\n",
      "星光农机\r\n",
      "钱江水利\r\n",
      "广联达\r\n",
      "华东数控\r\n",
      "爱施德\r\n",
      "高盟新材\r\n",
      "诚益通\r\n",
      "山河药辅\r\n",
      "正元智慧\r\n",
      "华建集团\r\n",
      "钢研高纳\r\n",
      "飞天诚信\r\n",
      "华菱星马\r\n",
      "中青宝\r\n",
      "佳隆股份\r\n",
      "赤峰黄金\r\n",
      "渤海股份\r\n",
      "恒华科技\r\n",
      "恒立液压\r\n",
      "吉药控股\r\n",
      "大千生态\r\n",
      "中国人寿\r\n",
      "世名科技\r\n",
      "华远地产\r\n",
      "永安林业\r\n",
      "中航光电\r\n",
      "招商蛇口\r\n",
      "亿嘉和\r\n",
      "空港股份\r\n",
      "道恩股份\r\n",
      "数据港\r\n",
      "晶盛机电\r\n",
      "山西路桥\r\n",
      "粤泰股份\r\n",
      "汉鼎宇佑\r\n",
      "百邦科技\r\n",
      "一心堂\r\n",
      "中公教育\r\n",
      "双塔食品\r\n",
      "高能环境\r\n",
      "通源石油\r\n",
      "江中药业\r\n",
      "万润科技\r\n",
      "四创电子\r\n",
      "荣丰控股\r\n",
      "正海磁材\r\n",
      "联合光电\r\n",
      "威孚高科\r\n",
      "海宁皮城\r\n",
      "盛和资源\r\n",
      "申万宏源\r\n",
      "太极实业\r\n",
      "盈趣科技\r\n",
      "晨光文具\r\n",
      "莱茵体育\r\n",
      "太化股份\r\n",
      "东风科技\r\n",
      "华控赛格\r\n",
      "一品红\r\n",
      "天宝食品\r\n",
      "河钢股份\r\n",
      "健康元\r\n",
      "广百股份\r\n",
      "华东医药\r\n",
      "东旭光电\r\n",
      "长春高新\r\n",
      "新诺威\r\n",
      "无锡银行\r\n",
      "歌华有线\r\n",
      "华录百纳\r\n",
      "新华联\r\n",
      "通合科技\r\n",
      "精研科技\r\n",
      "金桥信息\r\n",
      "方直科技\r\n",
      "中科曙光\r\n",
      "永福股份\r\n",
      "捷佳伟创\r\n",
      "上海瀚讯\r\n",
      "上药转换\r\n",
      "中船科技\r\n",
      "迪普科技\r\n",
      "潍柴重机\r\n",
      "奥佳华\r\n",
      "玲珑轮胎\r\n",
      "攀钢钒钛\r\n",
      "翠微股份\r\n",
      "长春燃气\r\n",
      "科远智慧\r\n",
      "菲利华\r\n",
      "特变电工\r\n",
      "鲁抗医药\r\n",
      "绿茵生态\r\n",
      "美格智能\r\n",
      "南大光电\r\n",
      "暴风集团\r\n",
      "高伟达\r\n",
      "富森美\r\n",
      "世运电路\r\n",
      "山东矿机\r\n",
      "天津磁卡\r\n",
      "风语筑\r\n",
      "鹏博士\r\n",
      "浙能电力\r\n",
      "招商轮船\r\n",
      "中西药业\r\n",
      "普莱柯\r\n",
      "延华智能\r\n",
      "宝新能源\r\n",
      "深深房A\r\n",
      "宝胜股份\r\n",
      "新宁物流\r\n",
      "雅戈尔\r\n",
      "外运发展\r\n",
      "飞利信\r\n",
      "皖能电力\r\n",
      "综艺股份\r\n",
      "科达洁能\r\n",
      "福斯特\r\n",
      "尚荣医疗\r\n",
      "北京城建\r\n",
      "农发种业\r\n",
      "宣亚国际\r\n",
      "探路者\r\n",
      "康辰药业\r\n",
      "双箭股份\r\n",
      "富安娜\r\n",
      "润都股份\r\n",
      "奥普光电\r\n",
      "全聚德\r\n",
      "生益科技\r\n",
      "炬华科技\r\n",
      "中国交建\r\n",
      "神雾环保\r\n",
      "神州信息\r\n",
      "平高电气\r\n",
      "聚隆科技\r\n",
      "北信源\r\n",
      "上海建工\r\n",
      "汇顶科技\r\n",
      "普邦股份\r\n",
      "太行水泥\r\n",
      "众信旅游\r\n",
      "美亚光电\r\n",
      "长高集团\r\n",
      "聚飞光电\r\n",
      "华培动力\r\n",
      "航锦科技\r\n",
      "大金重工\r\n",
      "翰宇药业\r\n",
      "茂硕电源\r\n",
      "海格通信\r\n",
      "四方股份\r\n",
      "台海核电\r\n",
      "兴齐眼药\r\n",
      "达刚路机\r\n",
      "博通集成\r\n",
      "拓邦股份\r\n",
      "青岛双星\r\n",
      "片仔癀\r\n",
      "巨力索具\r\n",
      "营口港\r\n",
      "广州发展\r\n",
      "泛微网络\r\n",
      "金龙羽\r\n",
      "红日药业\r\n",
      "东富龙\r\n",
      "百达精工\r\n",
      "德联集团\r\n",
      "华业资本\r\n",
      "东信和平\r\n",
      "佳创视讯\r\n",
      "道森股份\r\n",
      "天铁股份\r\n",
      "天邦股份\r\n",
      "日发精机\r\n",
      "东方集团\r\n",
      "康龙化成\r\n",
      "莎普爱思\r\n",
      "设计总院\r\n",
      "ST大控\r\n",
      "海得控制\r\n",
      "福鞍股份\r\n",
      "山鹰纸业\r\n",
      "神雾节能\r\n",
      "奇精机械\r\n",
      "同大股份\r\n",
      "申通快递\r\n",
      "中航沈飞\r\n",
      "锌业股份\r\n",
      "天海防务\r\n",
      "金徽酒\r\n",
      "三星新材\r\n",
      "招商银行\r\n",
      "山东药玻\r\n",
      "奥联电子\r\n",
      "中公高科\r\n",
      "置信电气\r\n",
      "三七互娱\r\n",
      "宝鼎科技\r\n",
      "蠡湖股份\r\n",
      "电连技术\r\n",
      "安奈儿\r\n",
      "鹏辉能源\r\n",
      "鄂武商A\r\n",
      "万方发展\r\n",
      "钧达股份\r\n",
      "今天国际\r\n",
      "重庆水务\r\n",
      "联诚精密\r\n",
      "健帆生物\r\n",
      "分众传媒\r\n",
      "苏州固锝\r\n",
      "腾邦国际\r\n",
      "神剑股份\r\n",
      "智能自控\r\n",
      "山大华特\r\n",
      "振华重工\r\n",
      "环旭电子\r\n",
      "开润股份\r\n",
      "未名医药\r\n",
      "英飞拓\r\n",
      "中煤能源\r\n",
      "华东重机\r\n",
      "浦东建设\r\n",
      "东方新星\r\n",
      "上海石化\r\n",
      "中科软\r\n",
      "中信出版\r\n",
      "潞安环能\r\n",
      "当代东方\r\n",
      "ST沪科\r\n",
      "西部矿业\r\n",
      "浪潮信息\r\n",
      "乾景园林\r\n",
      "广信股份\r\n",
      "美盈森\r\n",
      "柳工\r\n",
      "诚迈科技\r\n",
      "江西铜业\r\n",
      "三晖电气\r\n",
      "思创医惠\r\n",
      "中新药业\r\n",
      "汉宇集团\r\n",
      "农产品\r\n",
      "东旭蓝天\r\n",
      "华星创业\r\n",
      "鞍钢股份\r\n",
      "浔兴股份\r\n",
      "北巴传媒\r\n",
      "海辰药业\r\n",
      "长江投资\r\n",
      "承德露露\r\n",
      "嘉友国际\r\n",
      "棒杰股份\r\n",
      "内蒙华电\r\n",
      "大冷股份\r\n",
      "泛海控股\r\n",
      "燕京啤酒\r\n",
      "京粮控股\r\n",
      "瑞康医药\r\n",
      "力生制药\r\n",
      "天目药业\r\n",
      "晶瑞股份\r\n",
      "宇瞳光学\r\n",
      "现代投资\r\n",
      "城建发展\r\n",
      "万集科技\r\n",
      "老板电器\r\n",
      "汉邦高科\r\n",
      "新通联\r\n",
      "帝尔激光\r\n",
      "中材节能\r\n",
      "龙津药业\r\n",
      "紫光股份\r\n",
      "博实股份\r\n",
      "北化股份\r\n",
      "安徽合力\r\n",
      "岱勒新材\r\n",
      "宇通客车\r\n",
      "华西能源\r\n",
      "冀凯股份\r\n",
      "高乐股份\r\n",
      "恒泰艾普\r\n",
      "天瑞仪器\r\n",
      "华贸物流\r\n",
      "翔港科技\r\n",
      "千红制药\r\n",
      "快意电梯\r\n",
      "久远银海\r\n",
      "奥克股份\r\n",
      "西山煤电\r\n",
      "和而泰\r\n",
      "强力新材\r\n",
      "恒力股份\r\n",
      "日照港\r\n",
      "海油发展\r\n",
      "博创科技\r\n",
      "首商股份\r\n",
      "易事特\r\n",
      "富瀚微\r\n",
      "广汇物流\r\n",
      "牧高笛\r\n",
      "四川成渝\r\n",
      "莱绅通灵\r\n",
      "摩恩电气\r\n",
      "航天信息\r\n",
      "鼎汉技术\r\n",
      "中科金财\r\n",
      "山东路桥\r\n",
      "重庆建工\r\n",
      "诚邦股份\r\n",
      "鼎龙股份\r\n",
      "博迈科\r\n",
      "欧普照明\r\n",
      "国恩股份\r\n",
      "石基信息\r\n",
      "安妮股份\r\n",
      "中牧股份\r\n",
      "航天通信\r\n",
      "晋西车轴\r\n",
      "香江控股\r\n",
      "鸿利智汇\r\n",
      "广东鸿图\r\n",
      "因赛集团\r\n",
      "展鹏科技\r\n",
      "国际医学\r\n",
      "长城科技\r\n",
      "天汽模\r\n",
      "三孚股份\r\n",
      "东方锆业\r\n",
      "力星股份\r\n",
      "烯碳退\r\n",
      "凯乐科技\r\n",
      "中国中期\r\n",
      "利群股份\r\n",
      "粤电力A\r\n",
      "永利股份\r\n",
      "宝莫股份\r\n",
      "海翔药业\r\n",
      "聚龙股份\r\n",
      "华力创通\r\n",
      "交大昂立\r\n",
      "济民制药\r\n",
      "麦迪科技\r\n",
      "金风科技\r\n",
      "中国电影\r\n",
      "民和股份\r\n",
      "北京科锐\r\n",
      "新界泵业\r\n",
      "康弘药业\r\n",
      "上海洗霸\r\n",
      "周大生\r\n",
      "联建光电\r\n",
      "动力源\r\n",
      "三聚环保\r\n",
      "东山精密\r\n",
      "隆基机械\r\n",
      "南方传媒\r\n",
      "东华软件\r\n",
      "大秦铁路\r\n",
      "恩捷股份\r\n",
      "醋化股份\r\n",
      "金溢科技\r\n",
      "菲达环保\r\n",
      "顶固集创\r\n",
      "京汉股份\r\n",
      "国联水产\r\n",
      "中信证券\r\n",
      "吉华集团\r\n",
      "拓普集团\r\n",
      "木林森\r\n",
      "广汇汽车\r\n",
      "长江证券\r\n",
      "中国平安\r\n",
      "大连重工\r\n",
      "华侨城A\r\n",
      "金力永磁\r\n",
      "药石科技\r\n",
      "民生银行\r\n",
      "奥特迅\r\n",
      "蓝焰控股\r\n",
      "新北洋\r\n",
      "双象股份\r\n",
      "广田集团\r\n",
      "旭光股份\r\n",
      "南宁百货\r\n",
      "鸿达兴业\r\n",
      "新光圆成\r\n",
      "博士眼镜\r\n",
      "杭氧股份\r\n",
      "均胜电子\r\n",
      "万讯自控\r\n",
      "诺普信\r\n",
      "中海油服\r\n",
      "中粮资本\r\n",
      "长久物流\r\n",
      "方正科技\r\n",
      "海航投资\r\n",
      "鹏鼎控股\r\n",
      "万向德农\r\n",
      "广弘控股\r\n",
      "鸿博股份\r\n",
      "顺鑫农业\r\n",
      "移为通信\r\n",
      "力帆股份\r\n",
      "新疆浩源\r\n",
      "万年青\r\n",
      "天和防务\r\n",
      "先导智能\r\n",
      "亚光科技\r\n",
      "九安医疗\r\n",
      "瀛通通讯\r\n",
      "中原环保\r\n",
      "好莱客\r\n",
      "江海股份\r\n",
      "航天晨光\r\n",
      "国光电器\r\n",
      "龙蟠科技\r\n",
      "诺德股份\r\n",
      "市北高新\r\n",
      "梅花生物\r\n",
      "中光防雷\r\n",
      "卫士通\r\n",
      "南洋股份\r\n",
      "万通智控\r\n",
      "华联控股\r\n",
      "天虹股份\r\n",
      "华东电脑\r\n",
      "爱仕达\r\n",
      "迈为股份\r\n",
      "长盛轴承\r\n",
      "广发证券\r\n",
      "昆仑万维\r\n",
      "银龙股份\r\n",
      "新时达\r\n",
      "*ST安泰\r\n",
      "迎驾贡酒\r\n",
      "苏垦农发\r\n",
      "江苏神通\r\n",
      "飞荣达\r\n",
      "长亮科技\r\n",
      "立霸股份\r\n",
      "张江高科\r\n",
      "天龙光电\r\n",
      "双良节能\r\n",
      "中国电建\r\n",
      "捷成股份\r\n",
      "东方嘉盛\r\n",
      "锦浪科技\r\n",
      "绿庭投资\r\n",
      "山煤国际\r\n",
      "捷昌驱动\r\n",
      "大连热电\r\n",
      "中铝国际\r\n",
      "泸州老窖\r\n",
      "康旗股份\r\n",
      "寿仙谷\r\n",
      "安硕信息\r\n",
      "漳州发展\r\n",
      "德展健康\r\n",
      "塔牌集团\r\n",
      "国投电力\r\n",
      "高德红外\r\n",
      "高斯贝尔\r\n",
      "阳光股份\r\n",
      "浙数文化\r\n",
      "光迅科技\r\n",
      "远大智能\r\n",
      "新研股份\r\n",
      "徕木股份\r\n",
      "上海物贸\r\n",
      "倍加洁\r\n",
      "元祖股份\r\n",
      "东尼电子\r\n",
      "惠程科技\r\n",
      "尚纬股份\r\n",
      "创元科技\r\n",
      "达志科技\r\n",
      "新媒股份\r\n",
      "英可瑞\r\n",
      "酒鬼酒\r\n",
      "上海钢联\r\n",
      "华信新材\r\n",
      "中山金马\r\n",
      "山西证券\r\n",
      "华阳集团\r\n",
      "北京君正\r\n",
      "三垒股份\r\n",
      "华正新材\r\n",
      "翔鹭钨业\r\n",
      "广州港\r\n",
      "立昂技术\r\n",
      "长虹美菱\r\n",
      "姚记扑克\r\n",
      "华能水电\r\n",
      "杭叉集团\r\n",
      "石化油服\r\n",
      "友阿股份\r\n",
      "科沃斯\r\n",
      "上海雅仕\r\n",
      "联创光电\r\n",
      "常宝股份\r\n",
      "意华股份\r\n",
      "飞科电器\r\n",
      "三棵树\r\n",
      "凯撒旅游\r\n",
      "三维丝\r\n",
      "江南高纤\r\n",
      "华泰证券\r\n",
      "红宇新材\r\n",
      "常山药业\r\n",
      "盛达矿业\r\n",
      "赛托生物\r\n",
      "中迪投资\r\n",
      "苏试试验\r\n",
      "科顺股份\r\n",
      "亿帆医药\r\n",
      "亿纬锂能\r\n",
      "森远股份\r\n",
      "世纪华通\r\n",
      "锦江投资\r\n",
      "辰欣药业\r\n",
      "中华企业\r\n",
      "坚瑞沃能\r\n",
      "山西焦化\r\n",
      "保变电气\r\n",
      "益盛药业\r\n",
      "哈高科\r\n",
      "ST坊展\r\n",
      "顺发恒业\r\n",
      "中国中冶\r\n",
      "同有科技\r\n",
      "中水渔业\r\n",
      "立思辰\r\n",
      "康缘药业\r\n",
      "安图生物\r\n",
      "江淮汽车\r\n",
      "真视通\r\n",
      "亚太药业\r\n",
      "新天然气\r\n",
      "三泰控股\r\n",
      "漫步者\r\n",
      "瑞尔特\r\n",
      "新天科技\r\n",
      "红相股份\r\n",
      "远达环保\r\n",
      "御家汇\r\n",
      "至正股份\r\n",
      "莱宝高科\r\n",
      "恒为科技\r\n",
      "欧亚集团\r\n",
      "珠海中富\r\n",
      "英联股份\r\n",
      "佛慈制药\r\n",
      "东杰智能\r\n",
      "科创信息\r\n",
      "江南化工\r\n",
      "三角防务\r\n",
      "佳沃股份\r\n",
      "丝路视觉\r\n",
      "巴安水务\r\n",
      "兴业矿业\r\n",
      "佳发教育\r\n",
      "上海机电\r\n",
      "平庄能源\r\n",
      "中恒电气\r\n",
      "人民网\r\n",
      "江苏国泰\r\n",
      "东江环保\r\n",
      "康普顿\r\n",
      "南京聚隆\r\n",
      "梅雁吉祥\r\n",
      "泰嘉股份\r\n",
      "中石科技\r\n",
      "西藏珠峰\r\n",
      "金牌厨柜\r\n",
      "金智科技\r\n",
      "会稽山\r\n",
      "英洛华\r\n",
      "华灿光电\r\n",
      "移远通信\r\n",
      "三五互联\r\n",
      "海伦哲\r\n",
      "创维数字\r\n",
      "东华能源\r\n",
      "凤凰股份\r\n",
      "澳洋健康\r\n",
      "泰格医药\r\n",
      "永高股份\r\n",
      "达实智能\r\n",
      "浩云科技\r\n",
      "世茂股份\r\n",
      "太阳能\r\n",
      "畅联股份\r\n",
      "广誉远\r\n",
      "汉森制药\r\n",
      "宁沪高速\r\n",
      "金鸿控股\r\n",
      "华软科技\r\n",
      "金新农\r\n",
      "青鸟消防\r\n",
      "宁波热电\r\n",
      "锐奇股份\r\n",
      "焦点科技\r\n",
      "国检集团\r\n",
      "优博讯\r\n",
      "海王生物\r\n",
      "必创科技\r\n",
      "碧水源\r\n",
      "中科电气\r\n",
      "中嘉博创\r\n",
      "深华发A\r\n",
      "保税科技\r\n",
      "富春股份\r\n",
      "宁夏建材\r\n",
      "北方导航\r\n",
      "安通控股\r\n",
      "百联股份\r\n",
      "中国银河\r\n",
      "日出东方\r\n",
      "电光科技\r\n",
      "长城证券\r\n",
      "左江科技\r\n",
      "万通地产\r\n",
      "徐家汇\r\n",
      "宁德时代\r\n",
      "江苏租赁\r\n",
      "大智慧\r\n",
      "*ST云网\r\n",
      "星云股份\r\n",
      "华资实业\r\n",
      "荣盛石化\r\n",
      "湖南盐业\r\n",
      "乐通股份\r\n",
      "通策医疗\r\n",
      "大博医疗\r\n",
      "长沙银行\r\n",
      "敦煌种业\r\n",
      "广州浪奇\r\n",
      "盈峰环境\r\n",
      "富煌钢构\r\n",
      "曲美家居\r\n",
      "北方稀土\r\n",
      "中材科技\r\n",
      "罗博特科\r\n",
      "荣之联\r\n",
      "盛通股份\r\n",
      "招商南油\r\n",
      "中国海诚\r\n",
      "威创股份\r\n",
      "天银机电\r\n",
      "三盛教育\r\n",
      "澳洋顺昌\r\n",
      "科士达\r\n",
      "金莱特\r\n",
      "长川科技\r\n",
      "上海医药\r\n",
      "禾望电气\r\n",
      "隆基股份\r\n",
      "万里股份\r\n",
      "泰禾集团\r\n",
      "贝肯能源\r\n",
      "东音股份\r\n",
      "万达信息\r\n",
      "ST生化\r\n",
      "洛阳钼业\r\n",
      "恒通科技\r\n",
      "中国医药\r\n",
      "赞宇科技\r\n",
      "每日互动\r\n",
      "宁波银行\r\n",
      "恒丰纸业\r\n",
      "南京公用\r\n",
      "辽宁成大\r\n",
      "银泰资源\r\n",
      "安科瑞\r\n",
      "华通热力\r\n",
      "易华录\r\n",
      "安诺其\r\n",
      "七一二\r\n",
      "熙菱信息\r\n",
      "东方园林\r\n",
      "登海种业\r\n",
      "中航电子\r\n",
      "上实医药\r\n",
      "珠江钢琴\r\n",
      "葵花药业\r\n",
      "力源信息\r\n",
      "博云新材\r\n",
      "航天发展\r\n",
      "东易日盛\r\n",
      "三维股份\r\n",
      "凯恩股份\r\n",
      "新华保险\r\n",
      "江南水务\r\n",
      "骅威文化\r\n",
      "歌力思\r\n",
      "中信海直\r\n",
      "中文在线\r\n",
      "一拖股份\r\n",
      "文科园林\r\n",
      "全柴动力\r\n",
      "天坛生物\r\n",
      "佐力药业\r\n",
      "中国科传\r\n",
      "国脉科技\r\n",
      "桐昆股份\r\n",
      "新泉股份\r\n",
      "宏辉果蔬\r\n",
      "斯莱克\r\n",
      "皮阿诺\r\n",
      "海伦钢琴\r\n",
      "丰乐种业\r\n",
      "金达威\r\n",
      "美晨生态\r\n",
      "海利得\r\n",
      "宝信软件\r\n",
      "成飞集成\r\n",
      "璞泰来\r\n",
      "航天彩虹\r\n",
      "迪马股份\r\n",
      "爱朋医疗\r\n",
      "江化微\r\n",
      "红塔证券\r\n",
      "海航科技\r\n",
      "领益智造\r\n",
      "赣粤高速\r\n",
      "新余国科\r\n",
      "盛路通信\r\n",
      "冰轮环境\r\n",
      "广泽股份\r\n",
      "荣安地产\r\n",
      "云铝股份\r\n",
      "弘讯科技\r\n",
      "巨星科技\r\n",
      "长园集团\r\n",
      "中国卫通\r\n",
      "恒宝股份\r\n",
      "新元科技\r\n",
      "皇氏集团\r\n",
      "朗进科技\r\n",
      "科达股份\r\n",
      "岭南股份\r\n",
      "新希望\r\n",
      "南京证券\r\n",
      "广电运通\r\n",
      "洋河股份\r\n",
      "怡球资源\r\n",
      "国电南瑞\r\n",
      "吉林敖东\r\n",
      "雪人股份\r\n",
      "光莆股份\r\n",
      "沈阳化工\r\n",
      "同和药业\r\n",
      "宋都股份\r\n",
      "扬子新材\r\n",
      "开尔新材\r\n",
      "再升科技\r\n",
      "同济堂\r\n",
      "上海亚虹\r\n",
      "特一药业\r\n",
      "博敏电子\r\n",
      "九强生物\r\n",
      "阳煤化工\r\n",
      "桂冠电力\r\n",
      "三角轮胎\r\n",
      "银都股份\r\n",
      "西藏城投\r\n",
      "九阳股份\r\n",
      "新奥股份\r\n",
      "华中数控\r\n",
      "首航节能\r\n",
      "尖峰集团\r\n",
      "方大集团\r\n",
      "浪潮软件\r\n",
      "牧原股份\r\n",
      "中国国航\r\n",
      "*ST油服\r\n",
      "海联金汇\r\n",
      "航天工程\r\n",
      "通葡股份\r\n",
      "口子窖\r\n",
      "康欣新材\r\n",
      "乐普医疗\r\n",
      "濮耐股份\r\n",
      "深康佳A\r\n",
      "怡亚通\r\n",
      "沪宁股份\r\n",
      "工商银行\r\n",
      "金钼股份\r\n",
      "海峡股份\r\n",
      "上机数控\r\n",
      "香雪制药\r\n",
      "中核科技\r\n",
      "维业股份\r\n",
      "天宇股份\r\n",
      "奥拓电子\r\n",
      "华扬联众\r\n",
      "上工申贝\r\n",
      "国海证券\r\n",
      "深南电A\r\n",
      "太钢不锈\r\n",
      "依顿电子\r\n",
      "百洋股份\r\n",
      "泰瑞机器\r\n",
      "艾艾精工\r\n",
      "卓胜微\r\n",
      "艾迪精密\r\n",
      "深圳新星\r\n",
      "鹏欣资源\r\n",
      "格力电器\r\n",
      "悦达投资\r\n",
      "华西股份\r\n",
      "华策影视\r\n",
      "国药股份\r\n",
      "德宏股份\r\n",
      "志邦家居\r\n",
      "苏州高新\r\n",
      "顺钠股份\r\n",
      "东方电缆\r\n",
      "金发科技\r\n",
      "森源电气\r\n",
      "金洲慈航\r\n",
      "中国银行\r\n",
      "艾格拉斯\r\n",
      "金融街\r\n",
      "威尔泰\r\n",
      "世纪瑞尔\r\n",
      "恒康医疗\r\n",
      "运达股份\r\n",
      "江苏有线\r\n",
      "联发股份\r\n",
      "中昌数据\r\n",
      "永鼎股份\r\n",
      "曲江文旅\r\n",
      "福日电子\r\n",
      "中利集团\r\n",
      "博汇纸业\r\n",
      "长城汽车\r\n",
      "津劝业\r\n",
      "国药一致\r\n",
      "天目湖\r\n",
      "三江购物\r\n",
      "梦洁股份\r\n",
      "三维通信\r\n",
      "汇中股份\r\n",
      "同兴达\r\n",
      "中国巨石\r\n",
      "北京银行\r\n",
      "舍得酒业\r\n",
      "南京化纤\r\n",
      "中兴商业\r\n",
      "广和通\r\n",
      "乾照光电\r\n",
      "香山股份\r\n",
      "金杯汽车\r\n",
      "凤凰传媒\r\n",
      "天域生态\r\n",
      "海大集团\r\n",
      "天茂集团\r\n",
      "洪汇新材\r\n",
      "科伦药业\r\n",
      "激智科技\r\n",
      "瑞丰光电\r\n",
      "盛讯达\r\n",
      "温氏股份\r\n",
      "青岛啤酒\r\n",
      "东土科技\r\n",
      "量子生物\r\n",
      "宝钢股份\r\n",
      "中矿资源\r\n",
      "烽火通信\r\n",
      "东风汽车\r\n",
      "尔康制药\r\n",
      "四川双马\r\n",
      "川仪股份\r\n",
      "天山生物\r\n",
      "同方股份\r\n",
      "农尚环境\r\n",
      "有研新材\r\n",
      "闻泰科技\r\n",
      "航天电器\r\n",
      "好利来\r\n",
      "浙江交科\r\n",
      "东方材料\r\n",
      "香溢融通\r\n",
      "华懋科技\r\n",
      "民生控股\r\n",
      "泰胜风能\r\n",
      "铁岭新城\r\n",
      "双杰电气\r\n",
      "鸿路钢构\r\n",
      "通宇通讯\r\n",
      "天药股份\r\n",
      "中科三环\r\n",
      "运达科技\r\n",
      "药明康德\r\n",
      "新宝股份\r\n",
      "保利联合\r\n",
      "精伦电子\r\n",
      "太平洋\r\n",
      "横河模具\r\n",
      "慈星股份\r\n",
      "清新环境\r\n",
      "浙富控股\r\n",
      "恒顺醋业\r\n",
      "川恒股份\r\n",
      "界龙实业\r\n",
      "全新好\r\n",
      "金冠股份\r\n",
      "银河磁体\r\n",
      "达安股份\r\n",
      "华新水泥\r\n",
      "广宇集团\r\n",
      "西麦食品\r\n",
      "亚世光电\r\n",
      "光韵达\r\n",
      "星徽精密\r\n",
      "易成新能\r\n",
      "南卫股份\r\n",
      "绿地控股\r\n",
      "小熊电器\r\n",
      "同花顺\r\n",
      "东方时尚\r\n",
      "安彩高科\r\n",
      "中钨高新\r\n",
      "兴森科技\r\n",
      "德威新材\r\n",
      "中设集团\r\n",
      "唐山港\r\n",
      "多喜爱\r\n",
      "南山铝业\r\n",
      "二六三\r\n",
      "汇金科技\r\n",
      "盛运环保\r\n",
      "英唐智控\r\n",
      "朗玛信息\r\n",
      "合兴包装\r\n",
      "欧普康视\r\n",
      "建新股份\r\n",
      "东方证券\r\n",
      "宜华健康\r\n",
      "山东华鹏\r\n",
      "华荣股份\r\n",
      "福成股份\r\n",
      "鼎信通讯\r\n",
      "晨鑫科技\r\n",
      "*ST船舶\r\n",
      "新黄浦\r\n",
      "西部建设\r\n",
      "华鑫股份\r\n",
      "华邦健康\r\n",
      "亚宝药业\r\n",
      "西部黄金\r\n",
      "东方盛虹\r\n",
      "奥飞数据\r\n",
      "顺灏股份\r\n",
      "恒基达鑫\r\n",
      "中原证券\r\n",
      "和胜股份\r\n",
      "金科股份\r\n",
      "泰豪科技\r\n",
      "合力泰\r\n",
      "康强电子\r\n",
      "安居宝\r\n",
      "游族网络\r\n",
      "欧比特\r\n",
      "上柴股份\r\n",
      "德赛电池\r\n",
      "科锐国际\r\n",
      "齐心集团\r\n",
      "永新光学\r\n",
      "惠威科技\r\n",
      "新华百货\r\n",
      "福莱特\r\n",
      "索菲亚\r\n",
      "宋城演艺\r\n",
      "马钢股份\r\n",
      "郴电国际\r\n",
      "鄂尔多斯\r\n",
      "东方通信\r\n",
      "高科石化\r\n",
      "金轮股份\r\n",
      "拉夏贝尔\r\n",
      "春风动力\r\n",
      "南京新百\r\n",
      "锦州港\r\n",
      "宇信科技\r\n",
      "中信建投\r\n",
      "海立股份\r\n",
      "凤形股份\r\n",
      "*ST佳电\r\n",
      "中宠股份\r\n",
      "天地科技\r\n",
      "永太科技\r\n",
      "四川金顶\r\n",
      "伟明环保\r\n",
      "柳钢股份\r\n",
      "天齐锂业\r\n",
      "绿盟科技\r\n",
      "雷科防务\r\n",
      "苏大维格\r\n",
      "联明股份\r\n",
      "通富微电\r\n",
      "信维通信\r\n",
      "亚振家居\r\n",
      "华虹计通\r\n",
      "香梨股份\r\n",
      "沃尔核材\r\n",
      "章源钨业\r\n",
      "风华高科\r\n",
      "爱建集团\r\n",
      "路通视信\r\n",
      "长海股份\r\n",
      "凯撒文化\r\n",
      "和佳股份\r\n",
      "三鑫医疗\r\n",
      "广汽集团\r\n",
      "中天金融\r\n",
      "富祥股份\r\n",
      "东北制药\r\n",
      "方大特钢\r\n",
      "杭钢股份\r\n",
      "华脉科技\r\n",
      "新美星\r\n",
      "金杯电工\r\n",
      "艾德生物\r\n",
      "亨通光电\r\n",
      "新国都\r\n",
      "江丰电子\r\n",
      "冠昊生物\r\n",
      "龙头股份\r\n",
      "南都电源\r\n",
      "天山股份\r\n",
      "隆华科技\r\n",
      "健民集团\r\n",
      "西昌电力\r\n",
      "光力科技\r\n",
      "太原重工\r\n",
      "法兰泰克\r\n",
      "盐津铺子\r\n",
      "丹化科技\r\n",
      "士兰微\r\n",
      "合众思壮\r\n",
      "全信股份\r\n",
      "光华科技\r\n",
      "任子行\r\n",
      "龙建股份\r\n",
      "阳普医疗\r\n",
      "晶华新材\r\n",
      "石大胜华\r\n",
      "江阴银行\r\n",
      "立讯精密\r\n",
      "哈药股份\r\n",
      "沪电股份\r\n",
      "华东科技\r\n",
      "华测导航\r\n",
      "和顺电气\r\n",
      "华能国际\r\n",
      "森特股份\r\n",
      "光线传媒\r\n",
      "成都路桥\r\n",
      "上海临港\r\n",
      "长春一东\r\n",
      "透景生命\r\n",
      "天顺股份\r\n",
      "新农股份\r\n",
      "金麒麟\r\n",
      "北京城乡\r\n",
      "正平股份\r\n",
      "鸿远电子\r\n",
      "荃银高科\r\n",
      "深圳机场\r\n",
      "川金诺\r\n",
      "张家港行\r\n",
      "大康农业\r\n",
      "中交地产\r\n",
      "宏达矿业\r\n",
      "仙坛股份\r\n",
      "长方集团\r\n",
      "三夫户外\r\n",
      "完美世界\r\n",
      "海利生物\r\n",
      "华联股份\r\n",
      "新南洋\r\n",
      "中国宝安\r\n",
      "TCL集团\r\n",
      "四川路桥\r\n",
      "吉林化纤\r\n",
      "金辰股份\r\n",
      "福田汽车\r\n",
      "邦宝益智\r\n",
      "怡达股份\r\n",
      "圣济堂\r\n",
      "卓郎智能\r\n",
      "兰太实业\r\n",
      "西藏旅游\r\n",
      "华域汽车\r\n",
      "昊华能源\r\n",
      "华凯创意\r\n",
      "欧派家居\r\n",
      "佳讯飞鸿\r\n",
      "北京利尔\r\n",
      "多伦科技\r\n",
      "英科医疗\r\n",
      "光洋股份\r\n",
      "超华科技\r\n",
      "天健集团\r\n",
      "双一科技\r\n",
      "泰达股份\r\n",
      "长飞光纤\r\n",
      "长江电力\r\n",
      "哈三联\r\n",
      "陕天然气\r\n",
      "天津松江\r\n",
      "中国长城\r\n",
      "重药控股\r\n",
      "杰克股份\r\n",
      "燕塘乳业\r\n",
      "汉嘉设计\r\n",
      "利民股份\r\n",
      "汇洁股份\r\n",
      "德邦股份\r\n",
      "吴通控股\r\n",
      "东诚药业\r\n",
      "沃森生物\r\n",
      "养元饮品\r\n",
      "亚太科技\r\n",
      "露笑科技\r\n",
      "中飞股份\r\n",
      "迪生力\r\n",
      "杰瑞股份\r\n",
      "梅轮电梯\r\n",
      "九鼎投资\r\n",
      "天奥电子\r\n",
      "吉大通信\r\n",
      "首创股份\r\n",
      "贵州燃气\r\n",
      "上海三毛\r\n",
      "聚灿光电\r\n",
      "天宸股份\r\n",
      "天地源\r\n",
      "三德科技\r\n",
      "中远海能\r\n",
      "华源控股\r\n",
      "拓日新能\r\n",
      "长城军工\r\n",
      "深大通\r\n",
      "康拓红外\r\n",
      "瑞贝卡\r\n",
      "元力股份\r\n",
      "歌尔股份\r\n",
      "恒生电子\r\n",
      "金雷股份\r\n",
      "科创新源\r\n",
      "重庆百货\r\n",
      "国盛金控\r\n",
      "文一科技\r\n",
      "群兴玩具\r\n",
      "中集集团\r\n",
      "雄塑科技\r\n",
      "奥赛康\r\n",
      "龙蟒佰利\r\n",
      "比音勒芬\r\n",
      "中潜股份\r\n",
      "中国石化\r\n",
      "百傲化学\r\n",
      "传艺科技\r\n",
      "武汉凡谷\r\n",
      "特尔佳\r\n",
      "中环环保\r\n",
      "京投发展\r\n",
      "巨化股份\r\n",
      "*ST抚钢\r\n",
      "亚翔集成\r\n",
      "新和成\r\n",
      "永和智控\r\n",
      "三一重工\r\n",
      "潮宏基\r\n",
      "恺英网络\r\n",
      "万向钱潮\r\n",
      "鼎捷软件\r\n",
      "中电兴发\r\n",
      "益生股份\r\n",
      "海泰发展\r\n",
      "鲍斯股份\r\n",
      "中国石油\r\n",
      "金证股份\r\n",
      "德恩精工\r\n",
      "裕同科技\r\n",
      "国祯环保\r\n",
      "老百姓\r\n",
      "华峰超纤\r\n",
      "迪森股份\r\n",
      "中密控股\r\n",
      "山东黄金\r\n",
      "易见股份\r\n",
      "韦尔股份\r\n",
      "星星科技\r\n",
      "裕兴股份\r\n",
      "兰生股份\r\n",
      "建科院\r\n",
      "旋极信息\r\n",
      "欣天科技\r\n",
      "中油资本\r\n",
      "复星医药\r\n",
      "卫信康\r\n",
      "创源文化\r\n",
      "索通发展\r\n",
      "凤凰光学\r\n",
      "大商股份\r\n",
      "以岭药业\r\n",
      "恒久科技\r\n",
      "水井坊\r\n",
      "交通银行\r\n",
      "坚朗五金\r\n",
      "古井贡酒\r\n",
      "东睦股份\r\n",
      "万华化学\r\n",
      "信息发展\r\n",
      "珠江实业\r\n",
      "哈工智能\r\n",
      "常熟汽饰\r\n",
      "凯中精密\r\n",
      "濮阳惠成\r\n",
      "东方网力\r\n",
      "浦发银行\r\n",
      "水星家纺\r\n",
      "大港股份\r\n",
      "海能实业\r\n",
      "荣泰健康\r\n",
      "深纺织A\r\n",
      "贝因美\r\n",
      "东方日升\r\n",
      "天孚通信\r\n",
      "全志科技\r\n",
      "华谊嘉信\r\n",
      "北汽蓝谷\r\n",
      "三圣股份\r\n",
      "隧道股份\r\n",
      "浙江世宝\r\n",
      "风范股份\r\n",
      "露天煤业\r\n",
      "大东海A\r\n",
      "日海智能\r\n",
      "能科股份\r\n",
      "中国核建\r\n",
      "中国高科\r\n",
      "中国化学\r\n",
      "蓝光发展\r\n",
      "云海金属\r\n",
      "大连友谊\r\n",
      "航天机电\r\n",
      "新宏泰\r\n",
      "瑞丰高材\r\n",
      "西藏矿业\r\n",
      "云南旅游\r\n",
      "新湖中宝\r\n",
      "登云股份\r\n",
      "长盈精密\r\n",
      "基蛋生物\r\n",
      "戴维医疗\r\n",
      "捷荣技术\r\n",
      "*ST创兴\r\n",
      "易世达\r\n",
      "浙江震元\r\n",
      "越秀金控\r\n",
      "中青旅\r\n",
      "太空智造\r\n",
      "白云电器\r\n",
      "新洋丰\r\n",
      "渝开发\r\n",
      "新宙邦\r\n",
      "科瑞技术\r\n",
      "英飞特\r\n",
      "天际股份\r\n",
      "攀渝钛业\r\n",
      "雅运股份\r\n",
      "新雷能\r\n",
      "实丰文化\r\n",
      "安达维尔\r\n",
      "赛象科技\r\n",
      "振静股份\r\n",
      "海特生物\r\n",
      "中简科技\r\n",
      "外高桥\r\n",
      "华工科技\r\n",
      "汉得信息\r\n",
      "皖天然气\r\n",
      "秋林集团\r\n",
      "南京银行\r\n",
      "丸美股份\r\n",
      "三特索道\r\n",
      "积成电子\r\n",
      "桂林三金\r\n",
      "经纬纺机\r\n",
      "东宝生物\r\n",
      "申能股份\r\n",
      "东方财富\r\n",
      "佳都科技\r\n",
      "润禾材料\r\n",
      "掌阅科技\r\n",
      "通达动力\r\n",
      "金河生物\r\n",
      "晨鸣纸业\r\n",
      "顺丰控股\r\n",
      "星帅尔\r\n",
      "世联行\r\n",
      "光明地产\r\n",
      "友讯达\r\n",
      "道氏技术\r\n",
      "彩讯股份\r\n",
      "中通客车\r\n",
      "富临精工\r\n",
      "广信材料\r\n",
      "中旗股份\r\n",
      "招商港口\r\n",
      "明德生物\r\n",
      "大众公用\r\n",
      "亚联发展\r\n",
      "越博动力\r\n",
      "晋亿实业\r\n",
      "潜能恒信\r\n",
      "紫金矿业\r\n",
      "华电国际\r\n",
      "金自天正\r\n",
      "西安旅游\r\n",
      "金银河\r\n",
      "良信电器\r\n",
      "马应龙\r\n",
      "同达创业\r\n",
      "浙商证券\r\n",
      "新朋股份\r\n",
      "隆鑫通用\r\n",
      "国联股份\r\n",
      "山西汾酒\r\n",
      "浙江医药\r\n",
      "新兴装备\r\n",
      "深南股份\r\n",
      "兰花科创\r\n",
      "杭锅股份\r\n",
      "青岛港\r\n",
      "神宇股份\r\n",
      "国轩高科\r\n",
      "中国核电\r\n",
      "中泰股份\r\n",
      "林洋能源\r\n",
      "招商证券\r\n",
      "天风证券\r\n",
      "青岛银行\r\n",
      "天安新材\r\n",
      "中国软件\r\n",
      "光正集团\r\n",
      "经纬辉开\r\n",
      "星源材质\r\n",
      "康恩贝\r\n",
      "郑州银行\r\n",
      "凌云股份\r\n",
      "中源协和\r\n",
      "明星电力\r\n",
      "通产丽星\r\n",
      "生意宝\r\n",
      "兰石重装\r\n",
      "建艺集团\r\n",
      "宁波中百\r\n",
      "冀东装备\r\n",
      "三峡新材\r\n",
      "达华智能\r\n",
      "梦百合\r\n",
      "方大炭素\r\n",
      "莱茵生物\r\n",
      "中成股份\r\n",
      "洽洽食品\r\n",
      "红墙股份\r\n",
      "光威复材\r\n",
      "博晖创新\r\n",
      "亿晶光电\r\n",
      "兆丰股份\r\n",
      "新世界\r\n",
      "广汇能源\r\n",
      "易尚展示\r\n",
      "美诺华\r\n",
      "电魂网络\r\n",
      "ST锐电\r\n",
      "九洲电气\r\n",
      "科信技术\r\n",
      "中兴通讯\r\n",
      "上峰水泥\r\n",
      "常熟银行\r\n",
      "康斯特\r\n",
      "海南海药\r\n",
      "中钢国际\r\n",
      "沧州明珠\r\n",
      "西仪股份\r\n",
      "长信科技\r\n",
      "镇海股份\r\n",
      "厦门钨业\r\n",
      "和仁科技\r\n",
      "利德曼\r\n",
      "广日股份\r\n",
      "友邦吊顶\r\n",
      "新光药业\r\n",
      "特力A\r\n",
      "地素时尚\r\n",
      "华胜天成\r\n",
      "川投能源\r\n",
      "乐惠国际\r\n",
      "紫光学大\r\n",
      "仟源医药\r\n",
      "七彩化学\r\n",
      "航天电子\r\n",
      "北方国际\r\n",
      "高澜股份\r\n",
      "宗申动力\r\n",
      "永清环保\r\n",
      "三变科技\r\n",
      "中航资本\r\n",
      "三维工程\r\n",
      "科林电气\r\n",
      "宜安科技\r\n",
      "深高速\r\n",
      "好当家\r\n",
      "中航善达\r\n",
      "华阳国际\r\n",
      "中直股份\r\n",
      "中来股份\r\n",
      "吉宏股份\r\n",
      "雄帝科技\r\n",
      "南京高科\r\n",
      "华通医药\r\n",
      "智云股份\r\n",
      "扬杰科技\r\n",
      "第一医药\r\n",
      "福晶科技\r\n",
      "上海沪工\r\n",
      "亚玛顿\r\n",
      "永辉超市\r\n",
      "春兰股份\r\n",
      "安迪苏\r\n",
      "洛凯股份\r\n",
      "好想你\r\n",
      "勤上股份\r\n",
      "四方精创\r\n",
      "英威腾\r\n",
      "姚记科技\r\n",
      "中建环能\r\n",
      "云内动力\r\n",
      "号百控股\r\n",
      "万科A\r\n",
      "中国铁建\r\n",
      "北陆药业\r\n",
      "禾丰牧业\r\n",
      "平安银行\r\n",
      "本钢板材\r\n",
      "*ST信通\r\n",
      "贵绳股份\r\n",
      "珠江啤酒\r\n",
      "深赛格\r\n",
      "劲嘉股份\r\n",
      "昂利康\r\n",
      "上港集团\r\n",
      "恒逸石化\r\n",
      "桃李面包\r\n",
      "海康威视\r\n",
      "乐凯新材\r\n",
      "通润装备\r\n",
      "兆易创新\r\n",
      "奥瑞金\r\n",
      "庞大集团\r\n",
      "拉芳家化\r\n",
      "渤海租赁\r\n",
      "万马科技\r\n",
      "通化金马\r\n",
      "津膜科技\r\n",
      "金陵饭店\r\n",
      "爱康科技\r\n",
      "江苏银行\r\n",
      "天源迪科\r\n",
      "中船防务\r\n",
      "丽江旅游\r\n",
      "森马服饰\r\n",
      "银邦股份\r\n",
      "中油工程\r\n",
      "凯利泰\r\n",
      "中金黄金\r\n",
      "深圳燃气\r\n",
      "合锻智能\r\n",
      "上汽集团\r\n",
      "浩丰科技\r\n",
      "兴化股份\r\n",
      "齐翔腾达\r\n",
      "普丽盛\r\n",
      "山东出版\r\n",
      "中联重科\r\n",
      "梦网集团\r\n",
      "萃华珠宝\r\n",
      "利源精制\r\n",
      "远大控股\r\n",
      "银轮股份\r\n",
      "博瑞传播\r\n",
      "信捷电气\r\n",
      "优德精密\r\n",
      "南山控股\r\n",
      "瑞斯康达\r\n",
      "国新健康\r\n",
      "海顺新材\r\n",
      "建研院\r\n",
      "金字火腿\r\n",
      "惠天热电\r\n",
      "深粮控股\r\n",
      "广电电气\r\n",
      "浦东金桥\r\n",
      "麦捷科技\r\n",
      "赛意信息\r\n",
      "财通证券\r\n",
      "春兴精工\r\n",
      "英派斯\r\n",
      "汇纳科技\r\n",
      "创力集团\r\n",
      "亿联网络\r\n",
      "圣龙股份\r\n",
      "万邦德\r\n",
      "哈尔斯\r\n",
      "贝达药业\r\n",
      "音飞储存\r\n",
      "美尚生态\r\n",
      "上海新阳\r\n",
      "海汽集团\r\n",
      "德新交运\r\n",
      "安琪酵母\r\n",
      "美利云\r\n",
      "恒天海龙\r\n",
      "智飞生物\r\n",
      "福蓉科技\r\n",
      "中航高科\r\n",
      "用友网络\r\n",
      "一汽轿车\r\n",
      "青青稞酒\r\n",
      "华明装备\r\n",
      "冀东水泥\r\n",
      "华讯方舟\r\n",
      "润和软件\r\n",
      "合盛硅业\r\n",
      "锦江股份\r\n",
      "睿能科技\r\n",
      "民丰特纸\r\n",
      "天业通联\r\n",
      "嘉寓股份\r\n",
      "洛阳玻璃\r\n",
      "同益股份\r\n",
      "鲁亿通\r\n",
      "苏州科达\r\n",
      "金石东方\r\n",
      "康达新材\r\n",
      "国投中鲁\r\n",
      "上海能源\r\n",
      "鲁银投资\r\n",
      "岭南控股\r\n",
      "北新建材\r\n",
      "中泰化学\r\n",
      "*ST毅达\r\n",
      "广东骏亚\r\n",
      "广博股份\r\n",
      "五矿发展\r\n",
      "道明光学\r\n",
      "五粮液\r\n",
      "振德医疗\r\n",
      "北纬科技\r\n",
      "富邦股份\r\n",
      "横店东磁\r\n",
      "华润三九\r\n",
      "亚通股份\r\n",
      "融钰集团\r\n",
      "正虹科技\r\n",
      "韩建河山\r\n",
      "云南能投\r\n",
      "哈投股份\r\n",
      "兴源环境\r\n",
      "华夏幸福\r\n",
      "奥士康\r\n",
      "ST新梅\r\n",
      "杭州解百\r\n",
      "金太阳\r\n",
      "兴蓉环境\r\n",
      "华媒控股\r\n",
      "华自科技\r\n",
      "中光学\r\n",
      "海兰信\r\n",
      "水晶光电\r\n",
      "新天药业\r\n",
      "南天信息\r\n",
      "百利电气\r\n",
      "德赛西威\r\n",
      "三超新材\r\n",
      "隆平高科\r\n",
      "重庆燃气\r\n",
      "中广核技\r\n",
      "理工光科\r\n",
      "联美控股\r\n",
      "海南高速\r\n",
      "胜利精密\r\n",
      "中航飞机\r\n",
      "威唐工业\r\n",
      "鹏翎股份\r\n",
      "ST嘉陵\r\n",
      "方正电机\r\n",
      "深物业A\r\n",
      "*ST哈空\r\n",
      "铁汉生态\r\n",
      "财信发展\r\n",
      "上海梅林\r\n",
      "楚天高速\r\n",
      "旗滨集团\r\n",
      "昭衍新药\r\n",
      "国茂股份\r\n",
      "岷江水电\r\n",
      "凌钢股份\r\n",
      "海利尔\r\n",
      "世荣兆业\r\n",
      "瑞普生物\r\n",
      "万隆光电\r\n",
      "京城股份\r\n",
      "爱婴室\r\n",
      "神马电力\r\n",
      "三六零\r\n",
      "许继电气\r\n",
      "东莞控股\r\n",
      "中珠医疗\r\n",
      "仁和药业\r\n",
      "中信特钢\r\n",
      "四方科技\r\n",
      "富通鑫茂\r\n",
      "苏奥传感"
     ]
    }
   ],
   "source": [
    "!cat /home/work/miniconda3/envs/pycookly2/lib/python3.6/site-packages/nlpcda/data/company.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert4keras.models import build_transformer_model\n",
    "from bert4keras.tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_transformer_model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#! -*- coding: utf-8 -*-\r\n",
      "# 主要模型\r\n",
      "\r\n",
      "import numpy as np\r\n",
      "from bert4keras.layers import *\r\n",
      "from bert4keras.snippets import delete_arguments\r\n",
      "from keras.models import Model\r\n",
      "import json\r\n",
      "\r\n",
      "\r\n",
      "class Transformer(object):\r\n",
      "    \"\"\"模型基类\r\n",
      "    \"\"\"\r\n",
      "    def __init__(\r\n",
      "        self,\r\n",
      "        vocab_size,  # 词表大小\r\n",
      "        hidden_size,  # 编码维度\r\n",
      "        num_hidden_layers,  # Transformer总层数\r\n",
      "        num_attention_heads,  # Attention的头数\r\n",
      "        intermediate_size,  # FeedForward的隐层维度\r\n",
      "        hidden_act,  # FeedForward隐层的激活函数\r\n",
      "        dropout_rate=None,  # Dropout比例\r\n",
      "        embedding_size=None,  # 是否指定embedding_size\r\n",
      "        attention_key_size=None,  # Attention中Q,K的head_size\r\n",
      "        sequence_length=None,  # 是否固定序列长度\r\n",
      "        keep_tokens=None,  # 要保留的词ID列表\r\n",
      "        layers=None,  # 外部传入的Keras层\r\n",
      "        name=None,  # 模型名称\r\n",
      "        **kwargs\r\n",
      "    ):\r\n",
      "        if keep_tokens is None:\r\n",
      "            self.vocab_size = vocab_size\r\n",
      "        else:\r\n",
      "            self.vocab_size = len(keep_tokens)\r\n",
      "        self.hidden_size = hidden_size\r\n",
      "        self.num_hidden_layers = num_hidden_layers\r\n",
      "        self.num_attention_heads = num_attention_heads\r\n",
      "        self.attention_head_size = hidden_size // num_attention_heads\r\n",
      "        self.attention_key_size = attention_key_size or self.attention_head_size\r\n",
      "        self.intermediate_size = intermediate_size\r\n",
      "        self.dropout_rate = dropout_rate or 0\r\n",
      "        self.hidden_act = hidden_act\r\n",
      "        self.embedding_size = embedding_size or hidden_size\r\n",
      "        self.sequence_length = sequence_length\r\n",
      "        self.keep_tokens = keep_tokens\r\n",
      "        self.attention_mask = None\r\n",
      "        self.position_bias = None\r\n",
      "        self.layers = {} if layers is None else layers\r\n",
      "        self.name = name\r\n",
      "        self.built = False\r\n",
      "\r\n",
      "    def build(\r\n",
      "        self,\r\n",
      "        layer_norm_cond=None,\r\n",
      "        layer_norm_cond_hidden_size=None,\r\n",
      "        layer_norm_cond_hidden_act=None,\r\n",
      "        additional_input_layers=None,\r\n",
      "        **kwargs\r\n",
      "    ):\r\n",
      "        \"\"\"模型构建函数\r\n",
      "        layer_norm_*系列参数为实现Conditional Layer Normalization时使用，\r\n",
      "        用来实现以“固定长度向量”为条件的条件Bert。\r\n",
      "        \"\"\"\r\n",
      "        if self.built:\r\n",
      "            return None\r\n",
      "        # Input\r\n",
      "        inputs = self.get_inputs()\r\n",
      "        self.set_inputs(inputs, additional_input_layers)\r\n",
      "        # Other\r\n",
      "        self.layer_norm_conds = [\r\n",
      "            layer_norm_cond,\r\n",
      "            layer_norm_cond_hidden_size,\r\n",
      "            layer_norm_cond_hidden_act or 'linear',\r\n",
      "        ]\r\n",
      "        # Call\r\n",
      "        outputs = self.call(inputs)\r\n",
      "        self.set_outputs(outputs)\r\n",
      "        # Model\r\n",
      "        self.model = Model(self.inputs, self.outputs, name=self.name)\r\n",
      "        self.built = True\r\n",
      "\r\n",
      "    def call(self, inputs):\r\n",
      "        \"\"\"定义模型的执行流程\r\n",
      "        \"\"\"\r\n",
      "        # Embedding\r\n",
      "        outputs = self.apply_embeddings(inputs)\r\n",
      "        # Main\r\n",
      "        for i in range(self.num_hidden_layers):\r\n",
      "            outputs = self.apply_main_layers(outputs, i)\r\n",
      "        # Final\r\n",
      "        outputs = self.apply_final_layers(outputs)\r\n",
      "        return outputs\r\n",
      "\r\n",
      "    def apply(self, inputs, layer=None, arguments=None, **kwargs):\r\n",
      "        \"\"\"通过apply调用层会自动重用同名层\r\n",
      "        inputs: 上一层的输出；\r\n",
      "        layer: 要调用的层类名；\r\n",
      "        arguments: 传递给layer.call的参数；\r\n",
      "        kwargs: 传递给层初始化的参数。\r\n",
      "        \"\"\"\r\n",
      "        if layer is Dropout and self.dropout_rate == 0:\r\n",
      "            return inputs\r\n",
      "\r\n",
      "        arguments = arguments or {}\r\n",
      "        name = kwargs.get('name')\r\n",
      "        if name not in self.layers:\r\n",
      "            layer = layer(**kwargs)\r\n",
      "            name = layer.name\r\n",
      "            self.layers[name] = layer\r\n",
      "\r\n",
      "        return self.layers[name](inputs, **arguments)\r\n",
      "\r\n",
      "    def get_inputs(self):\r\n",
      "        raise NotImplementedError\r\n",
      "\r\n",
      "    def apply_embeddings(self, inputs):\r\n",
      "        raise NotImplementedError\r\n",
      "\r\n",
      "    def apply_main_layers(self, inputs, index):\r\n",
      "        raise NotImplementedError\r\n",
      "\r\n",
      "    def apply_final_layers(self, inputs):\r\n",
      "        raise NotImplementedError\r\n",
      "\r\n",
      "    def compute_attention_mask(self, inputs=None):\r\n",
      "        \"\"\"定义每一层的Attention Mask\r\n",
      "        \"\"\"\r\n",
      "        return self.attention_mask\r\n",
      "\r\n",
      "    def compute_position_bias(self, inputs=None):\r\n",
      "        \"\"\"定义每一层的Position Bias（一般相对位置编码用）\r\n",
      "        \"\"\"\r\n",
      "        return self.position_bias\r\n",
      "\r\n",
      "    def set_inputs(self, inputs, additional_input_layers=None):\r\n",
      "        \"\"\"设置input和inputs属性\r\n",
      "        \"\"\"\r\n",
      "        if inputs is None:\r\n",
      "            inputs = []\r\n",
      "        elif not isinstance(inputs, list):\r\n",
      "            inputs = [inputs]\r\n",
      "\r\n",
      "        inputs = inputs[:]\r\n",
      "        if additional_input_layers is not None:\r\n",
      "            if not isinstance(additional_input_layers, list):\r\n",
      "                additional_input_layers = [additional_input_layers]\r\n",
      "            inputs.extend(additional_input_layers)\r\n",
      "\r\n",
      "        self.inputs = inputs\r\n",
      "        if len(inputs) > 1:\r\n",
      "            self.input = inputs\r\n",
      "        else:\r\n",
      "            self.input = inputs[0]\r\n",
      "\r\n",
      "    def set_outputs(self, outputs):\r\n",
      "        \"\"\"设置output和oututs属性\r\n",
      "        \"\"\"\r\n",
      "        if not isinstance(outputs, list):\r\n",
      "            outputs = [outputs]\r\n",
      "\r\n",
      "        outputs = outputs[:]\r\n",
      "        self.outputs = outputs\r\n",
      "        if len(outputs) > 1:\r\n",
      "            self.output = outputs\r\n",
      "        else:\r\n",
      "            self.output = outputs[0]\r\n",
      "\r\n",
      "    @property\r\n",
      "    def initializer(self):\r\n",
      "        \"\"\"默认使用截断正态分布初始化\r\n",
      "        \"\"\"\r\n",
      "        return keras.initializers.TruncatedNormal(stddev=0.02)\r\n",
      "\r\n",
      "    def simplify(self, inputs):\r\n",
      "        \"\"\"将list中的None过滤掉\r\n",
      "        \"\"\"\r\n",
      "        inputs = [i for i in inputs if i is not None]\r\n",
      "        if len(inputs) == 1:\r\n",
      "            inputs = inputs[0]\r\n",
      "\r\n",
      "        return inputs\r\n",
      "\r\n",
      "    def load_variable(self, checkpoint, name):\r\n",
      "        \"\"\"加载单个变量的函数\r\n",
      "        \"\"\"\r\n",
      "        return tf.train.load_variable(checkpoint, name)\r\n",
      "\r\n",
      "    def create_variable(self, name, value):\r\n",
      "        \"\"\"在tensorflow中创建一个变量\r\n",
      "        \"\"\"\r\n",
      "        return tf.Variable(value, name=name)\r\n",
      "\r\n",
      "    def variable_mapping(self):\r\n",
      "        \"\"\"构建keras层与checkpoint的变量名之间的映射表\r\n",
      "        \"\"\"\r\n",
      "        return {}\r\n",
      "\r\n",
      "    def load_weights_from_checkpoint(self, checkpoint, mapping=None):\r\n",
      "        \"\"\"根据mapping从checkpoint加载权重\r\n",
      "        \"\"\"\r\n",
      "        mapping = mapping or self.variable_mapping()\r\n",
      "        mapping = {k: v for k, v in mapping.items() if k in self.layers}\r\n",
      "\r\n",
      "        weight_value_pairs = []\r\n",
      "        for layer, variables in mapping.items():\r\n",
      "            layer = self.layers[layer]\r\n",
      "            weights = layer.trainable_weights\r\n",
      "            values = [self.load_variable(checkpoint, v) for v in variables]\r\n",
      "\r\n",
      "            if isinstance(layer, MultiHeadAttention):\r\n",
      "                \"\"\"如果key_size不等于head_size，则可以通过\r\n",
      "                正交矩阵将相应的权重投影到合适的shape。\r\n",
      "                \"\"\"\r\n",
      "                count = 2\r\n",
      "                if layer.use_bias:\r\n",
      "                    count += 2\r\n",
      "                heads = self.num_attention_heads\r\n",
      "                head_size = self.attention_head_size\r\n",
      "                key_size = self.attention_key_size\r\n",
      "                W = np.linalg.qr(np.random.randn(key_size, head_size))[0].T\r\n",
      "                if layer.attention_scale:\r\n",
      "                    W = W * key_size**0.25 / head_size**0.25\r\n",
      "                for i in range(count):\r\n",
      "                    w, v = weights[i], values[i]\r\n",
      "                    w_shape, v_shape = K.int_shape(w), v.shape\r\n",
      "                    if w_shape[-1] != v_shape[-1]:\r\n",
      "                        pre_shape = w_shape[:-1]\r\n",
      "                        v = v.reshape(pre_shape + (heads, head_size))\r\n",
      "                        v = np.dot(v, W)\r\n",
      "                        v = v.reshape(pre_shape + (heads * key_size,))\r\n",
      "                        values[i] = v\r\n",
      "\r\n",
      "            weight_value_pairs.extend(zip(weights, values))\r\n",
      "\r\n",
      "        K.batch_set_value(weight_value_pairs)\r\n",
      "\r\n",
      "    def save_weights_as_checkpoint(self, filename, mapping=None):\r\n",
      "        \"\"\"根据mapping将权重保存为checkpoint格式\r\n",
      "        \"\"\"\r\n",
      "        mapping = mapping or self.variable_mapping()\r\n",
      "        mapping = {k: v for k, v in mapping.items() if k in self.layers}\r\n",
      "\r\n",
      "        with tf.Graph().as_default():\r\n",
      "            for layer, variables in mapping.items():\r\n",
      "                layer = self.layers[layer]\r\n",
      "                values = K.batch_get_value(layer.trainable_weights)\r\n",
      "                for name, value in zip(variables, values):\r\n",
      "                    self.create_variable(name, value)\r\n",
      "            with tf.Session() as sess:\r\n",
      "                sess.run(tf.global_variables_initializer())\r\n",
      "                saver = tf.train.Saver()\r\n",
      "                saver.save(sess, filename, write_meta_graph=False)\r\n",
      "\r\n",
      "\r\n",
      "class BERT(Transformer):\r\n",
      "    \"\"\"构建BERT模型\r\n",
      "    \"\"\"\r\n",
      "    def __init__(\r\n",
      "        self,\r\n",
      "        max_position,  # 序列最大长度\r\n",
      "        with_pool=False,  # 是否包含Pool部分\r\n",
      "        with_nsp=False,  # 是否包含NSP部分\r\n",
      "        with_mlm=False,  # 是否包含MLM部分\r\n",
      "        custom_position_ids=False,  # 是否自行传入位置id\r\n",
      "        **kwargs  # 其余参数\r\n",
      "    ):\r\n",
      "        super(BERT, self).__init__(**kwargs)\r\n",
      "        self.max_position = max_position\r\n",
      "        self.with_pool = with_pool\r\n",
      "        self.with_nsp = with_nsp\r\n",
      "        self.with_mlm = with_mlm\r\n",
      "        self.custom_position_ids = custom_position_ids\r\n",
      "\r\n",
      "    def get_inputs(self):\r\n",
      "        \"\"\"BERT的输入是token_ids和segment_ids\r\n",
      "        （但允许自行传入位置id，以实现一些特殊需求）\r\n",
      "        \"\"\"\r\n",
      "        x_in = Input(shape=(self.sequence_length,), name='Input-Token')\r\n",
      "        s_in = Input(shape=(self.sequence_length,), name='Input-Segment')\r\n",
      "\r\n",
      "        if self.custom_position_ids:\r\n",
      "            p_in = Input(shape=(self.sequence_length,), name='Input-Position')\r\n",
      "            return [x_in, s_in, p_in]\r\n",
      "        else:\r\n",
      "            return [x_in, s_in]\r\n",
      "\r\n",
      "    def apply_embeddings(self, inputs):\r\n",
      "        \"\"\"BERT的embedding是token、position、segment三者embedding之和\r\n",
      "        \"\"\"\r\n",
      "        x, s = inputs[:2]\r\n",
      "        z = self.layer_norm_conds[0]\r\n",
      "        if self.custom_position_ids:\r\n",
      "            p = inputs[2]\r\n",
      "        else:\r\n",
      "            p = None\r\n",
      "\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Embedding,\r\n",
      "            input_dim=self.vocab_size,\r\n",
      "            output_dim=self.embedding_size,\r\n",
      "            embeddings_initializer=self.initializer,\r\n",
      "            mask_zero=True,\r\n",
      "            name='Embedding-Token'\r\n",
      "        )\r\n",
      "        s = self.apply(\r\n",
      "            inputs=s,\r\n",
      "            layer=Embedding,\r\n",
      "            input_dim=2,\r\n",
      "            output_dim=self.embedding_size,\r\n",
      "            embeddings_initializer=self.initializer,\r\n",
      "            name='Embedding-Segment'\r\n",
      "        )\r\n",
      "        x = self.apply(inputs=[x, s], layer=Add, name='Embedding-Token-Segment')\r\n",
      "        x = self.apply(\r\n",
      "            inputs=self.simplify([x, p]),\r\n",
      "            layer=PositionEmbedding,\r\n",
      "            input_dim=self.max_position,\r\n",
      "            output_dim=self.embedding_size,\r\n",
      "            merge_mode='add',\r\n",
      "            embeddings_initializer=self.initializer,\r\n",
      "            custom_position_ids=self.custom_position_ids,\r\n",
      "            name='Embedding-Position'\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=self.simplify([x, z]),\r\n",
      "            layer=LayerNormalization,\r\n",
      "            conditional=(z is not None),\r\n",
      "            hidden_units=self.layer_norm_conds[1],\r\n",
      "            hidden_activation=self.layer_norm_conds[2],\r\n",
      "            hidden_initializer=self.initializer,\r\n",
      "            name='Embedding-Norm'\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Dropout,\r\n",
      "            rate=self.dropout_rate,\r\n",
      "            name='Embedding-Dropout'\r\n",
      "        )\r\n",
      "        if self.embedding_size != self.hidden_size:\r\n",
      "            x = self.apply(\r\n",
      "                inputs=x,\r\n",
      "                layer=Dense,\r\n",
      "                units=self.hidden_size,\r\n",
      "                kernel_initializer=self.initializer,\r\n",
      "                name='Embedding-Mapping'\r\n",
      "            )\r\n",
      "\r\n",
      "        return x\r\n",
      "\r\n",
      "    def apply_main_layers(self, inputs, index):\r\n",
      "        \"\"\"BERT的主体是基于Self-Attention的模块\r\n",
      "        顺序：Att --> Add --> LN --> FFN --> Add --> LN\r\n",
      "        \"\"\"\r\n",
      "        x = inputs\r\n",
      "        z = self.layer_norm_conds[0]\r\n",
      "\r\n",
      "        attention_name = 'Transformer-%d-MultiHeadSelfAttention' % index\r\n",
      "        feed_forward_name = 'Transformer-%d-FeedForward' % index\r\n",
      "        attention_mask = self.compute_attention_mask(index)\r\n",
      "\r\n",
      "        # Self Attention\r\n",
      "        xi, x, arguments = x, [x, x, x], {'a_mask': None}\r\n",
      "        if attention_mask is not None:\r\n",
      "            arguments['a_mask'] = True\r\n",
      "            x.append(attention_mask)\r\n",
      "\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=MultiHeadAttention,\r\n",
      "            arguments=arguments,\r\n",
      "            heads=self.num_attention_heads,\r\n",
      "            head_size=self.attention_head_size,\r\n",
      "            key_size=self.attention_key_size,\r\n",
      "            kernel_initializer=self.initializer,\r\n",
      "            name=attention_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Dropout,\r\n",
      "            rate=self.dropout_rate,\r\n",
      "            name='%s-Dropout' % attention_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=[xi, x], layer=Add, name='%s-Add' % attention_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=self.simplify([x, z]),\r\n",
      "            layer=LayerNormalization,\r\n",
      "            conditional=(z is not None),\r\n",
      "            hidden_units=self.layer_norm_conds[1],\r\n",
      "            hidden_activation=self.layer_norm_conds[2],\r\n",
      "            hidden_initializer=self.initializer,\r\n",
      "            name='%s-Norm' % attention_name\r\n",
      "        )\r\n",
      "\r\n",
      "        # Feed Forward\r\n",
      "        xi = x\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=FeedForward,\r\n",
      "            units=self.intermediate_size,\r\n",
      "            activation=self.hidden_act,\r\n",
      "            kernel_initializer=self.initializer,\r\n",
      "            name=feed_forward_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Dropout,\r\n",
      "            rate=self.dropout_rate,\r\n",
      "            name='%s-Dropout' % feed_forward_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=[xi, x], layer=Add, name='%s-Add' % feed_forward_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=self.simplify([x, z]),\r\n",
      "            layer=LayerNormalization,\r\n",
      "            conditional=(z is not None),\r\n",
      "            hidden_units=self.layer_norm_conds[1],\r\n",
      "            hidden_activation=self.layer_norm_conds[2],\r\n",
      "            hidden_initializer=self.initializer,\r\n",
      "            name='%s-Norm' % feed_forward_name\r\n",
      "        )\r\n",
      "\r\n",
      "        return x\r\n",
      "\r\n",
      "    def apply_final_layers(self, inputs):\r\n",
      "        \"\"\"根据剩余参数决定输出\r\n",
      "        \"\"\"\r\n",
      "        x = inputs\r\n",
      "        z = self.layer_norm_conds[0]\r\n",
      "        outputs = [x]\r\n",
      "\r\n",
      "        if self.with_pool or self.with_nsp:\r\n",
      "            # Pooler部分（提取CLS向量）\r\n",
      "            x = outputs[0]\r\n",
      "            x = self.apply(\r\n",
      "                inputs=x,\r\n",
      "                layer=Lambda,\r\n",
      "                function=lambda x: x[:, 0],\r\n",
      "                name='Pooler'\r\n",
      "            )\r\n",
      "            pool_activation = 'tanh' if self.with_pool is True else self.with_pool\r\n",
      "            x = self.apply(\r\n",
      "                inputs=x,\r\n",
      "                layer=Dense,\r\n",
      "                units=self.hidden_size,\r\n",
      "                activation=pool_activation,\r\n",
      "                kernel_initializer=self.initializer,\r\n",
      "                name='Pooler-Dense'\r\n",
      "            )\r\n",
      "            if self.with_nsp:\r\n",
      "                # Next Sentence Prediction部分\r\n",
      "                x = self.apply(\r\n",
      "                    inputs=x,\r\n",
      "                    layer=Dense,\r\n",
      "                    units=2,\r\n",
      "                    activation='softmax',\r\n",
      "                    kernel_initializer=self.initializer,\r\n",
      "                    name='NSP-Proba'\r\n",
      "                )\r\n",
      "            outputs.append(x)\r\n",
      "\r\n",
      "        if self.with_mlm:\r\n",
      "            # Masked Language Model部分\r\n",
      "            x = outputs[0]\r\n",
      "            x = self.apply(\r\n",
      "                inputs=x,\r\n",
      "                layer=Dense,\r\n",
      "                units=self.embedding_size,\r\n",
      "                activation=self.hidden_act,\r\n",
      "                kernel_initializer=self.initializer,\r\n",
      "                name='MLM-Dense'\r\n",
      "            )\r\n",
      "            x = self.apply(\r\n",
      "                inputs=self.simplify([x, z]),\r\n",
      "                layer=LayerNormalization,\r\n",
      "                conditional=(z is not None),\r\n",
      "                hidden_units=self.layer_norm_conds[1],\r\n",
      "                hidden_activation=self.layer_norm_conds[2],\r\n",
      "                hidden_initializer=self.initializer,\r\n",
      "                name='MLM-Norm'\r\n",
      "            )\r\n",
      "            x = self.apply(\r\n",
      "                inputs=x,\r\n",
      "                layer=Embedding,\r\n",
      "                arguments={'mode': 'dense'},\r\n",
      "                name='Embedding-Token'\r\n",
      "            )\r\n",
      "            x = self.apply(inputs=x, layer=BiasAdd, name='MLM-Bias')\r\n",
      "            mlm_activation = 'softmax' if self.with_mlm is True else self.with_mlm\r\n",
      "            x = self.apply(\r\n",
      "                inputs=x,\r\n",
      "                layer=Activation,\r\n",
      "                activation=mlm_activation,\r\n",
      "                name='MLM-Activation'\r\n",
      "            )\r\n",
      "            outputs.append(x)\r\n",
      "\r\n",
      "        if len(outputs) == 1:\r\n",
      "            outputs = outputs[0]\r\n",
      "        elif len(outputs) == 2:\r\n",
      "            outputs = outputs[1]\r\n",
      "        else:\r\n",
      "            outputs = outputs[1:]\r\n",
      "\r\n",
      "        return outputs\r\n",
      "\r\n",
      "    def load_variable(self, checkpoint, name):\r\n",
      "        \"\"\"加载单个变量的函数\r\n",
      "        \"\"\"\r\n",
      "        variable = super(BERT, self).load_variable(checkpoint, name)\r\n",
      "        if name in [\r\n",
      "            'bert/embeddings/word_embeddings',\r\n",
      "            'cls/predictions/output_bias',\r\n",
      "        ]:\r\n",
      "            if self.keep_tokens is None:\r\n",
      "                return variable\r\n",
      "            else:\r\n",
      "                return variable[self.keep_tokens]\r\n",
      "        elif name == 'cls/seq_relationship/output_weights':\r\n",
      "            return variable.T\r\n",
      "        else:\r\n",
      "            return variable\r\n",
      "\r\n",
      "    def create_variable(self, name, value):\r\n",
      "        \"\"\"在tensorflow中创建一个变量\r\n",
      "        \"\"\"\r\n",
      "        if name == 'cls/seq_relationship/output_weights':\r\n",
      "            value = value.T\r\n",
      "        return super(BERT, self).create_variable(name, value)\r\n",
      "\r\n",
      "    def variable_mapping(self):\r\n",
      "        \"\"\"映射到官方BERT权重格式\r\n",
      "        \"\"\"\r\n",
      "        mapping = {\r\n",
      "            'Embedding-Token': ['bert/embeddings/word_embeddings'],\r\n",
      "            'Embedding-Segment': ['bert/embeddings/token_type_embeddings'],\r\n",
      "            'Embedding-Position': ['bert/embeddings/position_embeddings'],\r\n",
      "            'Embedding-Norm': [\r\n",
      "                'bert/embeddings/LayerNorm/beta',\r\n",
      "                'bert/embeddings/LayerNorm/gamma',\r\n",
      "            ],\r\n",
      "            'Embedding-Mapping': [\r\n",
      "                'bert/encoder/embedding_hidden_mapping_in/kernel',\r\n",
      "                'bert/encoder/embedding_hidden_mapping_in/bias',\r\n",
      "            ],\r\n",
      "            'Pooler-Dense': [\r\n",
      "                'bert/pooler/dense/kernel',\r\n",
      "                'bert/pooler/dense/bias',\r\n",
      "            ],\r\n",
      "            'NSP-Proba': [\r\n",
      "                'cls/seq_relationship/output_weights',\r\n",
      "                'cls/seq_relationship/output_bias',\r\n",
      "            ],\r\n",
      "            'MLM-Dense': [\r\n",
      "                'cls/predictions/transform/dense/kernel',\r\n",
      "                'cls/predictions/transform/dense/bias',\r\n",
      "            ],\r\n",
      "            'MLM-Norm': [\r\n",
      "                'cls/predictions/transform/LayerNorm/beta',\r\n",
      "                'cls/predictions/transform/LayerNorm/gamma',\r\n",
      "            ],\r\n",
      "            'MLM-Bias': ['cls/predictions/output_bias'],\r\n",
      "        }\r\n",
      "\r\n",
      "        for i in range(self.num_hidden_layers):\r\n",
      "            prefix = 'bert/encoder/layer_%d/' % i\r\n",
      "            mapping.update({\r\n",
      "                'Transformer-%d-MultiHeadSelfAttention' % i: [\r\n",
      "                    prefix + 'attention/self/query/kernel',\r\n",
      "                    prefix + 'attention/self/query/bias',\r\n",
      "                    prefix + 'attention/self/key/kernel',\r\n",
      "                    prefix + 'attention/self/key/bias',\r\n",
      "                    prefix + 'attention/self/value/kernel',\r\n",
      "                    prefix + 'attention/self/value/bias',\r\n",
      "                    prefix + 'attention/output/dense/kernel',\r\n",
      "                    prefix + 'attention/output/dense/bias',\r\n",
      "                ],\r\n",
      "                'Transformer-%d-MultiHeadSelfAttention-Norm' % i: [\r\n",
      "                    prefix + 'attention/output/LayerNorm/beta',\r\n",
      "                    prefix + 'attention/output/LayerNorm/gamma',\r\n",
      "                ],\r\n",
      "                'Transformer-%d-FeedForward' % i: [\r\n",
      "                    prefix + 'intermediate/dense/kernel',\r\n",
      "                    prefix + 'intermediate/dense/bias',\r\n",
      "                    prefix + 'output/dense/kernel',\r\n",
      "                    prefix + 'output/dense/bias',\r\n",
      "                ],\r\n",
      "                'Transformer-%d-FeedForward-Norm' % i: [\r\n",
      "                    prefix + 'output/LayerNorm/beta',\r\n",
      "                    prefix + 'output/LayerNorm/gamma',\r\n",
      "                ],\r\n",
      "            })\r\n",
      "\r\n",
      "        return mapping\r\n",
      "\r\n",
      "\r\n",
      "class ALBERT(BERT):\r\n",
      "    \"\"\"构建ALBERT模型\r\n",
      "    \"\"\"\r\n",
      "    def apply_main_layers(self, inputs, index):\r\n",
      "        \"\"\"ALBERT的主体是基于Self-Attention的模块\r\n",
      "        顺序：Att --> Add --> LN --> FFN --> Add --> LN\r\n",
      "        \"\"\"\r\n",
      "        x = inputs\r\n",
      "        z = self.layer_norm_conds[0]\r\n",
      "\r\n",
      "        attention_name = 'Transformer-MultiHeadSelfAttention'\r\n",
      "        feed_forward_name = 'Transformer-FeedForward'\r\n",
      "        attention_mask = self.compute_attention_mask(index)\r\n",
      "\r\n",
      "        # Self Attention\r\n",
      "        xi, x, arguments = x, [x, x, x], {'a_mask': None}\r\n",
      "        if attention_mask is not None:\r\n",
      "            arguments['a_mask'] = True\r\n",
      "            x.append(attention_mask)\r\n",
      "\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=MultiHeadAttention,\r\n",
      "            arguments=arguments,\r\n",
      "            heads=self.num_attention_heads,\r\n",
      "            head_size=self.attention_head_size,\r\n",
      "            key_size=self.attention_key_size,\r\n",
      "            kernel_initializer=self.initializer,\r\n",
      "            name=attention_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Dropout,\r\n",
      "            rate=self.dropout_rate,\r\n",
      "            name='%s-Dropout' % attention_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=[xi, x], layer=Add, name='%s-Add' % attention_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=self.simplify([x, z]),\r\n",
      "            layer=LayerNormalization,\r\n",
      "            conditional=(z is not None),\r\n",
      "            hidden_units=self.layer_norm_conds[1],\r\n",
      "            hidden_activation=self.layer_norm_conds[2],\r\n",
      "            hidden_initializer=self.initializer,\r\n",
      "            name='%s-Norm' % attention_name\r\n",
      "        )\r\n",
      "\r\n",
      "        # Feed Forward\r\n",
      "        xi = x\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=FeedForward,\r\n",
      "            units=self.intermediate_size,\r\n",
      "            activation=self.hidden_act,\r\n",
      "            kernel_initializer=self.initializer,\r\n",
      "            name=feed_forward_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Dropout,\r\n",
      "            rate=self.dropout_rate,\r\n",
      "            name='%s-Dropout' % feed_forward_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=[xi, x], layer=Add, name='%s-Add' % feed_forward_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=self.simplify([x, z]),\r\n",
      "            layer=LayerNormalization,\r\n",
      "            conditional=(z is not None),\r\n",
      "            hidden_units=self.layer_norm_conds[1],\r\n",
      "            hidden_activation=self.layer_norm_conds[2],\r\n",
      "            hidden_initializer=self.initializer,\r\n",
      "            name='%s-Norm' % feed_forward_name\r\n",
      "        )\r\n",
      "\r\n",
      "        return x\r\n",
      "\r\n",
      "    def variable_mapping(self):\r\n",
      "        \"\"\"映射到官方ALBERT权重格式\r\n",
      "        \"\"\"\r\n",
      "        mapping = super(ALBERT, self).variable_mapping()\r\n",
      "\r\n",
      "        prefix = 'bert/encoder/transformer/group_0/inner_group_0/'\r\n",
      "        mapping.update({\r\n",
      "            'Transformer-MultiHeadSelfAttention': [\r\n",
      "                prefix + 'attention_1/self/query/kernel',\r\n",
      "                prefix + 'attention_1/self/query/bias',\r\n",
      "                prefix + 'attention_1/self/key/kernel',\r\n",
      "                prefix + 'attention_1/self/key/bias',\r\n",
      "                prefix + 'attention_1/self/value/kernel',\r\n",
      "                prefix + 'attention_1/self/value/bias',\r\n",
      "                prefix + 'attention_1/output/dense/kernel',\r\n",
      "                prefix + 'attention_1/output/dense/bias',\r\n",
      "            ],\r\n",
      "            'Transformer-MultiHeadSelfAttention-Norm': [\r\n",
      "                prefix + 'LayerNorm/beta',\r\n",
      "                prefix + 'LayerNorm/gamma',\r\n",
      "            ],\r\n",
      "            'Transformer-FeedForward': [\r\n",
      "                prefix + 'ffn_1/intermediate/dense/kernel',\r\n",
      "                prefix + 'ffn_1/intermediate/dense/bias',\r\n",
      "                prefix + 'ffn_1/intermediate/output/dense/kernel',\r\n",
      "                prefix + 'ffn_1/intermediate/output/dense/bias',\r\n",
      "            ],\r\n",
      "            'Transformer-FeedForward-Norm': [\r\n",
      "                prefix + 'LayerNorm_1/beta',\r\n",
      "                prefix + 'LayerNorm_1/gamma',\r\n",
      "            ],\r\n",
      "        })\r\n",
      "\r\n",
      "        return mapping\r\n",
      "\r\n",
      "\r\n",
      "class ALBERT_Unshared(BERT):\r\n",
      "    \"\"\"解开ALBERT共享约束，当成BERT用\r\n",
      "    \"\"\"\r\n",
      "    def variable_mapping(self):\r\n",
      "        \"\"\"映射到官方ALBERT权重格式\r\n",
      "        \"\"\"\r\n",
      "        mapping = super(ALBERT_Unshared, self).variable_mapping()\r\n",
      "\r\n",
      "        prefix = 'bert/encoder/transformer/group_0/inner_group_0/'\r\n",
      "        for i in range(self.num_hidden_layers):\r\n",
      "            mapping.update({\r\n",
      "                'Transformer-%d-MultiHeadSelfAttention' % i: [\r\n",
      "                    prefix + 'attention_1/self/query/kernel',\r\n",
      "                    prefix + 'attention_1/self/query/bias',\r\n",
      "                    prefix + 'attention_1/self/key/kernel',\r\n",
      "                    prefix + 'attention_1/self/key/bias',\r\n",
      "                    prefix + 'attention_1/self/value/kernel',\r\n",
      "                    prefix + 'attention_1/self/value/bias',\r\n",
      "                    prefix + 'attention_1/output/dense/kernel',\r\n",
      "                    prefix + 'attention_1/output/dense/bias',\r\n",
      "                ],\r\n",
      "                'Transformer-%d-MultiHeadSelfAttention-Norm' % i: [\r\n",
      "                    prefix + 'LayerNorm/beta',\r\n",
      "                    prefix + 'LayerNorm/gamma',\r\n",
      "                ],\r\n",
      "                'Transformer-%d-FeedForward' % i: [\r\n",
      "                    prefix + 'ffn_1/intermediate/dense/kernel',\r\n",
      "                    prefix + 'ffn_1/intermediate/dense/bias',\r\n",
      "                    prefix + 'ffn_1/intermediate/output/dense/kernel',\r\n",
      "                    prefix + 'ffn_1/intermediate/output/dense/bias',\r\n",
      "                ],\r\n",
      "                'Transformer-%d-FeedForward-Norm' % i: [\r\n",
      "                    prefix + 'LayerNorm_1/beta',\r\n",
      "                    prefix + 'LayerNorm_1/gamma',\r\n",
      "                ],\r\n",
      "            })\r\n",
      "\r\n",
      "        return mapping\r\n",
      "\r\n",
      "\r\n",
      "class NEZHA(BERT):\r\n",
      "    \"\"\"华为推出的NAZHA模型\r\n",
      "    链接：https://arxiv.org/abs/1909.00204\r\n",
      "    \"\"\"\r\n",
      "    def apply_embeddings(self, inputs):\r\n",
      "        \"\"\"NEZHA的embedding是token、segment两者embedding之和\r\n",
      "        \"\"\"\r\n",
      "        x, s = inputs\r\n",
      "        z = self.layer_norm_conds[0]\r\n",
      "\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Embedding,\r\n",
      "            input_dim=self.vocab_size,\r\n",
      "            output_dim=self.embedding_size,\r\n",
      "            embeddings_initializer=self.initializer,\r\n",
      "            mask_zero=True,\r\n",
      "            name='Embedding-Token'\r\n",
      "        )\r\n",
      "        s = self.apply(\r\n",
      "            inputs=s,\r\n",
      "            layer=Embedding,\r\n",
      "            input_dim=2,\r\n",
      "            output_dim=self.embedding_size,\r\n",
      "            embeddings_initializer=self.initializer,\r\n",
      "            name='Embedding-Segment'\r\n",
      "        )\r\n",
      "        x = self.apply(inputs=[x, s], layer=Add, name='Embedding-Token-Segment')\r\n",
      "        x = self.apply(\r\n",
      "            inputs=self.simplify([x, z]),\r\n",
      "            layer=LayerNormalization,\r\n",
      "            conditional=(z is not None),\r\n",
      "            hidden_units=self.layer_norm_conds[1],\r\n",
      "            hidden_activation=self.layer_norm_conds[2],\r\n",
      "            hidden_initializer=self.initializer,\r\n",
      "            name='Embedding-Norm'\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Dropout,\r\n",
      "            rate=self.dropout_rate,\r\n",
      "            name='Embedding-Dropout'\r\n",
      "        )\r\n",
      "        if self.embedding_size != self.hidden_size:\r\n",
      "            x = self.apply(\r\n",
      "                inputs=x,\r\n",
      "                layer=Dense,\r\n",
      "                units=self.hidden_size,\r\n",
      "                kernel_initializer=self.initializer,\r\n",
      "                name='Embedding-Mapping'\r\n",
      "            )\r\n",
      "\r\n",
      "        return x\r\n",
      "\r\n",
      "    def apply_main_layers(self, inputs, index):\r\n",
      "        \"\"\"NEZHA的主体是基于Self-Attention的模块\r\n",
      "        顺序：Att --> Add --> LN --> FFN --> Add --> LN\r\n",
      "        \"\"\"\r\n",
      "        x = inputs\r\n",
      "        z = self.layer_norm_conds[0]\r\n",
      "\r\n",
      "        attention_name = 'Transformer-%d-MultiHeadSelfAttention' % index\r\n",
      "        feed_forward_name = 'Transformer-%d-FeedForward' % index\r\n",
      "        attention_mask = self.compute_attention_mask(index)\r\n",
      "        position_bias = self.compute_position_bias(x)\r\n",
      "\r\n",
      "        # Self Attention\r\n",
      "        xi, x = x, [x, x, x, position_bias]\r\n",
      "        arguments = {'a_mask': None, 'p_bias': 'typical_relative'}\r\n",
      "        if attention_mask is not None:\r\n",
      "            arguments['a_mask'] = True\r\n",
      "            x.insert(3, attention_mask)\r\n",
      "\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=MultiHeadAttention,\r\n",
      "            arguments=arguments,\r\n",
      "            heads=self.num_attention_heads,\r\n",
      "            head_size=self.attention_head_size,\r\n",
      "            key_size=self.attention_key_size,\r\n",
      "            kernel_initializer=self.initializer,\r\n",
      "            name=attention_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Dropout,\r\n",
      "            rate=self.dropout_rate,\r\n",
      "            name='%s-Dropout' % attention_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=[xi, x], layer=Add, name='%s-Add' % attention_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=self.simplify([x, z]),\r\n",
      "            layer=LayerNormalization,\r\n",
      "            conditional=(z is not None),\r\n",
      "            hidden_units=self.layer_norm_conds[1],\r\n",
      "            hidden_activation=self.layer_norm_conds[2],\r\n",
      "            hidden_initializer=self.initializer,\r\n",
      "            name='%s-Norm' % attention_name\r\n",
      "        )\r\n",
      "\r\n",
      "        # Feed Forward\r\n",
      "        xi = x\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=FeedForward,\r\n",
      "            units=self.intermediate_size,\r\n",
      "            activation=self.hidden_act,\r\n",
      "            kernel_initializer=self.initializer,\r\n",
      "            name=feed_forward_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Dropout,\r\n",
      "            rate=self.dropout_rate,\r\n",
      "            name='%s-Dropout' % feed_forward_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=[xi, x], layer=Add, name='%s-Add' % feed_forward_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=self.simplify([x, z]),\r\n",
      "            layer=LayerNormalization,\r\n",
      "            conditional=(z is not None),\r\n",
      "            hidden_units=self.layer_norm_conds[1],\r\n",
      "            hidden_activation=self.layer_norm_conds[2],\r\n",
      "            hidden_initializer=self.initializer,\r\n",
      "            name='%s-Norm' % feed_forward_name\r\n",
      "        )\r\n",
      "\r\n",
      "        return x\r\n",
      "\r\n",
      "    def compute_position_bias(self, inputs=None):\r\n",
      "        \"\"\"经典相对位置编码\r\n",
      "        \"\"\"\r\n",
      "        if self.position_bias is None:\r\n",
      "\r\n",
      "            def sinusoidal(shape, dtype=None):\r\n",
      "                \"\"\"NEZHA直接使用Sin-Cos形式的位置向量\r\n",
      "                \"\"\"\r\n",
      "                vocab_size, depth = shape\r\n",
      "                embeddings = np.zeros(shape)\r\n",
      "                for pos in range(vocab_size):\r\n",
      "                    for i in range(depth // 2):\r\n",
      "                        theta = pos / np.power(10000, 2. * i / depth)\r\n",
      "                        embeddings[pos, 2 * i] = np.sin(theta)\r\n",
      "                        embeddings[pos, 2 * i + 1] = np.cos(theta)\r\n",
      "                return embeddings\r\n",
      "\r\n",
      "            x = inputs\r\n",
      "            self.position_bias = self.apply(\r\n",
      "                inputs=[x, x],\r\n",
      "                layer=RelativePositionEmbedding,\r\n",
      "                input_dim=2 * 64 + 1,\r\n",
      "                output_dim=self.attention_head_size,\r\n",
      "                embeddings_initializer=sinusoidal,\r\n",
      "                name='Embedding-Relative-Position',\r\n",
      "                trainable=False\r\n",
      "            )\r\n",
      "\r\n",
      "        return self.position_bias\r\n",
      "\r\n",
      "\r\n",
      "class ELECTRA(BERT):\r\n",
      "    \"\"\"Google推出的ELECTRA模型\r\n",
      "    链接：https://arxiv.org/abs/2003.10555\r\n",
      "    \"\"\"\r\n",
      "    @delete_arguments('with_pool', 'with_mlm')\r\n",
      "    def __init__(\r\n",
      "        self,\r\n",
      "        max_position,  # 序列最大长度\r\n",
      "        **kwargs  # 其余参数\r\n",
      "    ):\r\n",
      "        if 'keep_tokens' in kwargs:\r\n",
      "            del kwargs['keep_tokens']\r\n",
      "\r\n",
      "        super(ELECTRA, self).__init__(max_position, **kwargs)\r\n",
      "\r\n",
      "    def apply_final_layers(self, inputs):\r\n",
      "        x = inputs\r\n",
      "        z = self.layer_norm_conds[0]\r\n",
      "        return x\r\n",
      "\r\n",
      "    def variable_mapping(self):\r\n",
      "        mapping = super(ELECTRA, self).variable_mapping()\r\n",
      "        mapping['Embedding-Mapping'] = [\r\n",
      "            'electra/embeddings_project/kernel',\r\n",
      "            'electra/embeddings_project/bias',\r\n",
      "        ]\r\n",
      "        mapping = {\r\n",
      "            k: [i.replace('bert/', 'electra/') for i in v]\r\n",
      "            for k, v in mapping.items()\r\n",
      "        }\r\n",
      "        return mapping\r\n",
      "\r\n",
      "\r\n",
      "class GPT2_ML(Transformer):\r\n",
      "    \"\"\"构建GPT2_ML模型\r\n",
      "    链接: https://github.com/imcaspar/gpt2-ml\r\n",
      "    \"\"\"\r\n",
      "    def __init__(\r\n",
      "        self,\r\n",
      "        max_position,  # 序列最大长度\r\n",
      "        final_activation='softmax',  # 预测分布的激活函数\r\n",
      "        **kwargs  # 其余参数\r\n",
      "    ):\r\n",
      "        super(GPT2_ML, self).__init__(**kwargs)\r\n",
      "        self.max_position = max_position\r\n",
      "        self.final_activation = final_activation\r\n",
      "\r\n",
      "    def get_inputs(self):\r\n",
      "        \"\"\"GPT2_ML的输入是token_ids和segment_ids\r\n",
      "        \"\"\"\r\n",
      "        x_in = Input(shape=(self.sequence_length,), name='Input-Token')\r\n",
      "        return x_in\r\n",
      "\r\n",
      "    def apply_embeddings(self, inputs):\r\n",
      "        \"\"\"GPT2_ML的embedding是token、position两者embedding之和\r\n",
      "        \"\"\"\r\n",
      "        x = inputs\r\n",
      "        z = self.layer_norm_conds[0]\r\n",
      "\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Embedding,\r\n",
      "            input_dim=self.vocab_size,\r\n",
      "            output_dim=self.embedding_size,\r\n",
      "            embeddings_initializer=self.initializer,\r\n",
      "            mask_zero=True,\r\n",
      "            name='Embedding-Token'\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=PositionEmbedding,\r\n",
      "            input_dim=self.max_position,\r\n",
      "            output_dim=self.embedding_size,\r\n",
      "            merge_mode='add',\r\n",
      "            embeddings_initializer=self.initializer,\r\n",
      "            name='Embedding-Position'\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=self.simplify([x, z]),\r\n",
      "            layer=LayerNormalization,\r\n",
      "            epsilon=1e-5,\r\n",
      "            conditional=(z is not None),\r\n",
      "            hidden_units=self.layer_norm_conds[1],\r\n",
      "            hidden_activation=self.layer_norm_conds[2],\r\n",
      "            hidden_initializer=self.initializer,\r\n",
      "            name='Embedding-Norm'\r\n",
      "        )\r\n",
      "        if self.embedding_size != self.hidden_size:\r\n",
      "            x = self.apply(\r\n",
      "                inputs=x,\r\n",
      "                layer=Dense,\r\n",
      "                units=self.hidden_size,\r\n",
      "                kernel_initializer=self.initializer,\r\n",
      "                name='Embedding-Mapping'\r\n",
      "            )\r\n",
      "\r\n",
      "        return x\r\n",
      "\r\n",
      "    def apply_main_layers(self, inputs, index):\r\n",
      "        \"\"\"GPT2_ML的主体是基于Self-Attention的模块\r\n",
      "        顺序：Att  --> LN --> FFN --> Add --> LN\r\n",
      "        \"\"\"\r\n",
      "        x = inputs\r\n",
      "        z = self.layer_norm_conds[0]\r\n",
      "\r\n",
      "        attention_name = 'Transformer-%d-MultiHeadSelfAttention' % index\r\n",
      "        feed_forward_name = 'Transformer-%d-FeedForward' % index\r\n",
      "        attention_mask = self.compute_attention_mask(index)\r\n",
      "\r\n",
      "        # Self Attention\r\n",
      "        xi, x, arguments = x, [x, x, x, attention_mask], {'a_mask': True}\r\n",
      "\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=MultiHeadAttention,\r\n",
      "            arguments=arguments,\r\n",
      "            heads=self.num_attention_heads,\r\n",
      "            head_size=self.attention_head_size,\r\n",
      "            key_size=self.attention_key_size,\r\n",
      "            kernel_initializer=self.initializer,\r\n",
      "            name=attention_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Dropout,\r\n",
      "            rate=self.dropout_rate,\r\n",
      "            name='%s-Dropout' % attention_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=[xi, x], layer=Add, name='%s-Add' % attention_name\r\n",
      "        )\r\n",
      "\r\n",
      "        # Feed Forward\r\n",
      "        xi = x\r\n",
      "        x = self.apply(\r\n",
      "            inputs=self.simplify([x, z]),\r\n",
      "            layer=LayerNormalization,\r\n",
      "            epsilon=1e-5,\r\n",
      "            conditional=(z is not None),\r\n",
      "            hidden_units=self.layer_norm_conds[1],\r\n",
      "            hidden_activation=self.layer_norm_conds[2],\r\n",
      "            hidden_initializer=self.initializer,\r\n",
      "            name='%s-Norm-0' % feed_forward_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=FeedForward,\r\n",
      "            units=self.intermediate_size,\r\n",
      "            activation=self.hidden_act,\r\n",
      "            kernel_initializer=self.initializer,\r\n",
      "            name=feed_forward_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Dropout,\r\n",
      "            rate=self.dropout_rate,\r\n",
      "            name='%s-Dropout' % feed_forward_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=[xi, x], layer=Add, name='%s-Add' % feed_forward_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=self.simplify([x, z]),\r\n",
      "            layer=LayerNormalization,\r\n",
      "            epsilon=1e-5,\r\n",
      "            conditional=(z is not None),\r\n",
      "            hidden_units=self.layer_norm_conds[1],\r\n",
      "            hidden_activation=self.layer_norm_conds[2],\r\n",
      "            hidden_initializer=self.initializer,\r\n",
      "            name='%s-Norm-1' % feed_forward_name\r\n",
      "        )\r\n",
      "\r\n",
      "        return x\r\n",
      "\r\n",
      "    def apply_final_layers(self, inputs):\r\n",
      "        \"\"\"剩余部分\r\n",
      "        \"\"\"\r\n",
      "        x = inputs\r\n",
      "        z = self.layer_norm_conds[0]\r\n",
      "\r\n",
      "        # Language Model部分\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Embedding,\r\n",
      "            arguments={'mode': 'dense'},\r\n",
      "            name='Embedding-Token'\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Activation,\r\n",
      "            activation=self.final_activation,\r\n",
      "            name='LM-Activation'\r\n",
      "        )\r\n",
      "\r\n",
      "        return x\r\n",
      "\r\n",
      "    def load_variable(self, checkpoint, name):\r\n",
      "        \"\"\"加载单个变量的函数\r\n",
      "        \"\"\"\r\n",
      "        variable = super(GPT2_ML, self).load_variable(checkpoint, name)\r\n",
      "        if name == 'newslm/embeddings/word_embed':\r\n",
      "            if self.keep_tokens is None:\r\n",
      "                return variable\r\n",
      "            else:\r\n",
      "                return variable[self.keep_tokens]\r\n",
      "        else:\r\n",
      "            return variable\r\n",
      "\r\n",
      "    def compute_attention_mask(self, inputs=None):\r\n",
      "        \"\"\"添加下三角形式的attention mask\r\n",
      "        \"\"\"\r\n",
      "        if self.attention_mask is None:\r\n",
      "\r\n",
      "            def lm_mask(s):\r\n",
      "                seq_len = K.shape(s)[1]\r\n",
      "                idxs = K.arange(0, seq_len)\r\n",
      "                mask = idxs[None, :] <= idxs[:, None]\r\n",
      "                mask = K.cast(mask, K.floatx())\r\n",
      "                return mask[None, None]\r\n",
      "\r\n",
      "            self.attention_mask = self.apply(\r\n",
      "                inputs=self.inputs[0],\r\n",
      "                layer=Lambda,\r\n",
      "                function=lm_mask,\r\n",
      "                name='Attention-LM-Mask'\r\n",
      "            )\r\n",
      "\r\n",
      "        return self.attention_mask\r\n",
      "\r\n",
      "    def variable_mapping(self):\r\n",
      "        \"\"\"映射到官方GPT2_ML权重格式\r\n",
      "        \"\"\"\r\n",
      "        mapping = {\r\n",
      "            'Embedding-Token': ['newslm/embeddings/word_embed'],\r\n",
      "            'Embedding-Position': ['newslm/embeddings/pos_embed'],\r\n",
      "            'Embedding-Norm': [\r\n",
      "                'newslm/embeddings/LayerNorm_embed_norm/beta',\r\n",
      "                'newslm/embeddings/LayerNorm_embed_norm/gamma',\r\n",
      "            ],\r\n",
      "        }\r\n",
      "\r\n",
      "        for i in range(self.num_hidden_layers):\r\n",
      "            prefix = 'newslm/layer%02d/' % i\r\n",
      "            mapping.update({\r\n",
      "                'Transformer-%d-MultiHeadSelfAttention' % i: [\r\n",
      "                    prefix + 'query_layer/kernel',\r\n",
      "                    prefix + 'query_layer/bias',\r\n",
      "                    prefix + 'key_layer/kernel',\r\n",
      "                    prefix + 'key_layer/bias',\r\n",
      "                    prefix + 'value_layer/kernel',\r\n",
      "                    prefix + 'value_layer/bias',\r\n",
      "                    prefix + 'context_projection_layer/kernel',\r\n",
      "                    prefix + 'context_projection_layer/bias',\r\n",
      "                ],\r\n",
      "                'Transformer-%d-FeedForward-Norm-0' % i: [\r\n",
      "                    prefix + 'LayerNorm_mlp_ln0/beta',\r\n",
      "                    prefix + 'LayerNorm_mlp_ln0/gamma',\r\n",
      "                ],\r\n",
      "                'Transformer-%d-FeedForward' % i: [\r\n",
      "                    prefix + 'intermediate/kernel',\r\n",
      "                    prefix + 'intermediate/bias',\r\n",
      "                    prefix + 'output/kernel',\r\n",
      "                    prefix + 'output/bias',\r\n",
      "                ],\r\n",
      "                'Transformer-%d-FeedForward-Norm-1' % i: [\r\n",
      "                    prefix + 'LayerNorm_mlp_ln1/beta',\r\n",
      "                    prefix + 'LayerNorm_mlp_ln1/gamma',\r\n",
      "                ],\r\n",
      "            })\r\n",
      "\r\n",
      "        return mapping\r\n",
      "\r\n",
      "\r\n",
      "class T5_Base(Transformer):\r\n",
      "    \"\"\"Google的T5模型（基类）\r\n",
      "    \"\"\"\r\n",
      "    def load_variable(self, checkpoint, name):\r\n",
      "        \"\"\"加载单个变量的函数\r\n",
      "        \"\"\"\r\n",
      "        variable = super(T5_Base, self).load_variable(checkpoint, name)\r\n",
      "        if name == 'shared/embedding':\r\n",
      "            if self.keep_tokens is None:\r\n",
      "                return variable\r\n",
      "            else:\r\n",
      "                return variable[self.keep_tokens]\r\n",
      "        elif 'relative_attention_bias' in name:\r\n",
      "            return variable.T\r\n",
      "        else:\r\n",
      "            return variable\r\n",
      "\r\n",
      "    def create_variable(self, name, value):\r\n",
      "        \"\"\"在tensorflow中创建一个变量\r\n",
      "        \"\"\"\r\n",
      "        if 'relative_attention_bias' in name:\r\n",
      "            value = value.T\r\n",
      "        return super(T5_Base, self).create_variable(name, value)\r\n",
      "\r\n",
      "    def variable_mapping(self):\r\n",
      "        \"\"\"映射到官方T5权重格式\r\n",
      "        \"\"\"\r\n",
      "        mapping = {\r\n",
      "            'Embedding-Token': ['shared/embedding'],\r\n",
      "            'Encoder-Embedding-Relative-Position': [\r\n",
      "                'encoder/block_000/layer_000/SelfAttention/relative_attention_bias'\r\n",
      "            ],\r\n",
      "            'Encoder-Output-Norm': ['encoder/final_layer_norm/scale'],\r\n",
      "            'Decoder-Embedding-Relative-Position': [\r\n",
      "                'decoder/block_000/layer_000/SelfAttention/relative_attention_bias',\r\n",
      "            ],\r\n",
      "            'Decoder-Output-Norm': ['decoder/final_layer_norm/scale'],\r\n",
      "        }\r\n",
      "\r\n",
      "        for i in range(self.num_hidden_layers):\r\n",
      "            # Encoder主体\r\n",
      "            prefix = 'encoder/block_%03d/' % i\r\n",
      "            mapping.update({\r\n",
      "                'Encoder-Transformer-%d-MultiHeadSelfAttention' % i: [\r\n",
      "                    prefix + 'layer_000/SelfAttention/q',\r\n",
      "                    prefix + 'layer_000/SelfAttention/k',\r\n",
      "                    prefix + 'layer_000/SelfAttention/v',\r\n",
      "                    prefix + 'layer_000/SelfAttention/o',\r\n",
      "                ],\r\n",
      "                'Encoder-Transformer-%d-MultiHeadSelfAttention-Norm' % i: [\r\n",
      "                    prefix + 'layer_000/layer_norm/scale',\r\n",
      "                ],\r\n",
      "                'Encoder-Transformer-%d-FeedForward' % i: [\r\n",
      "                    prefix + 'layer_001/DenseReluDense/wi/kernel',\r\n",
      "                    prefix + 'layer_001/DenseReluDense/wo/kernel',\r\n",
      "                ],\r\n",
      "                'Encoder-Transformer-%d-FeedForward-Norm' % i: [\r\n",
      "                    prefix + 'layer_001/layer_norm/scale',\r\n",
      "                ],\r\n",
      "            })\r\n",
      "            # Decoder主体\r\n",
      "            prefix = 'decoder/block_%03d/' % i\r\n",
      "            mapping.update({\r\n",
      "                'Decoder-Transformer-%d-MultiHeadSelfAttention' % i: [\r\n",
      "                    prefix + 'layer_000/SelfAttention/q',\r\n",
      "                    prefix + 'layer_000/SelfAttention/k',\r\n",
      "                    prefix + 'layer_000/SelfAttention/v',\r\n",
      "                    prefix + 'layer_000/SelfAttention/o',\r\n",
      "                ],\r\n",
      "                'Decoder-Transformer-%d-MultiHeadSelfAttention-Norm' % i: [\r\n",
      "                    prefix + 'layer_000/layer_norm/scale',\r\n",
      "                ],\r\n",
      "                'Decoder-Transformer-%d-MultiHeadCrossAttention' % i: [\r\n",
      "                    prefix + 'layer_001/EncDecAttention/q',\r\n",
      "                    prefix + 'layer_001/EncDecAttention/k',\r\n",
      "                    prefix + 'layer_001/EncDecAttention/v',\r\n",
      "                    prefix + 'layer_001/EncDecAttention/o',\r\n",
      "                ],\r\n",
      "                'Decoder-Transformer-%d-MultiHeadCrossAttention-Norm' % i: [\r\n",
      "                    prefix + 'layer_001/layer_norm/scale',\r\n",
      "                ],\r\n",
      "                'Decoder-Transformer-%d-FeedForward' % i: [\r\n",
      "                    prefix + 'layer_002/DenseReluDense/wi/kernel',\r\n",
      "                    prefix + 'layer_002/DenseReluDense/wo/kernel',\r\n",
      "                ],\r\n",
      "                'Decoder-Transformer-%d-FeedForward-Norm' % i: [\r\n",
      "                    prefix + 'layer_002/layer_norm/scale',\r\n",
      "                ],\r\n",
      "            })\r\n",
      "\r\n",
      "        return mapping\r\n",
      "\r\n",
      "\r\n",
      "class T5_Encoder(T5_Base):\r\n",
      "    \"\"\"Google的T5模型（Encoder）\r\n",
      "    \"\"\"\r\n",
      "    def get_inputs(self):\r\n",
      "        \"\"\"T5的Encoder的输入只有token_ids\r\n",
      "        \"\"\"\r\n",
      "        x_in = Input(shape=(self.sequence_length,), name='Encoder-Input-Token')\r\n",
      "        return x_in\r\n",
      "\r\n",
      "    def apply_embeddings(self, inputs):\r\n",
      "        \"\"\"T5的embedding只有token embedding，\r\n",
      "        并把relative position embedding准备好，待attention使用。\r\n",
      "        \"\"\"\r\n",
      "        x = inputs\r\n",
      "\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Embedding,\r\n",
      "            input_dim=self.vocab_size,\r\n",
      "            output_dim=self.embedding_size,\r\n",
      "            embeddings_initializer=self.initializer,\r\n",
      "            mask_zero=True,\r\n",
      "            name='Embedding-Token'\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Dropout,\r\n",
      "            rate=self.dropout_rate,\r\n",
      "            name='Encoder-Embedding-Dropout'\r\n",
      "        )\r\n",
      "        if self.embedding_size != self.hidden_size:\r\n",
      "            x = self.apply(\r\n",
      "                inputs=x,\r\n",
      "                layer=Dense,\r\n",
      "                units=self.hidden_size,\r\n",
      "                kernel_initializer=self.initializer,\r\n",
      "                name='Encoder-Embedding-Mapping'\r\n",
      "            )\r\n",
      "\r\n",
      "        return x\r\n",
      "\r\n",
      "    def apply_main_layers(self, inputs, index):\r\n",
      "        \"\"\"T5的Encoder的主体是基于Self-Attention的模块\r\n",
      "        顺序：LN --> Att --> Add --> LN --> FFN --> Add\r\n",
      "        \"\"\"\r\n",
      "        x = inputs\r\n",
      "        z = self.layer_norm_conds[0]\r\n",
      "\r\n",
      "        attention_name = 'Encoder-Transformer-%d-MultiHeadSelfAttention' % index\r\n",
      "        feed_forward_name = 'Encoder-Transformer-%d-FeedForward' % index\r\n",
      "        attention_mask = self.compute_attention_mask(index)\r\n",
      "        position_bias = self.compute_position_bias(x)\r\n",
      "\r\n",
      "        # Self Attention\r\n",
      "        xi = x\r\n",
      "        x = self.apply(\r\n",
      "            inputs=self.simplify([x, z]),\r\n",
      "            layer=LayerNormalization,\r\n",
      "            center=False,\r\n",
      "            epsilon=1e-6,\r\n",
      "            conditional=(z is not None),\r\n",
      "            hidden_units=self.layer_norm_conds[1],\r\n",
      "            hidden_activation=self.layer_norm_conds[2],\r\n",
      "            hidden_initializer=self.initializer,\r\n",
      "            name='%s-Norm' % attention_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=[x, x, x, position_bias],\r\n",
      "            layer=MultiHeadAttention,\r\n",
      "            arguments={'p_bias': 't5_relative'},\r\n",
      "            heads=self.num_attention_heads,\r\n",
      "            head_size=self.attention_head_size,\r\n",
      "            key_size=self.attention_key_size,\r\n",
      "            use_bias=False,\r\n",
      "            attention_scale=False,\r\n",
      "            kernel_initializer=self.initializer,\r\n",
      "            name=attention_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Dropout,\r\n",
      "            rate=self.dropout_rate,\r\n",
      "            name='%s-Dropout' % attention_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=[xi, x], layer=Add, name='%s-Add' % attention_name\r\n",
      "        )\r\n",
      "\r\n",
      "        # Feed Forward\r\n",
      "        xi = x\r\n",
      "        x = self.apply(\r\n",
      "            inputs=self.simplify([x, z]),\r\n",
      "            layer=LayerNormalization,\r\n",
      "            center=False,\r\n",
      "            epsilon=1e-6,\r\n",
      "            conditional=(z is not None),\r\n",
      "            hidden_units=self.layer_norm_conds[1],\r\n",
      "            hidden_activation=self.layer_norm_conds[2],\r\n",
      "            hidden_initializer=self.initializer,\r\n",
      "            name='%s-Norm' % feed_forward_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=FeedForward,\r\n",
      "            units=self.intermediate_size,\r\n",
      "            activation=self.hidden_act,\r\n",
      "            use_bias=False,\r\n",
      "            kernel_initializer=self.initializer,\r\n",
      "            name=feed_forward_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Dropout,\r\n",
      "            rate=self.dropout_rate,\r\n",
      "            name='%s-Dropout' % feed_forward_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=[xi, x], layer=Add, name='%s-Add' % feed_forward_name\r\n",
      "        )\r\n",
      "\r\n",
      "        return x\r\n",
      "\r\n",
      "    def apply_final_layers(self, inputs):\r\n",
      "        \"\"\"剩余部分\r\n",
      "        \"\"\"\r\n",
      "        x = inputs\r\n",
      "        z = self.layer_norm_conds[0]\r\n",
      "\r\n",
      "        x = self.apply(\r\n",
      "            inputs=self.simplify([x, z]),\r\n",
      "            layer=LayerNormalization,\r\n",
      "            center=False,\r\n",
      "            epsilon=1e-6,\r\n",
      "            conditional=(z is not None),\r\n",
      "            hidden_units=self.layer_norm_conds[1],\r\n",
      "            hidden_activation=self.layer_norm_conds[2],\r\n",
      "            hidden_initializer=self.initializer,\r\n",
      "            name='Encoder-Output-Norm'\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Dropout,\r\n",
      "            rate=self.dropout_rate,\r\n",
      "            name='Encoder-Output-Dropout'\r\n",
      "        )\r\n",
      "\r\n",
      "        return x\r\n",
      "\r\n",
      "    def compute_position_bias(self, inputs=None):\r\n",
      "        \"\"\"T5相对位置编码\r\n",
      "        \"\"\"\r\n",
      "        if self.position_bias is None:\r\n",
      "\r\n",
      "            x = inputs\r\n",
      "            p = self.apply(\r\n",
      "                inputs=[x, x],\r\n",
      "                layer=RelativePositionEmbeddingT5,\r\n",
      "                input_dim=32,\r\n",
      "                output_dim=self.num_attention_heads,\r\n",
      "                bidirectional=True,\r\n",
      "                embeddings_initializer=self.initializer,\r\n",
      "                name='Encoder-Embedding-Relative-Position'\r\n",
      "            )\r\n",
      "            self.position_bias = p\r\n",
      "\r\n",
      "        return self.position_bias\r\n",
      "\r\n",
      "\r\n",
      "class T5_Decoder(Transformer):\r\n",
      "    \"\"\"Google的T5模型（Decoder）\r\n",
      "    \"\"\"\r\n",
      "    def __init__(self, with_lm=True, **kwargs):\r\n",
      "        super(T5_Decoder, self).__init__(**kwargs)\r\n",
      "        if with_lm is True:\r\n",
      "            self.with_lm = 'softmax'\r\n",
      "        else:\r\n",
      "            self.with_lm = with_lm\r\n",
      "\r\n",
      "    def get_inputs(self):\r\n",
      "        \"\"\"T5的Decoder的输入为context序列和token_ids\r\n",
      "        \"\"\"\r\n",
      "        c_in = Input(\r\n",
      "            shape=(self.sequence_length, self.hidden_size),\r\n",
      "            name='Input-Context'\r\n",
      "        )\r\n",
      "        x_in = Input(shape=(self.sequence_length,), name='Decoder-Input-Token')\r\n",
      "        return [c_in, x_in]\r\n",
      "\r\n",
      "    def apply_embeddings(self, inputs):\r\n",
      "        \"\"\"T5的embedding只有token embedding，\r\n",
      "        并把relative position embedding准备好，待attention使用。\r\n",
      "        \"\"\"\r\n",
      "        c, x = inputs\r\n",
      "\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Embedding,\r\n",
      "            input_dim=self.vocab_size,\r\n",
      "            output_dim=self.embedding_size,\r\n",
      "            embeddings_initializer=self.initializer,\r\n",
      "            mask_zero=True,\r\n",
      "            name='Embedding-Token'\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Dropout,\r\n",
      "            rate=self.dropout_rate,\r\n",
      "            name='Decoder-Embedding-Dropout'\r\n",
      "        )\r\n",
      "        if self.embedding_size != self.hidden_size:\r\n",
      "            x = self.apply(\r\n",
      "                inputs=x,\r\n",
      "                layer=Dense,\r\n",
      "                units=self.hidden_size,\r\n",
      "                kernel_initializer=self.initializer,\r\n",
      "                name='Decoder-Embedding-Mapping'\r\n",
      "            )\r\n",
      "\r\n",
      "        return [c, x]\r\n",
      "\r\n",
      "    def apply_main_layers(self, inputs, index):\r\n",
      "        \"\"\"T5的Dencoder主体是基于Self-Attention、Cross-Attention的模块\r\n",
      "        顺序：LN --> Att1 --> Add --> LN --> Att2 --> Add -->  LN --> FFN --> Add\r\n",
      "        \"\"\"\r\n",
      "        c, x = inputs\r\n",
      "        z = self.layer_norm_conds[0]\r\n",
      "\r\n",
      "        self_attention_name = 'Decoder-Transformer-%d-MultiHeadSelfAttention' % index\r\n",
      "        cross_attention_name = 'Decoder-Transformer-%d-MultiHeadCrossAttention' % index\r\n",
      "        feed_forward_name = 'Decoder-Transformer-%d-FeedForward' % index\r\n",
      "        attention_mask = self.compute_attention_mask(index)\r\n",
      "        position_bias = self.compute_position_bias([x, c])\r\n",
      "\r\n",
      "        # Self Attention\r\n",
      "        xi = x\r\n",
      "        x = self.apply(\r\n",
      "            inputs=self.simplify([x, z]),\r\n",
      "            layer=LayerNormalization,\r\n",
      "            center=False,\r\n",
      "            epsilon=1e-6,\r\n",
      "            conditional=(z is not None),\r\n",
      "            hidden_units=self.layer_norm_conds[1],\r\n",
      "            hidden_activation=self.layer_norm_conds[2],\r\n",
      "            hidden_initializer=self.initializer,\r\n",
      "            name='%s-Norm' % self_attention_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=[x, x, x, attention_mask, position_bias[0]],\r\n",
      "            layer=MultiHeadAttention,\r\n",
      "            arguments={\r\n",
      "                'a_mask': True,\r\n",
      "                'p_bias': 't5_relative'\r\n",
      "            },\r\n",
      "            heads=self.num_attention_heads,\r\n",
      "            head_size=self.attention_head_size,\r\n",
      "            key_size=self.attention_key_size,\r\n",
      "            use_bias=False,\r\n",
      "            attention_scale=False,\r\n",
      "            kernel_initializer=self.initializer,\r\n",
      "            name=self_attention_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Dropout,\r\n",
      "            rate=self.dropout_rate,\r\n",
      "            name='%s-Dropout' % self_attention_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=[xi, x], layer=Add, name='%s-Add' % self_attention_name\r\n",
      "        )\r\n",
      "\r\n",
      "        # Cross Attention\r\n",
      "        xi = x\r\n",
      "        x = self.apply(\r\n",
      "            inputs=self.simplify([x, z]),\r\n",
      "            layer=LayerNormalization,\r\n",
      "            center=False,\r\n",
      "            epsilon=1e-6,\r\n",
      "            conditional=(z is not None),\r\n",
      "            hidden_units=self.layer_norm_conds[1],\r\n",
      "            hidden_activation=self.layer_norm_conds[2],\r\n",
      "            hidden_initializer=self.initializer,\r\n",
      "            name='%s-Norm' % cross_attention_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=[x, c, c, position_bias[1]],\r\n",
      "            layer=MultiHeadAttention,\r\n",
      "            arguments={\r\n",
      "                'a_mask': None,\r\n",
      "                'p_bias': 't5_relative'\r\n",
      "            },\r\n",
      "            heads=self.num_attention_heads,\r\n",
      "            head_size=self.attention_head_size,\r\n",
      "            key_size=self.attention_key_size,\r\n",
      "            use_bias=False,\r\n",
      "            attention_scale=False,\r\n",
      "            kernel_initializer=self.initializer,\r\n",
      "            name=cross_attention_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Dropout,\r\n",
      "            rate=self.dropout_rate,\r\n",
      "            name='%s-Dropout' % cross_attention_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=[xi, x], layer=Add, name='%s-Add' % cross_attention_name\r\n",
      "        )\r\n",
      "\r\n",
      "        # Feed Forward\r\n",
      "        xi = x\r\n",
      "        x = self.apply(\r\n",
      "            inputs=self.simplify([x, z]),\r\n",
      "            layer=LayerNormalization,\r\n",
      "            center=False,\r\n",
      "            epsilon=1e-6,\r\n",
      "            conditional=(z is not None),\r\n",
      "            hidden_units=self.layer_norm_conds[1],\r\n",
      "            hidden_activation=self.layer_norm_conds[2],\r\n",
      "            hidden_initializer=self.initializer,\r\n",
      "            name='%s-Norm' % feed_forward_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=FeedForward,\r\n",
      "            units=self.intermediate_size,\r\n",
      "            activation=self.hidden_act,\r\n",
      "            use_bias=False,\r\n",
      "            kernel_initializer=self.initializer,\r\n",
      "            name=feed_forward_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Dropout,\r\n",
      "            rate=self.dropout_rate,\r\n",
      "            name='%s-Dropout' % feed_forward_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=[xi, x], layer=Add, name='%s-Add' % feed_forward_name\r\n",
      "        )\r\n",
      "\r\n",
      "        return [c, x]\r\n",
      "\r\n",
      "    def apply_final_layers(self, inputs):\r\n",
      "        \"\"\"剩余部分\r\n",
      "        \"\"\"\r\n",
      "        c, x = inputs\r\n",
      "        z = self.layer_norm_conds[0]\r\n",
      "\r\n",
      "        x = self.apply(\r\n",
      "            inputs=self.simplify([x, z]),\r\n",
      "            layer=LayerNormalization,\r\n",
      "            center=False,\r\n",
      "            epsilon=1e-6,\r\n",
      "            conditional=(z is not None),\r\n",
      "            hidden_units=self.layer_norm_conds[1],\r\n",
      "            hidden_activation=self.layer_norm_conds[2],\r\n",
      "            hidden_initializer=self.initializer,\r\n",
      "            name='Decoder-Output-Norm'\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Dropout,\r\n",
      "            rate=self.dropout_rate,\r\n",
      "            name='Decoder-Output-Dropout'\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Lambda,\r\n",
      "            function=lambda x: x / np.sqrt(self.hidden_size),\r\n",
      "            name='Decoder-Output-Scale'\r\n",
      "        )\r\n",
      "\r\n",
      "        if self.with_lm:\r\n",
      "            # 预测token概率部分\r\n",
      "            if self.embedding_size != self.hidden_size:\r\n",
      "                x = self.apply(\r\n",
      "                    inputs=x,\r\n",
      "                    layer=Dense,\r\n",
      "                    units=self.embedding_size,\r\n",
      "                    kernel_initializer=self.initializer,\r\n",
      "                    name='Decoder-Output-Mapping'\r\n",
      "                )\r\n",
      "            x = self.apply(\r\n",
      "                inputs=x,\r\n",
      "                layer=Embedding,\r\n",
      "                arguments={'mode': 'dense'},\r\n",
      "                name='Embedding-Token'\r\n",
      "            )\r\n",
      "            lm_activation = 'softmax' if self.with_lm is True else self.with_lm\r\n",
      "            x = self.apply(\r\n",
      "                inputs=x,\r\n",
      "                layer=Activation,\r\n",
      "                activation=lm_activation,\r\n",
      "                name='Dencoder-Output-LM-Activation'\r\n",
      "            )\r\n",
      "\r\n",
      "        return x\r\n",
      "\r\n",
      "    def compute_attention_mask(self, inputs=None):\r\n",
      "        \"\"\"添加下三角形式的attention mask\r\n",
      "        \"\"\"\r\n",
      "        if self.attention_mask is None:\r\n",
      "\r\n",
      "            def lm_mask(s):\r\n",
      "                seq_len = K.shape(s)[1]\r\n",
      "                idxs = K.arange(0, seq_len)\r\n",
      "                mask = idxs[None, :] <= idxs[:, None]\r\n",
      "                mask = K.cast(mask, K.floatx())\r\n",
      "                return mask[None, None]\r\n",
      "\r\n",
      "            self.attention_mask = self.apply(\r\n",
      "                inputs=self.inputs[1],\r\n",
      "                layer=Lambda,\r\n",
      "                function=lm_mask,\r\n",
      "                name='Attention-LM-Mask'\r\n",
      "            )\r\n",
      "\r\n",
      "        return self.attention_mask\r\n",
      "\r\n",
      "    def compute_position_bias(self, inputs=None):\r\n",
      "        \"\"\"T5相对位置编码\r\n",
      "        \"\"\"\r\n",
      "        if self.position_bias is None:\r\n",
      "\r\n",
      "            x, c = inputs\r\n",
      "            p1 = self.apply(\r\n",
      "                inputs=[x, x],\r\n",
      "                layer=RelativePositionEmbeddingT5,\r\n",
      "                input_dim=32,\r\n",
      "                output_dim=self.num_attention_heads,\r\n",
      "                bidirectional=False,\r\n",
      "                embeddings_initializer=self.initializer,\r\n",
      "                name='Decoder-Embedding-Relative-Position'\r\n",
      "            )\r\n",
      "            p2 = self.apply(\r\n",
      "                inputs=[x, c],\r\n",
      "                layer=RelativePositionEmbeddingT5,\r\n",
      "                input_dim=32,\r\n",
      "                output_dim=self.num_attention_heads,\r\n",
      "                bidirectional=False,\r\n",
      "                embeddings_initializer=self.initializer,\r\n",
      "                name='Decoder-Embedding-Relative-Position'\r\n",
      "            )\r\n",
      "            self.position_bias = (p1, p2)\r\n",
      "\r\n",
      "        return self.position_bias\r\n",
      "\r\n",
      "\r\n",
      "class T5(T5_Base):\r\n",
      "    \"\"\"Google的T5模型（Encoder-Decoder）\r\n",
      "    \"\"\"\r\n",
      "    def __init__(self, **kwargs):\r\n",
      "        super(T5, self).__init__(**kwargs)\r\n",
      "        kwargs['layers'] = self.layers\r\n",
      "        e_name, d_name = 'Encoder', 'Decoder'\r\n",
      "        if 'name' in kwargs:\r\n",
      "            e_name = '%s_%s' % (kwargs['name'], e_name)\r\n",
      "            d_name = '%s_%s' % (kwargs['name'], d_name)\r\n",
      "            del kwargs['name']  # 防止重复传参\r\n",
      "        self._encoder = T5_Encoder(name=e_name, **kwargs)\r\n",
      "        self._decoder = T5_Decoder(name=d_name, **kwargs)\r\n",
      "\r\n",
      "    def build(self, **kwargs):\r\n",
      "        \"\"\"同时构建Encoder和Decoder\r\n",
      "        \"\"\"\r\n",
      "        self._encoder.build(**kwargs)\r\n",
      "        self._decoder.build(**kwargs)\r\n",
      "        self.encoder = self._encoder.model\r\n",
      "        self.decoder = self._decoder.model\r\n",
      "        self.inputs = self.encoder.inputs + self.decoder.inputs[1:]\r\n",
      "        self.outputs = self.decoder(\r\n",
      "            self.encoder.outputs + self.decoder.inputs[1:]\r\n",
      "        )\r\n",
      "        self.model = Model(self.inputs, self.outputs)\r\n",
      "\r\n",
      "\r\n",
      "def extend_with_language_model(BaseModel):\r\n",
      "    \"\"\"添加下三角的Attention Mask（语言模型用）\r\n",
      "    \"\"\"\r\n",
      "    class LanguageModel(BaseModel):\r\n",
      "        \"\"\"带下三角Attention Mask的派生模型\r\n",
      "        \"\"\"\r\n",
      "        def __init__(self, *args, **kwargs):\r\n",
      "            super(LanguageModel, self).__init__(*args, **kwargs)\r\n",
      "            self.with_mlm = self.with_mlm or True\r\n",
      "\r\n",
      "        def compute_attention_mask(self, inputs=None):\r\n",
      "            \"\"\"重载此函数即可\r\n",
      "            \"\"\"\r\n",
      "            if self.attention_mask is None:\r\n",
      "\r\n",
      "                def lm_mask(s):\r\n",
      "                    seq_len = K.shape(s)[1]\r\n",
      "                    idxs = K.arange(0, seq_len)\r\n",
      "                    mask = idxs[None, :] <= idxs[:, None]\r\n",
      "                    mask = K.cast(mask, K.floatx())\r\n",
      "                    return mask[None, None]\r\n",
      "\r\n",
      "                self.attention_mask = self.apply(\r\n",
      "                    inputs=self.inputs[1],\r\n",
      "                    layer=Lambda,\r\n",
      "                    function=lm_mask,\r\n",
      "                    name='Attention-LM-Mask'\r\n",
      "                )\r\n",
      "\r\n",
      "            return self.attention_mask\r\n",
      "\r\n",
      "    return LanguageModel\r\n",
      "\r\n",
      "\r\n",
      "def extend_with_unified_language_model(BaseModel):\r\n",
      "    \"\"\"添加UniLM的Attention Mask（UnifiedLanguageModel用）\r\n",
      "    \"\"\"\r\n",
      "    class UnifiedLanguageModel(BaseModel):\r\n",
      "        \"\"\"带UniLM的Attention Mask的派生模型\r\n",
      "        UniLM: https://arxiv.org/abs/1905.03197\r\n",
      "        \"\"\"\r\n",
      "        def __init__(self, *args, **kwargs):\r\n",
      "            super(UnifiedLanguageModel, self).__init__(*args, **kwargs)\r\n",
      "            self.with_mlm = self.with_mlm or True\r\n",
      "\r\n",
      "        def compute_attention_mask(self, inputs=None):\r\n",
      "            \"\"\"重载此函数即可\r\n",
      "            \"\"\"\r\n",
      "            if self.attention_mask is None:\r\n",
      "\r\n",
      "                def unilm_mask(s):\r\n",
      "                    idxs = K.cumsum(s, axis=1)\r\n",
      "                    mask = idxs[:, None, :] <= idxs[:, :, None]\r\n",
      "                    mask = K.cast(mask, K.floatx())\r\n",
      "                    return mask[:, None]\r\n",
      "\r\n",
      "                self.attention_mask = self.apply(\r\n",
      "                    inputs=self.inputs[1],\r\n",
      "                    layer=Lambda,\r\n",
      "                    function=unilm_mask,\r\n",
      "                    name='Attention-UniLM-Mask'\r\n",
      "                )\r\n",
      "\r\n",
      "            return self.attention_mask\r\n",
      "\r\n",
      "    return UnifiedLanguageModel\r\n",
      "\r\n",
      "\r\n",
      "def build_transformer_model(\r\n",
      "    config_path=None,\r\n",
      "    checkpoint_path=None,\r\n",
      "    model='bert',\r\n",
      "    application='encoder',\r\n",
      "    return_keras_model=True,\r\n",
      "    **kwargs\r\n",
      "):\r\n",
      "    \"\"\"根据配置文件构建模型，可选加载checkpoint权重\r\n",
      "    \"\"\"\r\n",
      "    configs = {}\r\n",
      "    if config_path is not None:\r\n",
      "        configs.update(json.load(open(config_path)))\r\n",
      "    configs.update(kwargs)\r\n",
      "    if 'max_position' not in configs:\r\n",
      "        configs['max_position'] = configs.get('max_position_embeddings')\r\n",
      "    if 'dropout_rate' not in configs:\r\n",
      "        configs['dropout_rate'] = configs.get('hidden_dropout_prob')\r\n",
      "\r\n",
      "    model, application = model.lower(), application.lower()\r\n",
      "\r\n",
      "    models = {\r\n",
      "        'bert': BERT,\r\n",
      "        'albert': ALBERT,\r\n",
      "        'albert_unshared': ALBERT_Unshared,\r\n",
      "        'nezha': NEZHA,\r\n",
      "        'electra': ELECTRA,\r\n",
      "        'gpt2_ml': GPT2_ML,\r\n",
      "        't5': T5,\r\n",
      "    }\r\n",
      "    MODEL = models[model]\r\n",
      "\r\n",
      "    if model != 't5':\r\n",
      "        if application == 'lm':\r\n",
      "            MODEL = extend_with_language_model(MODEL)\r\n",
      "        elif application == 'unilm':\r\n",
      "            MODEL = extend_with_unified_language_model(MODEL)\r\n",
      "\r\n",
      "    transformer = MODEL(**configs)\r\n",
      "    transformer.build(**configs)\r\n",
      "\r\n",
      "    if checkpoint_path is not None:\r\n",
      "        transformer.load_weights_from_checkpoint(checkpoint_path)\r\n",
      "\r\n",
      "    if return_keras_model:\r\n",
      "        return transformer.model\r\n",
      "    else:\r\n",
      "        return transformer\r\n"
     ]
    }
   ],
   "source": [
    "!cat ~/miniconda3/envs/pycookly2/lib/python3.6/site-packages/bert4keras/models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#! -*- coding: utf-8 -*-\r\n",
      "# 主要模型\r\n",
      "\r\n",
      "import numpy as np\r\n",
      "from bert4keras.layers import *\r\n",
      "from bert4keras.snippets import delete_arguments\r\n",
      "from keras.models import Model\r\n",
      "import json\r\n",
      "\r\n",
      "\r\n",
      "class Transformer(object):\r\n",
      "    \"\"\"模型基类\r\n",
      "    \"\"\"\r\n",
      "    def __init__(\r\n",
      "        self,\r\n",
      "        vocab_size,  # 词表大小\r\n",
      "        hidden_size,  # 编码维度\r\n",
      "        num_hidden_layers,  # Transformer总层数\r\n",
      "        num_attention_heads,  # Attention的头数\r\n",
      "        intermediate_size,  # FeedForward的隐层维度\r\n",
      "        hidden_act,  # FeedForward隐层的激活函数\r\n",
      "        dropout_rate=None,  # Dropout比例\r\n",
      "        embedding_size=None,  # 是否指定embedding_size\r\n",
      "        attention_key_size=None,  # Attention中Q,K的head_size\r\n",
      "        sequence_length=None,  # 是否固定序列长度\r\n",
      "        keep_tokens=None,  # 要保留的词ID列表\r\n",
      "        layers=None,  # 外部传入的Keras层\r\n",
      "        name=None,  # 模型名称\r\n",
      "        **kwargs\r\n",
      "    ):\r\n",
      "        if keep_tokens is None:\r\n",
      "            self.vocab_size = vocab_size\r\n",
      "        else:\r\n",
      "            self.vocab_size = len(keep_tokens)\r\n",
      "        self.hidden_size = hidden_size\r\n",
      "        self.num_hidden_layers = num_hidden_layers\r\n",
      "        self.num_attention_heads = num_attention_heads\r\n",
      "        self.attention_head_size = hidden_size // num_attention_heads\r\n",
      "        self.attention_key_size = attention_key_size or self.attention_head_size\r\n",
      "        self.intermediate_size = intermediate_size\r\n",
      "        self.dropout_rate = dropout_rate or 0\r\n",
      "        self.hidden_act = hidden_act\r\n",
      "        self.embedding_size = embedding_size or hidden_size\r\n",
      "        self.sequence_length = sequence_length\r\n",
      "        self.keep_tokens = keep_tokens\r\n",
      "        self.attention_mask = None\r\n",
      "        self.position_bias = None\r\n",
      "        self.layers = {} if layers is None else layers\r\n",
      "        self.name = name\r\n",
      "        self.built = False\r\n",
      "\r\n",
      "    def build(\r\n",
      "        self,\r\n",
      "        layer_norm_cond=None,\r\n",
      "        layer_norm_cond_hidden_size=None,\r\n",
      "        layer_norm_cond_hidden_act=None,\r\n",
      "        additional_input_layers=None,\r\n",
      "        **kwargs\r\n",
      "    ):\r\n",
      "        \"\"\"模型构建函数\r\n",
      "        layer_norm_*系列参数为实现Conditional Layer Normalization时使用，\r\n",
      "        用来实现以“固定长度向量”为条件的条件Bert。\r\n",
      "        \"\"\"\r\n",
      "        if self.built:\r\n",
      "            return None\r\n",
      "        # Input\r\n",
      "        inputs = self.get_inputs()\r\n",
      "        self.set_inputs(inputs, additional_input_layers)\r\n",
      "        # Other\r\n",
      "        self.layer_norm_conds = [\r\n",
      "            layer_norm_cond,\r\n",
      "            layer_norm_cond_hidden_size,\r\n",
      "            layer_norm_cond_hidden_act or 'linear',\r\n",
      "        ]\r\n",
      "        # Call\r\n",
      "        outputs = self.call(inputs)\r\n",
      "        self.set_outputs(outputs)\r\n",
      "        # Model\r\n",
      "        self.model = Model(self.inputs, self.outputs, name=self.name)\r\n",
      "        self.built = True\r\n",
      "\r\n",
      "    def call(self, inputs):\r\n",
      "        \"\"\"定义模型的执行流程\r\n",
      "        \"\"\"\r\n",
      "        # Embedding\r\n",
      "        outputs = self.apply_embeddings(inputs)\r\n",
      "        # Main\r\n",
      "        for i in range(self.num_hidden_layers):\r\n",
      "            outputs = self.apply_main_layers(outputs, i)\r\n",
      "        # Final\r\n",
      "        outputs = self.apply_final_layers(outputs)\r\n",
      "        return outputs\r\n",
      "\r\n",
      "    def apply(self, inputs, layer=None, arguments=None, **kwargs):\r\n",
      "        \"\"\"通过apply调用层会自动重用同名层\r\n",
      "        inputs: 上一层的输出；\r\n",
      "        layer: 要调用的层类名；\r\n",
      "        arguments: 传递给layer.call的参数；\r\n",
      "        kwargs: 传递给层初始化的参数。\r\n",
      "        \"\"\"\r\n",
      "        if layer is Dropout and self.dropout_rate == 0:\r\n",
      "            return inputs\r\n",
      "\r\n",
      "        arguments = arguments or {}\r\n",
      "        name = kwargs.get('name')\r\n",
      "        if name not in self.layers:\r\n",
      "            layer = layer(**kwargs)\r\n",
      "            name = layer.name\r\n",
      "            self.layers[name] = layer\r\n",
      "\r\n",
      "        return self.layers[name](inputs, **arguments)\r\n",
      "\r\n",
      "    def get_inputs(self):\r\n",
      "        raise NotImplementedError\r\n",
      "\r\n",
      "    def apply_embeddings(self, inputs):\r\n",
      "        raise NotImplementedError\r\n",
      "\r\n",
      "    def apply_main_layers(self, inputs, index):\r\n",
      "        raise NotImplementedError\r\n",
      "\r\n",
      "    def apply_final_layers(self, inputs):\r\n",
      "        raise NotImplementedError\r\n",
      "\r\n",
      "    def compute_attention_mask(self, inputs=None):\r\n",
      "        \"\"\"定义每一层的Attention Mask\r\n",
      "        \"\"\"\r\n",
      "        return self.attention_mask\r\n",
      "\r\n",
      "    def compute_position_bias(self, inputs=None):\r\n",
      "        \"\"\"定义每一层的Position Bias（一般相对位置编码用）\r\n",
      "        \"\"\"\r\n",
      "        return self.position_bias\r\n",
      "\r\n",
      "    def set_inputs(self, inputs, additional_input_layers=None):\r\n",
      "        \"\"\"设置input和inputs属性\r\n",
      "        \"\"\"\r\n",
      "        if inputs is None:\r\n",
      "            inputs = []\r\n",
      "        elif not isinstance(inputs, list):\r\n",
      "            inputs = [inputs]\r\n",
      "\r\n",
      "        inputs = inputs[:]\r\n",
      "        if additional_input_layers is not None:\r\n",
      "            if not isinstance(additional_input_layers, list):\r\n",
      "                additional_input_layers = [additional_input_layers]\r\n",
      "            inputs.extend(additional_input_layers)\r\n",
      "\r\n",
      "        self.inputs = inputs\r\n",
      "        if len(inputs) > 1:\r\n",
      "            self.input = inputs\r\n",
      "        else:\r\n",
      "            self.input = inputs[0]\r\n",
      "\r\n",
      "    def set_outputs(self, outputs):\r\n",
      "        \"\"\"设置output和oututs属性\r\n",
      "        \"\"\"\r\n",
      "        if not isinstance(outputs, list):\r\n",
      "            outputs = [outputs]\r\n",
      "\r\n",
      "        outputs = outputs[:]\r\n",
      "        self.outputs = outputs\r\n",
      "        if len(outputs) > 1:\r\n",
      "            self.output = outputs\r\n",
      "        else:\r\n",
      "            self.output = outputs[0]\r\n",
      "\r\n",
      "    @property\r\n",
      "    def initializer(self):\r\n",
      "        \"\"\"默认使用截断正态分布初始化\r\n",
      "        \"\"\"\r\n",
      "        return keras.initializers.TruncatedNormal(stddev=0.02)\r\n",
      "\r\n",
      "    def simplify(self, inputs):\r\n",
      "        \"\"\"将list中的None过滤掉\r\n",
      "        \"\"\"\r\n",
      "        inputs = [i for i in inputs if i is not None]\r\n",
      "        if len(inputs) == 1:\r\n",
      "            inputs = inputs[0]\r\n",
      "\r\n",
      "        return inputs\r\n",
      "\r\n",
      "    def load_variable(self, checkpoint, name):\r\n",
      "        \"\"\"加载单个变量的函数\r\n",
      "        \"\"\"\r\n",
      "        return tf.train.load_variable(checkpoint, name)\r\n",
      "\r\n",
      "    def create_variable(self, name, value):\r\n",
      "        \"\"\"在tensorflow中创建一个变量\r\n",
      "        \"\"\"\r\n",
      "        return tf.Variable(value, name=name)\r\n",
      "\r\n",
      "    def variable_mapping(self):\r\n",
      "        \"\"\"构建keras层与checkpoint的变量名之间的映射表\r\n",
      "        \"\"\"\r\n",
      "        return {}\r\n",
      "\r\n",
      "    def load_weights_from_checkpoint(self, checkpoint, mapping=None):\r\n",
      "        \"\"\"根据mapping从checkpoint加载权重\r\n",
      "        \"\"\"\r\n",
      "        mapping = mapping or self.variable_mapping()\r\n",
      "        mapping = {k: v for k, v in mapping.items() if k in self.layers}\r\n",
      "\r\n",
      "        weight_value_pairs = []\r\n",
      "        for layer, variables in mapping.items():\r\n",
      "            layer = self.layers[layer]\r\n",
      "            weights = layer.trainable_weights\r\n",
      "            values = [self.load_variable(checkpoint, v) for v in variables]\r\n",
      "\r\n",
      "            if isinstance(layer, MultiHeadAttention):\r\n",
      "                \"\"\"如果key_size不等于head_size，则可以通过\r\n",
      "                正交矩阵将相应的权重投影到合适的shape。\r\n",
      "                \"\"\"\r\n",
      "                count = 2\r\n",
      "                if layer.use_bias:\r\n",
      "                    count += 2\r\n",
      "                heads = self.num_attention_heads\r\n",
      "                head_size = self.attention_head_size\r\n",
      "                key_size = self.attention_key_size\r\n",
      "                W = np.linalg.qr(np.random.randn(key_size, head_size))[0].T\r\n",
      "                if layer.attention_scale:\r\n",
      "                    W = W * key_size**0.25 / head_size**0.25\r\n",
      "                for i in range(count):\r\n",
      "                    w, v = weights[i], values[i]\r\n",
      "                    w_shape, v_shape = K.int_shape(w), v.shape\r\n",
      "                    if w_shape[-1] != v_shape[-1]:\r\n",
      "                        pre_shape = w_shape[:-1]\r\n",
      "                        v = v.reshape(pre_shape + (heads, head_size))\r\n",
      "                        v = np.dot(v, W)\r\n",
      "                        v = v.reshape(pre_shape + (heads * key_size,))\r\n",
      "                        values[i] = v\r\n",
      "\r\n",
      "            weight_value_pairs.extend(zip(weights, values))\r\n",
      "\r\n",
      "        K.batch_set_value(weight_value_pairs)\r\n",
      "\r\n",
      "    def save_weights_as_checkpoint(self, filename, mapping=None):\r\n",
      "        \"\"\"根据mapping将权重保存为checkpoint格式\r\n",
      "        \"\"\"\r\n",
      "        mapping = mapping or self.variable_mapping()\r\n",
      "        mapping = {k: v for k, v in mapping.items() if k in self.layers}\r\n",
      "\r\n",
      "        with tf.Graph().as_default():\r\n",
      "            for layer, variables in mapping.items():\r\n",
      "                layer = self.layers[layer]\r\n",
      "                values = K.batch_get_value(layer.trainable_weights)\r\n",
      "                for name, value in zip(variables, values):\r\n",
      "                    self.create_variable(name, value)\r\n",
      "            with tf.Session() as sess:\r\n",
      "                sess.run(tf.global_variables_initializer())\r\n",
      "                saver = tf.train.Saver()\r\n",
      "                saver.save(sess, filename, write_meta_graph=False)\r\n",
      "\r\n",
      "\r\n",
      "class BERT(Transformer):\r\n",
      "    \"\"\"构建BERT模型\r\n",
      "    \"\"\"\r\n",
      "    def __init__(\r\n",
      "        self,\r\n",
      "        max_position,  # 序列最大长度\r\n",
      "        with_pool=False,  # 是否包含Pool部分\r\n",
      "        with_nsp=False,  # 是否包含NSP部分\r\n",
      "        with_mlm=False,  # 是否包含MLM部分\r\n",
      "        custom_position_ids=False,  # 是否自行传入位置id\r\n",
      "        **kwargs  # 其余参数\r\n",
      "    ):\r\n",
      "        super(BERT, self).__init__(**kwargs)\r\n",
      "        self.max_position = max_position\r\n",
      "        self.with_pool = with_pool\r\n",
      "        self.with_nsp = with_nsp\r\n",
      "        self.with_mlm = with_mlm\r\n",
      "        self.custom_position_ids = custom_position_ids\r\n",
      "\r\n",
      "    def get_inputs(self):\r\n",
      "        \"\"\"BERT的输入是token_ids和segment_ids\r\n",
      "        （但允许自行传入位置id，以实现一些特殊需求）\r\n",
      "        \"\"\"\r\n",
      "        x_in = Input(shape=(self.sequence_length,), name='Input-Token')\r\n",
      "        s_in = Input(shape=(self.sequence_length,), name='Input-Segment')\r\n",
      "\r\n",
      "        if self.custom_position_ids:\r\n",
      "            p_in = Input(shape=(self.sequence_length,), name='Input-Position')\r\n",
      "            return [x_in, s_in, p_in]\r\n",
      "        else:\r\n",
      "            return [x_in, s_in]\r\n",
      "\r\n",
      "    def apply_embeddings(self, inputs):\r\n",
      "        \"\"\"BERT的embedding是token、position、segment三者embedding之和\r\n",
      "        \"\"\"\r\n",
      "        x, s = inputs[:2]\r\n",
      "        z = self.layer_norm_conds[0]\r\n",
      "        if self.custom_position_ids:\r\n",
      "            p = inputs[2]\r\n",
      "        else:\r\n",
      "            p = None\r\n",
      "\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Embedding,\r\n",
      "            input_dim=self.vocab_size,\r\n",
      "            output_dim=self.embedding_size,\r\n",
      "            embeddings_initializer=self.initializer,\r\n",
      "            mask_zero=True,\r\n",
      "            name='Embedding-Token'\r\n",
      "        )\r\n",
      "        s = self.apply(\r\n",
      "            inputs=s,\r\n",
      "            layer=Embedding,\r\n",
      "            input_dim=2,\r\n",
      "            output_dim=self.embedding_size,\r\n",
      "            embeddings_initializer=self.initializer,\r\n",
      "            name='Embedding-Segment'\r\n",
      "        )\r\n",
      "        x = self.apply(inputs=[x, s], layer=Add, name='Embedding-Token-Segment')\r\n",
      "        x = self.apply(\r\n",
      "            inputs=self.simplify([x, p]),\r\n",
      "            layer=PositionEmbedding,\r\n",
      "            input_dim=self.max_position,\r\n",
      "            output_dim=self.embedding_size,\r\n",
      "            merge_mode='add',\r\n",
      "            embeddings_initializer=self.initializer,\r\n",
      "            custom_position_ids=self.custom_position_ids,\r\n",
      "            name='Embedding-Position'\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=self.simplify([x, z]),\r\n",
      "            layer=LayerNormalization,\r\n",
      "            conditional=(z is not None),\r\n",
      "            hidden_units=self.layer_norm_conds[1],\r\n",
      "            hidden_activation=self.layer_norm_conds[2],\r\n",
      "            hidden_initializer=self.initializer,\r\n",
      "            name='Embedding-Norm'\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Dropout,\r\n",
      "            rate=self.dropout_rate,\r\n",
      "            name='Embedding-Dropout'\r\n",
      "        )\r\n",
      "        if self.embedding_size != self.hidden_size:\r\n",
      "            x = self.apply(\r\n",
      "                inputs=x,\r\n",
      "                layer=Dense,\r\n",
      "                units=self.hidden_size,\r\n",
      "                kernel_initializer=self.initializer,\r\n",
      "                name='Embedding-Mapping'\r\n",
      "            )\r\n",
      "\r\n",
      "        return x\r\n",
      "\r\n",
      "    def apply_main_layers(self, inputs, index):\r\n",
      "        \"\"\"BERT的主体是基于Self-Attention的模块\r\n",
      "        顺序：Att --> Add --> LN --> FFN --> Add --> LN\r\n",
      "        \"\"\"\r\n",
      "        x = inputs\r\n",
      "        z = self.layer_norm_conds[0]\r\n",
      "\r\n",
      "        attention_name = 'Transformer-%d-MultiHeadSelfAttention' % index\r\n",
      "        feed_forward_name = 'Transformer-%d-FeedForward' % index\r\n",
      "        attention_mask = self.compute_attention_mask(index)\r\n",
      "\r\n",
      "        # Self Attention\r\n",
      "        xi, x, arguments = x, [x, x, x], {'a_mask': None}\r\n",
      "        if attention_mask is not None:\r\n",
      "            arguments['a_mask'] = True\r\n",
      "            x.append(attention_mask)\r\n",
      "\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=MultiHeadAttention,\r\n",
      "            arguments=arguments,\r\n",
      "            heads=self.num_attention_heads,\r\n",
      "            head_size=self.attention_head_size,\r\n",
      "            key_size=self.attention_key_size,\r\n",
      "            kernel_initializer=self.initializer,\r\n",
      "            name=attention_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Dropout,\r\n",
      "            rate=self.dropout_rate,\r\n",
      "            name='%s-Dropout' % attention_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=[xi, x], layer=Add, name='%s-Add' % attention_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=self.simplify([x, z]),\r\n",
      "            layer=LayerNormalization,\r\n",
      "            conditional=(z is not None),\r\n",
      "            hidden_units=self.layer_norm_conds[1],\r\n",
      "            hidden_activation=self.layer_norm_conds[2],\r\n",
      "            hidden_initializer=self.initializer,\r\n",
      "            name='%s-Norm' % attention_name\r\n",
      "        )\r\n",
      "\r\n",
      "        # Feed Forward\r\n",
      "        xi = x\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=FeedForward,\r\n",
      "            units=self.intermediate_size,\r\n",
      "            activation=self.hidden_act,\r\n",
      "            kernel_initializer=self.initializer,\r\n",
      "            name=feed_forward_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Dropout,\r\n",
      "            rate=self.dropout_rate,\r\n",
      "            name='%s-Dropout' % feed_forward_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=[xi, x], layer=Add, name='%s-Add' % feed_forward_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=self.simplify([x, z]),\r\n",
      "            layer=LayerNormalization,\r\n",
      "            conditional=(z is not None),\r\n",
      "            hidden_units=self.layer_norm_conds[1],\r\n",
      "            hidden_activation=self.layer_norm_conds[2],\r\n",
      "            hidden_initializer=self.initializer,\r\n",
      "            name='%s-Norm' % feed_forward_name\r\n",
      "        )\r\n",
      "\r\n",
      "        return x\r\n",
      "\r\n",
      "    def apply_final_layers(self, inputs):\r\n",
      "        \"\"\"根据剩余参数决定输出\r\n",
      "        \"\"\"\r\n",
      "        x = inputs\r\n",
      "        z = self.layer_norm_conds[0]\r\n",
      "        outputs = [x]\r\n",
      "\r\n",
      "        if self.with_pool or self.with_nsp:\r\n",
      "            # Pooler部分（提取CLS向量）\r\n",
      "            x = outputs[0]\r\n",
      "            x = self.apply(\r\n",
      "                inputs=x,\r\n",
      "                layer=Lambda,\r\n",
      "                function=lambda x: x[:, 0],\r\n",
      "                name='Pooler'\r\n",
      "            )\r\n",
      "            pool_activation = 'tanh' if self.with_pool is True else self.with_pool\r\n",
      "            x = self.apply(\r\n",
      "                inputs=x,\r\n",
      "                layer=Dense,\r\n",
      "                units=self.hidden_size,\r\n",
      "                activation=pool_activation,\r\n",
      "                kernel_initializer=self.initializer,\r\n",
      "                name='Pooler-Dense'\r\n",
      "            )\r\n",
      "            if self.with_nsp:\r\n",
      "                # Next Sentence Prediction部分\r\n",
      "                x = self.apply(\r\n",
      "                    inputs=x,\r\n",
      "                    layer=Dense,\r\n",
      "                    units=2,\r\n",
      "                    activation='softmax',\r\n",
      "                    kernel_initializer=self.initializer,\r\n",
      "                    name='NSP-Proba'\r\n",
      "                )\r\n",
      "            outputs.append(x)\r\n",
      "\r\n",
      "        if self.with_mlm:\r\n",
      "            # Masked Language Model部分\r\n",
      "            x = outputs[0]\r\n",
      "            x = self.apply(\r\n",
      "                inputs=x,\r\n",
      "                layer=Dense,\r\n",
      "                units=self.embedding_size,\r\n",
      "                activation=self.hidden_act,\r\n",
      "                kernel_initializer=self.initializer,\r\n",
      "                name='MLM-Dense'\r\n",
      "            )\r\n",
      "            x = self.apply(\r\n",
      "                inputs=self.simplify([x, z]),\r\n",
      "                layer=LayerNormalization,\r\n",
      "                conditional=(z is not None),\r\n",
      "                hidden_units=self.layer_norm_conds[1],\r\n",
      "                hidden_activation=self.layer_norm_conds[2],\r\n",
      "                hidden_initializer=self.initializer,\r\n",
      "                name='MLM-Norm'\r\n",
      "            )\r\n",
      "            x = self.apply(\r\n",
      "                inputs=x,\r\n",
      "                layer=Embedding,\r\n",
      "                arguments={'mode': 'dense'},\r\n",
      "                name='Embedding-Token'\r\n",
      "            )\r\n",
      "            x = self.apply(inputs=x, layer=BiasAdd, name='MLM-Bias')\r\n",
      "            mlm_activation = 'softmax' if self.with_mlm is True else self.with_mlm\r\n",
      "            x = self.apply(\r\n",
      "                inputs=x,\r\n",
      "                layer=Activation,\r\n",
      "                activation=mlm_activation,\r\n",
      "                name='MLM-Activation'\r\n",
      "            )\r\n",
      "            outputs.append(x)\r\n",
      "\r\n",
      "        if len(outputs) == 1:\r\n",
      "            outputs = outputs[0]\r\n",
      "        elif len(outputs) == 2:\r\n",
      "            outputs = outputs[1]\r\n",
      "        else:\r\n",
      "            outputs = outputs[1:]\r\n",
      "\r\n",
      "        return outputs\r\n",
      "\r\n",
      "    def load_variable(self, checkpoint, name):\r\n",
      "        \"\"\"加载单个变量的函数\r\n",
      "        \"\"\"\r\n",
      "        variable = super(BERT, self).load_variable(checkpoint, name)\r\n",
      "        if name in [\r\n",
      "            'bert/embeddings/word_embeddings',\r\n",
      "            'cls/predictions/output_bias',\r\n",
      "        ]:\r\n",
      "            if self.keep_tokens is None:\r\n",
      "                return variable\r\n",
      "            else:\r\n",
      "                return variable[self.keep_tokens]\r\n",
      "        elif name == 'cls/seq_relationship/output_weights':\r\n",
      "            return variable.T\r\n",
      "        else:\r\n",
      "            return variable\r\n",
      "\r\n",
      "    def create_variable(self, name, value):\r\n",
      "        \"\"\"在tensorflow中创建一个变量\r\n",
      "        \"\"\"\r\n",
      "        if name == 'cls/seq_relationship/output_weights':\r\n",
      "            value = value.T\r\n",
      "        return super(BERT, self).create_variable(name, value)\r\n",
      "\r\n",
      "    def variable_mapping(self):\r\n",
      "        \"\"\"映射到官方BERT权重格式\r\n",
      "        \"\"\"\r\n",
      "        mapping = {\r\n",
      "            'Embedding-Token': ['bert/embeddings/word_embeddings'],\r\n",
      "            'Embedding-Segment': ['bert/embeddings/token_type_embeddings'],\r\n",
      "            'Embedding-Position': ['bert/embeddings/position_embeddings'],\r\n",
      "            'Embedding-Norm': [\r\n",
      "                'bert/embeddings/LayerNorm/beta',\r\n",
      "                'bert/embeddings/LayerNorm/gamma',\r\n",
      "            ],\r\n",
      "            'Embedding-Mapping': [\r\n",
      "                'bert/encoder/embedding_hidden_mapping_in/kernel',\r\n",
      "                'bert/encoder/embedding_hidden_mapping_in/bias',\r\n",
      "            ],\r\n",
      "            'Pooler-Dense': [\r\n",
      "                'bert/pooler/dense/kernel',\r\n",
      "                'bert/pooler/dense/bias',\r\n",
      "            ],\r\n",
      "            'NSP-Proba': [\r\n",
      "                'cls/seq_relationship/output_weights',\r\n",
      "                'cls/seq_relationship/output_bias',\r\n",
      "            ],\r\n",
      "            'MLM-Dense': [\r\n",
      "                'cls/predictions/transform/dense/kernel',\r\n",
      "                'cls/predictions/transform/dense/bias',\r\n",
      "            ],\r\n",
      "            'MLM-Norm': [\r\n",
      "                'cls/predictions/transform/LayerNorm/beta',\r\n",
      "                'cls/predictions/transform/LayerNorm/gamma',\r\n",
      "            ],\r\n",
      "            'MLM-Bias': ['cls/predictions/output_bias'],\r\n",
      "        }\r\n",
      "\r\n",
      "        for i in range(self.num_hidden_layers):\r\n",
      "            prefix = 'bert/encoder/layer_%d/' % i\r\n",
      "            mapping.update({\r\n",
      "                'Transformer-%d-MultiHeadSelfAttention' % i: [\r\n",
      "                    prefix + 'attention/self/query/kernel',\r\n",
      "                    prefix + 'attention/self/query/bias',\r\n",
      "                    prefix + 'attention/self/key/kernel',\r\n",
      "                    prefix + 'attention/self/key/bias',\r\n",
      "                    prefix + 'attention/self/value/kernel',\r\n",
      "                    prefix + 'attention/self/value/bias',\r\n",
      "                    prefix + 'attention/output/dense/kernel',\r\n",
      "                    prefix + 'attention/output/dense/bias',\r\n",
      "                ],\r\n",
      "                'Transformer-%d-MultiHeadSelfAttention-Norm' % i: [\r\n",
      "                    prefix + 'attention/output/LayerNorm/beta',\r\n",
      "                    prefix + 'attention/output/LayerNorm/gamma',\r\n",
      "                ],\r\n",
      "                'Transformer-%d-FeedForward' % i: [\r\n",
      "                    prefix + 'intermediate/dense/kernel',\r\n",
      "                    prefix + 'intermediate/dense/bias',\r\n",
      "                    prefix + 'output/dense/kernel',\r\n",
      "                    prefix + 'output/dense/bias',\r\n",
      "                ],\r\n",
      "                'Transformer-%d-FeedForward-Norm' % i: [\r\n",
      "                    prefix + 'output/LayerNorm/beta',\r\n",
      "                    prefix + 'output/LayerNorm/gamma',\r\n",
      "                ],\r\n",
      "            })\r\n",
      "\r\n",
      "        return mapping\r\n",
      "\r\n",
      "\r\n",
      "class ALBERT(BERT):\r\n",
      "    \"\"\"构建ALBERT模型\r\n",
      "    \"\"\"\r\n",
      "    def apply_main_layers(self, inputs, index):\r\n",
      "        \"\"\"ALBERT的主体是基于Self-Attention的模块\r\n",
      "        顺序：Att --> Add --> LN --> FFN --> Add --> LN\r\n",
      "        \"\"\"\r\n",
      "        x = inputs\r\n",
      "        z = self.layer_norm_conds[0]\r\n",
      "\r\n",
      "        attention_name = 'Transformer-MultiHeadSelfAttention'\r\n",
      "        feed_forward_name = 'Transformer-FeedForward'\r\n",
      "        attention_mask = self.compute_attention_mask(index)\r\n",
      "\r\n",
      "        # Self Attention\r\n",
      "        xi, x, arguments = x, [x, x, x], {'a_mask': None}\r\n",
      "        if attention_mask is not None:\r\n",
      "            arguments['a_mask'] = True\r\n",
      "            x.append(attention_mask)\r\n",
      "\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=MultiHeadAttention,\r\n",
      "            arguments=arguments,\r\n",
      "            heads=self.num_attention_heads,\r\n",
      "            head_size=self.attention_head_size,\r\n",
      "            key_size=self.attention_key_size,\r\n",
      "            kernel_initializer=self.initializer,\r\n",
      "            name=attention_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Dropout,\r\n",
      "            rate=self.dropout_rate,\r\n",
      "            name='%s-Dropout' % attention_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=[xi, x], layer=Add, name='%s-Add' % attention_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=self.simplify([x, z]),\r\n",
      "            layer=LayerNormalization,\r\n",
      "            conditional=(z is not None),\r\n",
      "            hidden_units=self.layer_norm_conds[1],\r\n",
      "            hidden_activation=self.layer_norm_conds[2],\r\n",
      "            hidden_initializer=self.initializer,\r\n",
      "            name='%s-Norm' % attention_name\r\n",
      "        )\r\n",
      "\r\n",
      "        # Feed Forward\r\n",
      "        xi = x\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=FeedForward,\r\n",
      "            units=self.intermediate_size,\r\n",
      "            activation=self.hidden_act,\r\n",
      "            kernel_initializer=self.initializer,\r\n",
      "            name=feed_forward_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Dropout,\r\n",
      "            rate=self.dropout_rate,\r\n",
      "            name='%s-Dropout' % feed_forward_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=[xi, x], layer=Add, name='%s-Add' % feed_forward_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=self.simplify([x, z]),\r\n",
      "            layer=LayerNormalization,\r\n",
      "            conditional=(z is not None),\r\n",
      "            hidden_units=self.layer_norm_conds[1],\r\n",
      "            hidden_activation=self.layer_norm_conds[2],\r\n",
      "            hidden_initializer=self.initializer,\r\n",
      "            name='%s-Norm' % feed_forward_name\r\n",
      "        )\r\n",
      "\r\n",
      "        return x\r\n",
      "\r\n",
      "    def variable_mapping(self):\r\n",
      "        \"\"\"映射到官方ALBERT权重格式\r\n",
      "        \"\"\"\r\n",
      "        mapping = super(ALBERT, self).variable_mapping()\r\n",
      "\r\n",
      "        prefix = 'bert/encoder/transformer/group_0/inner_group_0/'\r\n",
      "        mapping.update({\r\n",
      "            'Transformer-MultiHeadSelfAttention': [\r\n",
      "                prefix + 'attention_1/self/query/kernel',\r\n",
      "                prefix + 'attention_1/self/query/bias',\r\n",
      "                prefix + 'attention_1/self/key/kernel',\r\n",
      "                prefix + 'attention_1/self/key/bias',\r\n",
      "                prefix + 'attention_1/self/value/kernel',\r\n",
      "                prefix + 'attention_1/self/value/bias',\r\n",
      "                prefix + 'attention_1/output/dense/kernel',\r\n",
      "                prefix + 'attention_1/output/dense/bias',\r\n",
      "            ],\r\n",
      "            'Transformer-MultiHeadSelfAttention-Norm': [\r\n",
      "                prefix + 'LayerNorm/beta',\r\n",
      "                prefix + 'LayerNorm/gamma',\r\n",
      "            ],\r\n",
      "            'Transformer-FeedForward': [\r\n",
      "                prefix + 'ffn_1/intermediate/dense/kernel',\r\n",
      "                prefix + 'ffn_1/intermediate/dense/bias',\r\n",
      "                prefix + 'ffn_1/intermediate/output/dense/kernel',\r\n",
      "                prefix + 'ffn_1/intermediate/output/dense/bias',\r\n",
      "            ],\r\n",
      "            'Transformer-FeedForward-Norm': [\r\n",
      "                prefix + 'LayerNorm_1/beta',\r\n",
      "                prefix + 'LayerNorm_1/gamma',\r\n",
      "            ],\r\n",
      "        })\r\n",
      "\r\n",
      "        return mapping\r\n",
      "\r\n",
      "\r\n",
      "class ALBERT_Unshared(BERT):\r\n",
      "    \"\"\"解开ALBERT共享约束，当成BERT用\r\n",
      "    \"\"\"\r\n",
      "    def variable_mapping(self):\r\n",
      "        \"\"\"映射到官方ALBERT权重格式\r\n",
      "        \"\"\"\r\n",
      "        mapping = super(ALBERT_Unshared, self).variable_mapping()\r\n",
      "\r\n",
      "        prefix = 'bert/encoder/transformer/group_0/inner_group_0/'\r\n",
      "        for i in range(self.num_hidden_layers):\r\n",
      "            mapping.update({\r\n",
      "                'Transformer-%d-MultiHeadSelfAttention' % i: [\r\n",
      "                    prefix + 'attention_1/self/query/kernel',\r\n",
      "                    prefix + 'attention_1/self/query/bias',\r\n",
      "                    prefix + 'attention_1/self/key/kernel',\r\n",
      "                    prefix + 'attention_1/self/key/bias',\r\n",
      "                    prefix + 'attention_1/self/value/kernel',\r\n",
      "                    prefix + 'attention_1/self/value/bias',\r\n",
      "                    prefix + 'attention_1/output/dense/kernel',\r\n",
      "                    prefix + 'attention_1/output/dense/bias',\r\n",
      "                ],\r\n",
      "                'Transformer-%d-MultiHeadSelfAttention-Norm' % i: [\r\n",
      "                    prefix + 'LayerNorm/beta',\r\n",
      "                    prefix + 'LayerNorm/gamma',\r\n",
      "                ],\r\n",
      "                'Transformer-%d-FeedForward' % i: [\r\n",
      "                    prefix + 'ffn_1/intermediate/dense/kernel',\r\n",
      "                    prefix + 'ffn_1/intermediate/dense/bias',\r\n",
      "                    prefix + 'ffn_1/intermediate/output/dense/kernel',\r\n",
      "                    prefix + 'ffn_1/intermediate/output/dense/bias',\r\n",
      "                ],\r\n",
      "                'Transformer-%d-FeedForward-Norm' % i: [\r\n",
      "                    prefix + 'LayerNorm_1/beta',\r\n",
      "                    prefix + 'LayerNorm_1/gamma',\r\n",
      "                ],\r\n",
      "            })\r\n",
      "\r\n",
      "        return mapping\r\n",
      "\r\n",
      "\r\n",
      "class NEZHA(BERT):\r\n",
      "    \"\"\"华为推出的NAZHA模型\r\n",
      "    链接：https://arxiv.org/abs/1909.00204\r\n",
      "    \"\"\"\r\n",
      "    def apply_embeddings(self, inputs):\r\n",
      "        \"\"\"NEZHA的embedding是token、segment两者embedding之和\r\n",
      "        \"\"\"\r\n",
      "        x, s = inputs\r\n",
      "        z = self.layer_norm_conds[0]\r\n",
      "\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Embedding,\r\n",
      "            input_dim=self.vocab_size,\r\n",
      "            output_dim=self.embedding_size,\r\n",
      "            embeddings_initializer=self.initializer,\r\n",
      "            mask_zero=True,\r\n",
      "            name='Embedding-Token'\r\n",
      "        )\r\n",
      "        s = self.apply(\r\n",
      "            inputs=s,\r\n",
      "            layer=Embedding,\r\n",
      "            input_dim=2,\r\n",
      "            output_dim=self.embedding_size,\r\n",
      "            embeddings_initializer=self.initializer,\r\n",
      "            name='Embedding-Segment'\r\n",
      "        )\r\n",
      "        x = self.apply(inputs=[x, s], layer=Add, name='Embedding-Token-Segment')\r\n",
      "        x = self.apply(\r\n",
      "            inputs=self.simplify([x, z]),\r\n",
      "            layer=LayerNormalization,\r\n",
      "            conditional=(z is not None),\r\n",
      "            hidden_units=self.layer_norm_conds[1],\r\n",
      "            hidden_activation=self.layer_norm_conds[2],\r\n",
      "            hidden_initializer=self.initializer,\r\n",
      "            name='Embedding-Norm'\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Dropout,\r\n",
      "            rate=self.dropout_rate,\r\n",
      "            name='Embedding-Dropout'\r\n",
      "        )\r\n",
      "        if self.embedding_size != self.hidden_size:\r\n",
      "            x = self.apply(\r\n",
      "                inputs=x,\r\n",
      "                layer=Dense,\r\n",
      "                units=self.hidden_size,\r\n",
      "                kernel_initializer=self.initializer,\r\n",
      "                name='Embedding-Mapping'\r\n",
      "            )\r\n",
      "\r\n",
      "        return x\r\n",
      "\r\n",
      "    def apply_main_layers(self, inputs, index):\r\n",
      "        \"\"\"NEZHA的主体是基于Self-Attention的模块\r\n",
      "        顺序：Att --> Add --> LN --> FFN --> Add --> LN\r\n",
      "        \"\"\"\r\n",
      "        x = inputs\r\n",
      "        z = self.layer_norm_conds[0]\r\n",
      "\r\n",
      "        attention_name = 'Transformer-%d-MultiHeadSelfAttention' % index\r\n",
      "        feed_forward_name = 'Transformer-%d-FeedForward' % index\r\n",
      "        attention_mask = self.compute_attention_mask(index)\r\n",
      "        position_bias = self.compute_position_bias(x)\r\n",
      "\r\n",
      "        # Self Attention\r\n",
      "        xi, x = x, [x, x, x, position_bias]\r\n",
      "        arguments = {'a_mask': None, 'p_bias': 'typical_relative'}\r\n",
      "        if attention_mask is not None:\r\n",
      "            arguments['a_mask'] = True\r\n",
      "            x.insert(3, attention_mask)\r\n",
      "\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=MultiHeadAttention,\r\n",
      "            arguments=arguments,\r\n",
      "            heads=self.num_attention_heads,\r\n",
      "            head_size=self.attention_head_size,\r\n",
      "            key_size=self.attention_key_size,\r\n",
      "            kernel_initializer=self.initializer,\r\n",
      "            name=attention_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Dropout,\r\n",
      "            rate=self.dropout_rate,\r\n",
      "            name='%s-Dropout' % attention_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=[xi, x], layer=Add, name='%s-Add' % attention_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=self.simplify([x, z]),\r\n",
      "            layer=LayerNormalization,\r\n",
      "            conditional=(z is not None),\r\n",
      "            hidden_units=self.layer_norm_conds[1],\r\n",
      "            hidden_activation=self.layer_norm_conds[2],\r\n",
      "            hidden_initializer=self.initializer,\r\n",
      "            name='%s-Norm' % attention_name\r\n",
      "        )\r\n",
      "\r\n",
      "        # Feed Forward\r\n",
      "        xi = x\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=FeedForward,\r\n",
      "            units=self.intermediate_size,\r\n",
      "            activation=self.hidden_act,\r\n",
      "            kernel_initializer=self.initializer,\r\n",
      "            name=feed_forward_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Dropout,\r\n",
      "            rate=self.dropout_rate,\r\n",
      "            name='%s-Dropout' % feed_forward_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=[xi, x], layer=Add, name='%s-Add' % feed_forward_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=self.simplify([x, z]),\r\n",
      "            layer=LayerNormalization,\r\n",
      "            conditional=(z is not None),\r\n",
      "            hidden_units=self.layer_norm_conds[1],\r\n",
      "            hidden_activation=self.layer_norm_conds[2],\r\n",
      "            hidden_initializer=self.initializer,\r\n",
      "            name='%s-Norm' % feed_forward_name\r\n",
      "        )\r\n",
      "\r\n",
      "        return x\r\n",
      "\r\n",
      "    def compute_position_bias(self, inputs=None):\r\n",
      "        \"\"\"经典相对位置编码\r\n",
      "        \"\"\"\r\n",
      "        if self.position_bias is None:\r\n",
      "\r\n",
      "            def sinusoidal(shape, dtype=None):\r\n",
      "                \"\"\"NEZHA直接使用Sin-Cos形式的位置向量\r\n",
      "                \"\"\"\r\n",
      "                vocab_size, depth = shape\r\n",
      "                embeddings = np.zeros(shape)\r\n",
      "                for pos in range(vocab_size):\r\n",
      "                    for i in range(depth // 2):\r\n",
      "                        theta = pos / np.power(10000, 2. * i / depth)\r\n",
      "                        embeddings[pos, 2 * i] = np.sin(theta)\r\n",
      "                        embeddings[pos, 2 * i + 1] = np.cos(theta)\r\n",
      "                return embeddings\r\n",
      "\r\n",
      "            x = inputs\r\n",
      "            self.position_bias = self.apply(\r\n",
      "                inputs=[x, x],\r\n",
      "                layer=RelativePositionEmbedding,\r\n",
      "                input_dim=2 * 64 + 1,\r\n",
      "                output_dim=self.attention_head_size,\r\n",
      "                embeddings_initializer=sinusoidal,\r\n",
      "                name='Embedding-Relative-Position',\r\n",
      "                trainable=False\r\n",
      "            )\r\n",
      "\r\n",
      "        return self.position_bias\r\n",
      "\r\n",
      "\r\n",
      "class ELECTRA(BERT):\r\n",
      "    \"\"\"Google推出的ELECTRA模型\r\n",
      "    链接：https://arxiv.org/abs/2003.10555\r\n",
      "    \"\"\"\r\n",
      "    @delete_arguments('with_pool', 'with_mlm')\r\n",
      "    def __init__(\r\n",
      "        self,\r\n",
      "        max_position,  # 序列最大长度\r\n",
      "        **kwargs  # 其余参数\r\n",
      "    ):\r\n",
      "        if 'keep_tokens' in kwargs:\r\n",
      "            del kwargs['keep_tokens']\r\n",
      "\r\n",
      "        super(ELECTRA, self).__init__(max_position, **kwargs)\r\n",
      "\r\n",
      "    def apply_final_layers(self, inputs):\r\n",
      "        x = inputs\r\n",
      "        z = self.layer_norm_conds[0]\r\n",
      "        return x\r\n",
      "\r\n",
      "    def variable_mapping(self):\r\n",
      "        mapping = super(ELECTRA, self).variable_mapping()\r\n",
      "        mapping['Embedding-Mapping'] = [\r\n",
      "            'electra/embeddings_project/kernel',\r\n",
      "            'electra/embeddings_project/bias',\r\n",
      "        ]\r\n",
      "        mapping = {\r\n",
      "            k: [i.replace('bert/', 'electra/') for i in v]\r\n",
      "            for k, v in mapping.items()\r\n",
      "        }\r\n",
      "        return mapping\r\n",
      "\r\n",
      "\r\n",
      "class GPT2_ML(Transformer):\r\n",
      "    \"\"\"构建GPT2_ML模型\r\n",
      "    链接: https://github.com/imcaspar/gpt2-ml\r\n",
      "    \"\"\"\r\n",
      "    def __init__(\r\n",
      "        self,\r\n",
      "        max_position,  # 序列最大长度\r\n",
      "        final_activation='softmax',  # 预测分布的激活函数\r\n",
      "        **kwargs  # 其余参数\r\n",
      "    ):\r\n",
      "        super(GPT2_ML, self).__init__(**kwargs)\r\n",
      "        self.max_position = max_position\r\n",
      "        self.final_activation = final_activation\r\n",
      "\r\n",
      "    def get_inputs(self):\r\n",
      "        \"\"\"GPT2_ML的输入是token_ids和segment_ids\r\n",
      "        \"\"\"\r\n",
      "        x_in = Input(shape=(self.sequence_length,), name='Input-Token')\r\n",
      "        return x_in\r\n",
      "\r\n",
      "    def apply_embeddings(self, inputs):\r\n",
      "        \"\"\"GPT2_ML的embedding是token、position两者embedding之和\r\n",
      "        \"\"\"\r\n",
      "        x = inputs\r\n",
      "        z = self.layer_norm_conds[0]\r\n",
      "\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Embedding,\r\n",
      "            input_dim=self.vocab_size,\r\n",
      "            output_dim=self.embedding_size,\r\n",
      "            embeddings_initializer=self.initializer,\r\n",
      "            mask_zero=True,\r\n",
      "            name='Embedding-Token'\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=PositionEmbedding,\r\n",
      "            input_dim=self.max_position,\r\n",
      "            output_dim=self.embedding_size,\r\n",
      "            merge_mode='add',\r\n",
      "            embeddings_initializer=self.initializer,\r\n",
      "            name='Embedding-Position'\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=self.simplify([x, z]),\r\n",
      "            layer=LayerNormalization,\r\n",
      "            epsilon=1e-5,\r\n",
      "            conditional=(z is not None),\r\n",
      "            hidden_units=self.layer_norm_conds[1],\r\n",
      "            hidden_activation=self.layer_norm_conds[2],\r\n",
      "            hidden_initializer=self.initializer,\r\n",
      "            name='Embedding-Norm'\r\n",
      "        )\r\n",
      "        if self.embedding_size != self.hidden_size:\r\n",
      "            x = self.apply(\r\n",
      "                inputs=x,\r\n",
      "                layer=Dense,\r\n",
      "                units=self.hidden_size,\r\n",
      "                kernel_initializer=self.initializer,\r\n",
      "                name='Embedding-Mapping'\r\n",
      "            )\r\n",
      "\r\n",
      "        return x\r\n",
      "\r\n",
      "    def apply_main_layers(self, inputs, index):\r\n",
      "        \"\"\"GPT2_ML的主体是基于Self-Attention的模块\r\n",
      "        顺序：Att  --> LN --> FFN --> Add --> LN\r\n",
      "        \"\"\"\r\n",
      "        x = inputs\r\n",
      "        z = self.layer_norm_conds[0]\r\n",
      "\r\n",
      "        attention_name = 'Transformer-%d-MultiHeadSelfAttention' % index\r\n",
      "        feed_forward_name = 'Transformer-%d-FeedForward' % index\r\n",
      "        attention_mask = self.compute_attention_mask(index)\r\n",
      "\r\n",
      "        # Self Attention\r\n",
      "        xi, x, arguments = x, [x, x, x, attention_mask], {'a_mask': True}\r\n",
      "\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=MultiHeadAttention,\r\n",
      "            arguments=arguments,\r\n",
      "            heads=self.num_attention_heads,\r\n",
      "            head_size=self.attention_head_size,\r\n",
      "            key_size=self.attention_key_size,\r\n",
      "            kernel_initializer=self.initializer,\r\n",
      "            name=attention_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Dropout,\r\n",
      "            rate=self.dropout_rate,\r\n",
      "            name='%s-Dropout' % attention_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=[xi, x], layer=Add, name='%s-Add' % attention_name\r\n",
      "        )\r\n",
      "\r\n",
      "        # Feed Forward\r\n",
      "        xi = x\r\n",
      "        x = self.apply(\r\n",
      "            inputs=self.simplify([x, z]),\r\n",
      "            layer=LayerNormalization,\r\n",
      "            epsilon=1e-5,\r\n",
      "            conditional=(z is not None),\r\n",
      "            hidden_units=self.layer_norm_conds[1],\r\n",
      "            hidden_activation=self.layer_norm_conds[2],\r\n",
      "            hidden_initializer=self.initializer,\r\n",
      "            name='%s-Norm-0' % feed_forward_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=FeedForward,\r\n",
      "            units=self.intermediate_size,\r\n",
      "            activation=self.hidden_act,\r\n",
      "            kernel_initializer=self.initializer,\r\n",
      "            name=feed_forward_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Dropout,\r\n",
      "            rate=self.dropout_rate,\r\n",
      "            name='%s-Dropout' % feed_forward_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=[xi, x], layer=Add, name='%s-Add' % feed_forward_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=self.simplify([x, z]),\r\n",
      "            layer=LayerNormalization,\r\n",
      "            epsilon=1e-5,\r\n",
      "            conditional=(z is not None),\r\n",
      "            hidden_units=self.layer_norm_conds[1],\r\n",
      "            hidden_activation=self.layer_norm_conds[2],\r\n",
      "            hidden_initializer=self.initializer,\r\n",
      "            name='%s-Norm-1' % feed_forward_name\r\n",
      "        )\r\n",
      "\r\n",
      "        return x\r\n",
      "\r\n",
      "    def apply_final_layers(self, inputs):\r\n",
      "        \"\"\"剩余部分\r\n",
      "        \"\"\"\r\n",
      "        x = inputs\r\n",
      "        z = self.layer_norm_conds[0]\r\n",
      "\r\n",
      "        # Language Model部分\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Embedding,\r\n",
      "            arguments={'mode': 'dense'},\r\n",
      "            name='Embedding-Token'\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Activation,\r\n",
      "            activation=self.final_activation,\r\n",
      "            name='LM-Activation'\r\n",
      "        )\r\n",
      "\r\n",
      "        return x\r\n",
      "\r\n",
      "    def load_variable(self, checkpoint, name):\r\n",
      "        \"\"\"加载单个变量的函数\r\n",
      "        \"\"\"\r\n",
      "        variable = super(GPT2_ML, self).load_variable(checkpoint, name)\r\n",
      "        if name == 'newslm/embeddings/word_embed':\r\n",
      "            if self.keep_tokens is None:\r\n",
      "                return variable\r\n",
      "            else:\r\n",
      "                return variable[self.keep_tokens]\r\n",
      "        else:\r\n",
      "            return variable\r\n",
      "\r\n",
      "    def compute_attention_mask(self, inputs=None):\r\n",
      "        \"\"\"添加下三角形式的attention mask\r\n",
      "        \"\"\"\r\n",
      "        if self.attention_mask is None:\r\n",
      "\r\n",
      "            def lm_mask(s):\r\n",
      "                seq_len = K.shape(s)[1]\r\n",
      "                idxs = K.arange(0, seq_len)\r\n",
      "                mask = idxs[None, :] <= idxs[:, None]\r\n",
      "                mask = K.cast(mask, K.floatx())\r\n",
      "                return mask[None, None]\r\n",
      "\r\n",
      "            self.attention_mask = self.apply(\r\n",
      "                inputs=self.inputs[0],\r\n",
      "                layer=Lambda,\r\n",
      "                function=lm_mask,\r\n",
      "                name='Attention-LM-Mask'\r\n",
      "            )\r\n",
      "\r\n",
      "        return self.attention_mask\r\n",
      "\r\n",
      "    def variable_mapping(self):\r\n",
      "        \"\"\"映射到官方GPT2_ML权重格式\r\n",
      "        \"\"\"\r\n",
      "        mapping = {\r\n",
      "            'Embedding-Token': ['newslm/embeddings/word_embed'],\r\n",
      "            'Embedding-Position': ['newslm/embeddings/pos_embed'],\r\n",
      "            'Embedding-Norm': [\r\n",
      "                'newslm/embeddings/LayerNorm_embed_norm/beta',\r\n",
      "                'newslm/embeddings/LayerNorm_embed_norm/gamma',\r\n",
      "            ],\r\n",
      "        }\r\n",
      "\r\n",
      "        for i in range(self.num_hidden_layers):\r\n",
      "            prefix = 'newslm/layer%02d/' % i\r\n",
      "            mapping.update({\r\n",
      "                'Transformer-%d-MultiHeadSelfAttention' % i: [\r\n",
      "                    prefix + 'query_layer/kernel',\r\n",
      "                    prefix + 'query_layer/bias',\r\n",
      "                    prefix + 'key_layer/kernel',\r\n",
      "                    prefix + 'key_layer/bias',\r\n",
      "                    prefix + 'value_layer/kernel',\r\n",
      "                    prefix + 'value_layer/bias',\r\n",
      "                    prefix + 'context_projection_layer/kernel',\r\n",
      "                    prefix + 'context_projection_layer/bias',\r\n",
      "                ],\r\n",
      "                'Transformer-%d-FeedForward-Norm-0' % i: [\r\n",
      "                    prefix + 'LayerNorm_mlp_ln0/beta',\r\n",
      "                    prefix + 'LayerNorm_mlp_ln0/gamma',\r\n",
      "                ],\r\n",
      "                'Transformer-%d-FeedForward' % i: [\r\n",
      "                    prefix + 'intermediate/kernel',\r\n",
      "                    prefix + 'intermediate/bias',\r\n",
      "                    prefix + 'output/kernel',\r\n",
      "                    prefix + 'output/bias',\r\n",
      "                ],\r\n",
      "                'Transformer-%d-FeedForward-Norm-1' % i: [\r\n",
      "                    prefix + 'LayerNorm_mlp_ln1/beta',\r\n",
      "                    prefix + 'LayerNorm_mlp_ln1/gamma',\r\n",
      "                ],\r\n",
      "            })\r\n",
      "\r\n",
      "        return mapping\r\n",
      "\r\n",
      "\r\n",
      "class T5_Base(Transformer):\r\n",
      "    \"\"\"Google的T5模型（基类）\r\n",
      "    \"\"\"\r\n",
      "    def load_variable(self, checkpoint, name):\r\n",
      "        \"\"\"加载单个变量的函数\r\n",
      "        \"\"\"\r\n",
      "        variable = super(T5_Base, self).load_variable(checkpoint, name)\r\n",
      "        if name == 'shared/embedding':\r\n",
      "            if self.keep_tokens is None:\r\n",
      "                return variable\r\n",
      "            else:\r\n",
      "                return variable[self.keep_tokens]\r\n",
      "        elif 'relative_attention_bias' in name:\r\n",
      "            return variable.T\r\n",
      "        else:\r\n",
      "            return variable\r\n",
      "\r\n",
      "    def create_variable(self, name, value):\r\n",
      "        \"\"\"在tensorflow中创建一个变量\r\n",
      "        \"\"\"\r\n",
      "        if 'relative_attention_bias' in name:\r\n",
      "            value = value.T\r\n",
      "        return super(T5_Base, self).create_variable(name, value)\r\n",
      "\r\n",
      "    def variable_mapping(self):\r\n",
      "        \"\"\"映射到官方T5权重格式\r\n",
      "        \"\"\"\r\n",
      "        mapping = {\r\n",
      "            'Embedding-Token': ['shared/embedding'],\r\n",
      "            'Encoder-Embedding-Relative-Position': [\r\n",
      "                'encoder/block_000/layer_000/SelfAttention/relative_attention_bias'\r\n",
      "            ],\r\n",
      "            'Encoder-Output-Norm': ['encoder/final_layer_norm/scale'],\r\n",
      "            'Decoder-Embedding-Relative-Position': [\r\n",
      "                'decoder/block_000/layer_000/SelfAttention/relative_attention_bias',\r\n",
      "            ],\r\n",
      "            'Decoder-Output-Norm': ['decoder/final_layer_norm/scale'],\r\n",
      "        }\r\n",
      "\r\n",
      "        for i in range(self.num_hidden_layers):\r\n",
      "            # Encoder主体\r\n",
      "            prefix = 'encoder/block_%03d/' % i\r\n",
      "            mapping.update({\r\n",
      "                'Encoder-Transformer-%d-MultiHeadSelfAttention' % i: [\r\n",
      "                    prefix + 'layer_000/SelfAttention/q',\r\n",
      "                    prefix + 'layer_000/SelfAttention/k',\r\n",
      "                    prefix + 'layer_000/SelfAttention/v',\r\n",
      "                    prefix + 'layer_000/SelfAttention/o',\r\n",
      "                ],\r\n",
      "                'Encoder-Transformer-%d-MultiHeadSelfAttention-Norm' % i: [\r\n",
      "                    prefix + 'layer_000/layer_norm/scale',\r\n",
      "                ],\r\n",
      "                'Encoder-Transformer-%d-FeedForward' % i: [\r\n",
      "                    prefix + 'layer_001/DenseReluDense/wi/kernel',\r\n",
      "                    prefix + 'layer_001/DenseReluDense/wo/kernel',\r\n",
      "                ],\r\n",
      "                'Encoder-Transformer-%d-FeedForward-Norm' % i: [\r\n",
      "                    prefix + 'layer_001/layer_norm/scale',\r\n",
      "                ],\r\n",
      "            })\r\n",
      "            # Decoder主体\r\n",
      "            prefix = 'decoder/block_%03d/' % i\r\n",
      "            mapping.update({\r\n",
      "                'Decoder-Transformer-%d-MultiHeadSelfAttention' % i: [\r\n",
      "                    prefix + 'layer_000/SelfAttention/q',\r\n",
      "                    prefix + 'layer_000/SelfAttention/k',\r\n",
      "                    prefix + 'layer_000/SelfAttention/v',\r\n",
      "                    prefix + 'layer_000/SelfAttention/o',\r\n",
      "                ],\r\n",
      "                'Decoder-Transformer-%d-MultiHeadSelfAttention-Norm' % i: [\r\n",
      "                    prefix + 'layer_000/layer_norm/scale',\r\n",
      "                ],\r\n",
      "                'Decoder-Transformer-%d-MultiHeadCrossAttention' % i: [\r\n",
      "                    prefix + 'layer_001/EncDecAttention/q',\r\n",
      "                    prefix + 'layer_001/EncDecAttention/k',\r\n",
      "                    prefix + 'layer_001/EncDecAttention/v',\r\n",
      "                    prefix + 'layer_001/EncDecAttention/o',\r\n",
      "                ],\r\n",
      "                'Decoder-Transformer-%d-MultiHeadCrossAttention-Norm' % i: [\r\n",
      "                    prefix + 'layer_001/layer_norm/scale',\r\n",
      "                ],\r\n",
      "                'Decoder-Transformer-%d-FeedForward' % i: [\r\n",
      "                    prefix + 'layer_002/DenseReluDense/wi/kernel',\r\n",
      "                    prefix + 'layer_002/DenseReluDense/wo/kernel',\r\n",
      "                ],\r\n",
      "                'Decoder-Transformer-%d-FeedForward-Norm' % i: [\r\n",
      "                    prefix + 'layer_002/layer_norm/scale',\r\n",
      "                ],\r\n",
      "            })\r\n",
      "\r\n",
      "        return mapping\r\n",
      "\r\n",
      "\r\n",
      "class T5_Encoder(T5_Base):\r\n",
      "    \"\"\"Google的T5模型（Encoder）\r\n",
      "    \"\"\"\r\n",
      "    def get_inputs(self):\r\n",
      "        \"\"\"T5的Encoder的输入只有token_ids\r\n",
      "        \"\"\"\r\n",
      "        x_in = Input(shape=(self.sequence_length,), name='Encoder-Input-Token')\r\n",
      "        return x_in\r\n",
      "\r\n",
      "    def apply_embeddings(self, inputs):\r\n",
      "        \"\"\"T5的embedding只有token embedding，\r\n",
      "        并把relative position embedding准备好，待attention使用。\r\n",
      "        \"\"\"\r\n",
      "        x = inputs\r\n",
      "\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Embedding,\r\n",
      "            input_dim=self.vocab_size,\r\n",
      "            output_dim=self.embedding_size,\r\n",
      "            embeddings_initializer=self.initializer,\r\n",
      "            mask_zero=True,\r\n",
      "            name='Embedding-Token'\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Dropout,\r\n",
      "            rate=self.dropout_rate,\r\n",
      "            name='Encoder-Embedding-Dropout'\r\n",
      "        )\r\n",
      "        if self.embedding_size != self.hidden_size:\r\n",
      "            x = self.apply(\r\n",
      "                inputs=x,\r\n",
      "                layer=Dense,\r\n",
      "                units=self.hidden_size,\r\n",
      "                kernel_initializer=self.initializer,\r\n",
      "                name='Encoder-Embedding-Mapping'\r\n",
      "            )\r\n",
      "\r\n",
      "        return x\r\n",
      "\r\n",
      "    def apply_main_layers(self, inputs, index):\r\n",
      "        \"\"\"T5的Encoder的主体是基于Self-Attention的模块\r\n",
      "        顺序：LN --> Att --> Add --> LN --> FFN --> Add\r\n",
      "        \"\"\"\r\n",
      "        x = inputs\r\n",
      "        z = self.layer_norm_conds[0]\r\n",
      "\r\n",
      "        attention_name = 'Encoder-Transformer-%d-MultiHeadSelfAttention' % index\r\n",
      "        feed_forward_name = 'Encoder-Transformer-%d-FeedForward' % index\r\n",
      "        attention_mask = self.compute_attention_mask(index)\r\n",
      "        position_bias = self.compute_position_bias(x)\r\n",
      "\r\n",
      "        # Self Attention\r\n",
      "        xi = x\r\n",
      "        x = self.apply(\r\n",
      "            inputs=self.simplify([x, z]),\r\n",
      "            layer=LayerNormalization,\r\n",
      "            center=False,\r\n",
      "            epsilon=1e-6,\r\n",
      "            conditional=(z is not None),\r\n",
      "            hidden_units=self.layer_norm_conds[1],\r\n",
      "            hidden_activation=self.layer_norm_conds[2],\r\n",
      "            hidden_initializer=self.initializer,\r\n",
      "            name='%s-Norm' % attention_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=[x, x, x, position_bias],\r\n",
      "            layer=MultiHeadAttention,\r\n",
      "            arguments={'p_bias': 't5_relative'},\r\n",
      "            heads=self.num_attention_heads,\r\n",
      "            head_size=self.attention_head_size,\r\n",
      "            key_size=self.attention_key_size,\r\n",
      "            use_bias=False,\r\n",
      "            attention_scale=False,\r\n",
      "            kernel_initializer=self.initializer,\r\n",
      "            name=attention_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Dropout,\r\n",
      "            rate=self.dropout_rate,\r\n",
      "            name='%s-Dropout' % attention_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=[xi, x], layer=Add, name='%s-Add' % attention_name\r\n",
      "        )\r\n",
      "\r\n",
      "        # Feed Forward\r\n",
      "        xi = x\r\n",
      "        x = self.apply(\r\n",
      "            inputs=self.simplify([x, z]),\r\n",
      "            layer=LayerNormalization,\r\n",
      "            center=False,\r\n",
      "            epsilon=1e-6,\r\n",
      "            conditional=(z is not None),\r\n",
      "            hidden_units=self.layer_norm_conds[1],\r\n",
      "            hidden_activation=self.layer_norm_conds[2],\r\n",
      "            hidden_initializer=self.initializer,\r\n",
      "            name='%s-Norm' % feed_forward_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=FeedForward,\r\n",
      "            units=self.intermediate_size,\r\n",
      "            activation=self.hidden_act,\r\n",
      "            use_bias=False,\r\n",
      "            kernel_initializer=self.initializer,\r\n",
      "            name=feed_forward_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Dropout,\r\n",
      "            rate=self.dropout_rate,\r\n",
      "            name='%s-Dropout' % feed_forward_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=[xi, x], layer=Add, name='%s-Add' % feed_forward_name\r\n",
      "        )\r\n",
      "\r\n",
      "        return x\r\n",
      "\r\n",
      "    def apply_final_layers(self, inputs):\r\n",
      "        \"\"\"剩余部分\r\n",
      "        \"\"\"\r\n",
      "        x = inputs\r\n",
      "        z = self.layer_norm_conds[0]\r\n",
      "\r\n",
      "        x = self.apply(\r\n",
      "            inputs=self.simplify([x, z]),\r\n",
      "            layer=LayerNormalization,\r\n",
      "            center=False,\r\n",
      "            epsilon=1e-6,\r\n",
      "            conditional=(z is not None),\r\n",
      "            hidden_units=self.layer_norm_conds[1],\r\n",
      "            hidden_activation=self.layer_norm_conds[2],\r\n",
      "            hidden_initializer=self.initializer,\r\n",
      "            name='Encoder-Output-Norm'\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Dropout,\r\n",
      "            rate=self.dropout_rate,\r\n",
      "            name='Encoder-Output-Dropout'\r\n",
      "        )\r\n",
      "\r\n",
      "        return x\r\n",
      "\r\n",
      "    def compute_position_bias(self, inputs=None):\r\n",
      "        \"\"\"T5相对位置编码\r\n",
      "        \"\"\"\r\n",
      "        if self.position_bias is None:\r\n",
      "\r\n",
      "            x = inputs\r\n",
      "            p = self.apply(\r\n",
      "                inputs=[x, x],\r\n",
      "                layer=RelativePositionEmbeddingT5,\r\n",
      "                input_dim=32,\r\n",
      "                output_dim=self.num_attention_heads,\r\n",
      "                bidirectional=True,\r\n",
      "                embeddings_initializer=self.initializer,\r\n",
      "                name='Encoder-Embedding-Relative-Position'\r\n",
      "            )\r\n",
      "            self.position_bias = p\r\n",
      "\r\n",
      "        return self.position_bias\r\n",
      "\r\n",
      "\r\n",
      "class T5_Decoder(Transformer):\r\n",
      "    \"\"\"Google的T5模型（Decoder）\r\n",
      "    \"\"\"\r\n",
      "    def __init__(self, with_lm=True, **kwargs):\r\n",
      "        super(T5_Decoder, self).__init__(**kwargs)\r\n",
      "        if with_lm is True:\r\n",
      "            self.with_lm = 'softmax'\r\n",
      "        else:\r\n",
      "            self.with_lm = with_lm\r\n",
      "\r\n",
      "    def get_inputs(self):\r\n",
      "        \"\"\"T5的Decoder的输入为context序列和token_ids\r\n",
      "        \"\"\"\r\n",
      "        c_in = Input(\r\n",
      "            shape=(self.sequence_length, self.hidden_size),\r\n",
      "            name='Input-Context'\r\n",
      "        )\r\n",
      "        x_in = Input(shape=(self.sequence_length,), name='Decoder-Input-Token')\r\n",
      "        return [c_in, x_in]\r\n",
      "\r\n",
      "    def apply_embeddings(self, inputs):\r\n",
      "        \"\"\"T5的embedding只有token embedding，\r\n",
      "        并把relative position embedding准备好，待attention使用。\r\n",
      "        \"\"\"\r\n",
      "        c, x = inputs\r\n",
      "\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Embedding,\r\n",
      "            input_dim=self.vocab_size,\r\n",
      "            output_dim=self.embedding_size,\r\n",
      "            embeddings_initializer=self.initializer,\r\n",
      "            mask_zero=True,\r\n",
      "            name='Embedding-Token'\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Dropout,\r\n",
      "            rate=self.dropout_rate,\r\n",
      "            name='Decoder-Embedding-Dropout'\r\n",
      "        )\r\n",
      "        if self.embedding_size != self.hidden_size:\r\n",
      "            x = self.apply(\r\n",
      "                inputs=x,\r\n",
      "                layer=Dense,\r\n",
      "                units=self.hidden_size,\r\n",
      "                kernel_initializer=self.initializer,\r\n",
      "                name='Decoder-Embedding-Mapping'\r\n",
      "            )\r\n",
      "\r\n",
      "        return [c, x]\r\n",
      "\r\n",
      "    def apply_main_layers(self, inputs, index):\r\n",
      "        \"\"\"T5的Dencoder主体是基于Self-Attention、Cross-Attention的模块\r\n",
      "        顺序：LN --> Att1 --> Add --> LN --> Att2 --> Add -->  LN --> FFN --> Add\r\n",
      "        \"\"\"\r\n",
      "        c, x = inputs\r\n",
      "        z = self.layer_norm_conds[0]\r\n",
      "\r\n",
      "        self_attention_name = 'Decoder-Transformer-%d-MultiHeadSelfAttention' % index\r\n",
      "        cross_attention_name = 'Decoder-Transformer-%d-MultiHeadCrossAttention' % index\r\n",
      "        feed_forward_name = 'Decoder-Transformer-%d-FeedForward' % index\r\n",
      "        attention_mask = self.compute_attention_mask(index)\r\n",
      "        position_bias = self.compute_position_bias([x, c])\r\n",
      "\r\n",
      "        # Self Attention\r\n",
      "        xi = x\r\n",
      "        x = self.apply(\r\n",
      "            inputs=self.simplify([x, z]),\r\n",
      "            layer=LayerNormalization,\r\n",
      "            center=False,\r\n",
      "            epsilon=1e-6,\r\n",
      "            conditional=(z is not None),\r\n",
      "            hidden_units=self.layer_norm_conds[1],\r\n",
      "            hidden_activation=self.layer_norm_conds[2],\r\n",
      "            hidden_initializer=self.initializer,\r\n",
      "            name='%s-Norm' % self_attention_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=[x, x, x, attention_mask, position_bias[0]],\r\n",
      "            layer=MultiHeadAttention,\r\n",
      "            arguments={\r\n",
      "                'a_mask': True,\r\n",
      "                'p_bias': 't5_relative'\r\n",
      "            },\r\n",
      "            heads=self.num_attention_heads,\r\n",
      "            head_size=self.attention_head_size,\r\n",
      "            key_size=self.attention_key_size,\r\n",
      "            use_bias=False,\r\n",
      "            attention_scale=False,\r\n",
      "            kernel_initializer=self.initializer,\r\n",
      "            name=self_attention_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Dropout,\r\n",
      "            rate=self.dropout_rate,\r\n",
      "            name='%s-Dropout' % self_attention_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=[xi, x], layer=Add, name='%s-Add' % self_attention_name\r\n",
      "        )\r\n",
      "\r\n",
      "        # Cross Attention\r\n",
      "        xi = x\r\n",
      "        x = self.apply(\r\n",
      "            inputs=self.simplify([x, z]),\r\n",
      "            layer=LayerNormalization,\r\n",
      "            center=False,\r\n",
      "            epsilon=1e-6,\r\n",
      "            conditional=(z is not None),\r\n",
      "            hidden_units=self.layer_norm_conds[1],\r\n",
      "            hidden_activation=self.layer_norm_conds[2],\r\n",
      "            hidden_initializer=self.initializer,\r\n",
      "            name='%s-Norm' % cross_attention_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=[x, c, c, position_bias[1]],\r\n",
      "            layer=MultiHeadAttention,\r\n",
      "            arguments={\r\n",
      "                'a_mask': None,\r\n",
      "                'p_bias': 't5_relative'\r\n",
      "            },\r\n",
      "            heads=self.num_attention_heads,\r\n",
      "            head_size=self.attention_head_size,\r\n",
      "            key_size=self.attention_key_size,\r\n",
      "            use_bias=False,\r\n",
      "            attention_scale=False,\r\n",
      "            kernel_initializer=self.initializer,\r\n",
      "            name=cross_attention_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Dropout,\r\n",
      "            rate=self.dropout_rate,\r\n",
      "            name='%s-Dropout' % cross_attention_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=[xi, x], layer=Add, name='%s-Add' % cross_attention_name\r\n",
      "        )\r\n",
      "\r\n",
      "        # Feed Forward\r\n",
      "        xi = x\r\n",
      "        x = self.apply(\r\n",
      "            inputs=self.simplify([x, z]),\r\n",
      "            layer=LayerNormalization,\r\n",
      "            center=False,\r\n",
      "            epsilon=1e-6,\r\n",
      "            conditional=(z is not None),\r\n",
      "            hidden_units=self.layer_norm_conds[1],\r\n",
      "            hidden_activation=self.layer_norm_conds[2],\r\n",
      "            hidden_initializer=self.initializer,\r\n",
      "            name='%s-Norm' % feed_forward_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=FeedForward,\r\n",
      "            units=self.intermediate_size,\r\n",
      "            activation=self.hidden_act,\r\n",
      "            use_bias=False,\r\n",
      "            kernel_initializer=self.initializer,\r\n",
      "            name=feed_forward_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Dropout,\r\n",
      "            rate=self.dropout_rate,\r\n",
      "            name='%s-Dropout' % feed_forward_name\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=[xi, x], layer=Add, name='%s-Add' % feed_forward_name\r\n",
      "        )\r\n",
      "\r\n",
      "        return [c, x]\r\n",
      "\r\n",
      "    def apply_final_layers(self, inputs):\r\n",
      "        \"\"\"剩余部分\r\n",
      "        \"\"\"\r\n",
      "        c, x = inputs\r\n",
      "        z = self.layer_norm_conds[0]\r\n",
      "\r\n",
      "        x = self.apply(\r\n",
      "            inputs=self.simplify([x, z]),\r\n",
      "            layer=LayerNormalization,\r\n",
      "            center=False,\r\n",
      "            epsilon=1e-6,\r\n",
      "            conditional=(z is not None),\r\n",
      "            hidden_units=self.layer_norm_conds[1],\r\n",
      "            hidden_activation=self.layer_norm_conds[2],\r\n",
      "            hidden_initializer=self.initializer,\r\n",
      "            name='Decoder-Output-Norm'\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Dropout,\r\n",
      "            rate=self.dropout_rate,\r\n",
      "            name='Decoder-Output-Dropout'\r\n",
      "        )\r\n",
      "        x = self.apply(\r\n",
      "            inputs=x,\r\n",
      "            layer=Lambda,\r\n",
      "            function=lambda x: x / np.sqrt(self.hidden_size),\r\n",
      "            name='Decoder-Output-Scale'\r\n",
      "        )\r\n",
      "\r\n",
      "        if self.with_lm:\r\n",
      "            # 预测token概率部分\r\n",
      "            if self.embedding_size != self.hidden_size:\r\n",
      "                x = self.apply(\r\n",
      "                    inputs=x,\r\n",
      "                    layer=Dense,\r\n",
      "                    units=self.embedding_size,\r\n",
      "                    kernel_initializer=self.initializer,\r\n",
      "                    name='Decoder-Output-Mapping'\r\n",
      "                )\r\n",
      "            x = self.apply(\r\n",
      "                inputs=x,\r\n",
      "                layer=Embedding,\r\n",
      "                arguments={'mode': 'dense'},\r\n",
      "                name='Embedding-Token'\r\n",
      "            )\r\n",
      "            lm_activation = 'softmax' if self.with_lm is True else self.with_lm\r\n",
      "            x = self.apply(\r\n",
      "                inputs=x,\r\n",
      "                layer=Activation,\r\n",
      "                activation=lm_activation,\r\n",
      "                name='Dencoder-Output-LM-Activation'\r\n",
      "            )\r\n",
      "\r\n",
      "        return x\r\n",
      "\r\n",
      "    def compute_attention_mask(self, inputs=None):\r\n",
      "        \"\"\"添加下三角形式的attention mask\r\n",
      "        \"\"\"\r\n",
      "        if self.attention_mask is None:\r\n",
      "\r\n",
      "            def lm_mask(s):\r\n",
      "                seq_len = K.shape(s)[1]\r\n",
      "                idxs = K.arange(0, seq_len)\r\n",
      "                mask = idxs[None, :] <= idxs[:, None]\r\n",
      "                mask = K.cast(mask, K.floatx())\r\n",
      "                return mask[None, None]\r\n",
      "\r\n",
      "            self.attention_mask = self.apply(\r\n",
      "                inputs=self.inputs[1],\r\n",
      "                layer=Lambda,\r\n",
      "                function=lm_mask,\r\n",
      "                name='Attention-LM-Mask'\r\n",
      "            )\r\n",
      "\r\n",
      "        return self.attention_mask\r\n",
      "\r\n",
      "    def compute_position_bias(self, inputs=None):\r\n",
      "        \"\"\"T5相对位置编码\r\n",
      "        \"\"\"\r\n",
      "        if self.position_bias is None:\r\n",
      "\r\n",
      "            x, c = inputs\r\n",
      "            p1 = self.apply(\r\n",
      "                inputs=[x, x],\r\n",
      "                layer=RelativePositionEmbeddingT5,\r\n",
      "                input_dim=32,\r\n",
      "                output_dim=self.num_attention_heads,\r\n",
      "                bidirectional=False,\r\n",
      "                embeddings_initializer=self.initializer,\r\n",
      "                name='Decoder-Embedding-Relative-Position'\r\n",
      "            )\r\n",
      "            p2 = self.apply(\r\n",
      "                inputs=[x, c],\r\n",
      "                layer=RelativePositionEmbeddingT5,\r\n",
      "                input_dim=32,\r\n",
      "                output_dim=self.num_attention_heads,\r\n",
      "                bidirectional=False,\r\n",
      "                embeddings_initializer=self.initializer,\r\n",
      "                name='Decoder-Embedding-Relative-Position'\r\n",
      "            )\r\n",
      "            self.position_bias = (p1, p2)\r\n",
      "\r\n",
      "        return self.position_bias\r\n",
      "\r\n",
      "\r\n",
      "class T5(T5_Base):\r\n",
      "    \"\"\"Google的T5模型（Encoder-Decoder）\r\n",
      "    \"\"\"\r\n",
      "    def __init__(self, **kwargs):\r\n",
      "        super(T5, self).__init__(**kwargs)\r\n",
      "        kwargs['layers'] = self.layers\r\n",
      "        e_name, d_name = 'Encoder', 'Decoder'\r\n",
      "        if 'name' in kwargs:\r\n",
      "            e_name = '%s_%s' % (kwargs['name'], e_name)\r\n",
      "            d_name = '%s_%s' % (kwargs['name'], d_name)\r\n",
      "            del kwargs['name']  # 防止重复传参\r\n",
      "        self._encoder = T5_Encoder(name=e_name, **kwargs)\r\n",
      "        self._decoder = T5_Decoder(name=d_name, **kwargs)\r\n",
      "\r\n",
      "    def build(self, **kwargs):\r\n",
      "        \"\"\"同时构建Encoder和Decoder\r\n",
      "        \"\"\"\r\n",
      "        self._encoder.build(**kwargs)\r\n",
      "        self._decoder.build(**kwargs)\r\n",
      "        self.encoder = self._encoder.model\r\n",
      "        self.decoder = self._decoder.model\r\n",
      "        self.inputs = self.encoder.inputs + self.decoder.inputs[1:]\r\n",
      "        self.outputs = self.decoder(\r\n",
      "            self.encoder.outputs + self.decoder.inputs[1:]\r\n",
      "        )\r\n",
      "        self.model = Model(self.inputs, self.outputs)\r\n",
      "\r\n",
      "\r\n",
      "def extend_with_language_model(BaseModel):\r\n",
      "    \"\"\"添加下三角的Attention Mask（语言模型用）\r\n",
      "    \"\"\"\r\n",
      "    class LanguageModel(BaseModel):\r\n",
      "        \"\"\"带下三角Attention Mask的派生模型\r\n",
      "        \"\"\"\r\n",
      "        def __init__(self, *args, **kwargs):\r\n",
      "            super(LanguageModel, self).__init__(*args, **kwargs)\r\n",
      "            self.with_mlm = self.with_mlm or True\r\n",
      "\r\n",
      "        def compute_attention_mask(self, inputs=None):\r\n",
      "            \"\"\"重载此函数即可\r\n",
      "            \"\"\"\r\n",
      "            if self.attention_mask is None:\r\n",
      "\r\n",
      "                def lm_mask(s):\r\n",
      "                    seq_len = K.shape(s)[1]\r\n",
      "                    idxs = K.arange(0, seq_len)\r\n",
      "                    mask = idxs[None, :] <= idxs[:, None]\r\n",
      "                    mask = K.cast(mask, K.floatx())\r\n",
      "                    return mask[None, None]\r\n",
      "\r\n",
      "                self.attention_mask = self.apply(\r\n",
      "                    inputs=self.inputs[1],\r\n",
      "                    layer=Lambda,\r\n",
      "                    function=lm_mask,\r\n",
      "                    name='Attention-LM-Mask'\r\n",
      "                )\r\n",
      "\r\n",
      "            return self.attention_mask\r\n",
      "\r\n",
      "    return LanguageModel\r\n",
      "\r\n",
      "\r\n",
      "def extend_with_unified_language_model(BaseModel):\r\n",
      "    \"\"\"添加UniLM的Attention Mask（UnifiedLanguageModel用）\r\n",
      "    \"\"\"\r\n",
      "    class UnifiedLanguageModel(BaseModel):\r\n",
      "        \"\"\"带UniLM的Attention Mask的派生模型\r\n",
      "        UniLM: https://arxiv.org/abs/1905.03197\r\n",
      "        \"\"\"\r\n",
      "        def __init__(self, *args, **kwargs):\r\n",
      "            super(UnifiedLanguageModel, self).__init__(*args, **kwargs)\r\n",
      "            self.with_mlm = self.with_mlm or True\r\n",
      "\r\n",
      "        def compute_attention_mask(self, inputs=None):\r\n",
      "            \"\"\"重载此函数即可\r\n",
      "            \"\"\"\r\n",
      "            if self.attention_mask is None:\r\n",
      "\r\n",
      "                def unilm_mask(s):\r\n",
      "                    idxs = K.cumsum(s, axis=1)\r\n",
      "                    mask = idxs[:, None, :] <= idxs[:, :, None]\r\n",
      "                    mask = K.cast(mask, K.floatx())\r\n",
      "                    return mask[:, None]\r\n",
      "\r\n",
      "                self.attention_mask = self.apply(\r\n",
      "                    inputs=self.inputs[1],\r\n",
      "                    layer=Lambda,\r\n",
      "                    function=unilm_mask,\r\n",
      "                    name='Attention-UniLM-Mask'\r\n",
      "                )\r\n",
      "\r\n",
      "            return self.attention_mask\r\n",
      "\r\n",
      "    return UnifiedLanguageModel\r\n",
      "\r\n",
      "\r\n",
      "def build_transformer_model(\r\n",
      "    config_path=None,\r\n",
      "    checkpoint_path=None,\r\n",
      "    model='bert',\r\n",
      "    application='encoder',\r\n",
      "    return_keras_model=True,\r\n",
      "    **kwargs\r\n",
      "):\r\n",
      "    \"\"\"根据配置文件构建模型，可选加载checkpoint权重\r\n",
      "    \"\"\"\r\n",
      "    configs = {}\r\n",
      "    if config_path is not None:\r\n",
      "        configs.update(json.load(open(config_path)))\r\n",
      "    configs.update(kwargs)\r\n",
      "    if 'max_position' not in configs:\r\n",
      "        configs['max_position'] = configs.get('max_position_embeddings')\r\n",
      "    if 'dropout_rate' not in configs:\r\n",
      "        configs['dropout_rate'] = configs.get('hidden_dropout_prob')\r\n",
      "\r\n",
      "    model, application = model.lower(), application.lower()\r\n",
      "\r\n",
      "    models = {\r\n",
      "        'bert': BERT,\r\n",
      "        'albert': ALBERT,\r\n",
      "        'albert_unshared': ALBERT_Unshared,\r\n",
      "        'nezha': NEZHA,\r\n",
      "        'electra': ELECTRA,\r\n",
      "        'gpt2_ml': GPT2_ML,\r\n",
      "        't5': T5,\r\n",
      "    }\r\n",
      "    MODEL = models[model]\r\n",
      "\r\n",
      "    if model != 't5':\r\n",
      "        if application == 'lm':\r\n",
      "            MODEL = extend_with_language_model(MODEL)\r\n",
      "        elif application == 'unilm':\r\n",
      "            MODEL = extend_with_unified_language_model(MODEL)\r\n",
      "\r\n",
      "    transformer = MODEL(**configs)\r\n",
      "    transformer.build(**configs)\r\n",
      "\r\n",
      "    if checkpoint_path is not None:\r\n",
      "        transformer.load_weights_from_checkpoint(checkpoint_path)\r\n",
      "\r\n",
      "    if return_keras_model:\r\n",
      "        return transformer.model\r\n",
      "    else:\r\n",
      "        return transformer\r\n"
     ]
    }
   ],
   "source": [
    "!cat ~/miniconda3/envs/pycookly2/lib/python3.6/site-packages/bert4keras/models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
