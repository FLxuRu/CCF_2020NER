{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bert4keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert4keras.models import build_transformer_model\n",
    "from bert4keras.tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Model\n",
    "from bert4keras.layers import ConditionalRandomField\n",
    "from bert4keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_transformer_model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bert4keras.models import build_transformer_model\n",
    "# from bert4keras.tokenizers import Tokenizer\n",
    "# import numpy as np\n",
    "\n",
    "# config_path = '/root/kg/bert/chinese_L-12_H-768_A-12/bert_config.json'\n",
    "# checkpoint_path = '/root/kg/bert/chinese_L-12_H-768_A-12/bert_model.ckpt'\n",
    "# dict_path = '/root/kg/bert/chinese_L-12_H-768_A-12/vocab.txt'\n",
    "\n",
    "# tokenizer = Tokenizer(dict_path, do_lower_case=True)  # 建立分词器\n",
    "# model = build_transformer_model(config_path, checkpoint_path)  # 建立模型，加载权重\n",
    "\n",
    "# # 编码测试\n",
    "# token_ids, segment_ids = tokenizer.encode(u'语言模型')\n",
    "\n",
    "# print('\\n ===== predicting =====\\n')\n",
    "# print(model.predict([np.array([token_ids]), np.array([segment_ids])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category\tPrivacy\n",
    "# 0\tQQ\t527622886 229215757 1827967769 130238288 53344...\n",
    "# 1\taddress\t西直门 伊朗 红领巾公园 建国门街道 海豐 廣州 智利 深圳 九寨县城 汶川 西西里岛 中国...\n",
    "# 2\tbook\t《不停的陀螺》 神雕侠侣》 《鹿鼎记》 后汉纪》 《续汉书》 书剑恩仇录 EGM 《洛丽塔》...\n",
    "# 3\tcompany\tCW 索尼 佳士得 必和必拓 必和必拓 暴雪 环球网 育碧 渣打 贝发集团 中诚信国际 美林...\n",
    "# 4\temail\tsaent@netvigator.com aben@alegendstar.com 3483...\n",
    "# 5\tgame\t卡米洛特的黑暗时代》 《永恒的任务》 (EverQuest) 《特种部队》 巫妖王之怒》 《...\n",
    "# 6\tgovernment\t伊朗外交部 民航昌都站 美国导弹防御局 欧佩克成员国 FTC 美陆军 日军 国军 八路军 伪...\n",
    "# 7\tmobile\t010-6362212113910492247 18857150858 00852-6901...\n",
    "# 8\tmovie\t《加勒比海盗1》 《雪青马》 《花园街五号》 《吐鲁番情歌》 圣诞颂歌》 （achristm...\n",
    "# 9\tname\t崔新魁 金庸 英黛安娜 梅赫曼帕拉斯特 孙慧祥先生 孙慧祥 邹力强 胡尔西德.吐尔地 黄刚 ...\n",
    "# 10\torganization\t曼联队 桑德兰队 英超联赛 意甲 切沃 民謠乐队 博洛尼亚 那不勒斯 民航西藏区局扶贫办 福...\n",
    "# 11\tposition\tCEO 专家 古典大师 发言人 北京人 八卦掌高级教练 八卦掌高级教练 开发商 国家一极演员...\n",
    "# 12\tscene\t邙山 怪兽公园 魔语怪兽公园 九寨沟 岷江源 川主寺 黄龙松潘古城 牟尼沟 叠溪海子 松坪沟...\n",
    "# 13\tvx\tzhuzhiyi91510 1827967769 rachel-213 zhangzhizh..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_transformer_model(\n",
    "#     config_path=None,  # 模型的配置文件（对应的文件为json格式）\n",
    "#     checkpoint_path=None,  # 模型的预训练权重（tensorflow的ckpt格式）\n",
    "#     model='bert',  # 模型的类型（bert、albert、albert_unshared、nezha、electra、gpt2_ml、t5）\n",
    "#     application='encoder',  # 模型的用途（encoder、lm、unilm）\n",
    "#     return_keras_model=True,  # 返回Keras模型，还是返回bert4keras的模型类\n",
    "#     **kwargs  # 其他传递参数\n",
    "# ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_roberta=build_transformer_model(\n",
    "    vocab_size=30000,\n",
    "    hidden_size=206,#SEQ长度\n",
    "    num_hidden_layers=3,#transformers个数\n",
    "    num_attention_heads=3,#一个transformers中的MultiHeadSelfAttent个数\n",
    "    intermediate_size=64,#Transformer-1-FeedForward 层即fc层的大小\n",
    "    hidden_act=\"gelu\",\n",
    "    model=\"roberta\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input-Token (InputLayer)        (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Segment (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token (Embedding)     (None, None, 206)    6180000     Input-Token[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Segment (Embedding)   (None, None, 206)    412         Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token-Segment (Add)   (None, None, 206)    0           Embedding-Token[0][0]            \n",
      "                                                                 Embedding-Segment[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Position (PositionEmb (None, None, 206)    105472      Embedding-Token-Segment[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Norm (LayerNormalizat (None, None, 206)    412         Embedding-Position[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 206)    168914      Embedding-Norm[0][0]             \n",
      "                                                                 Embedding-Norm[0][0]             \n",
      "                                                                 Embedding-Norm[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 206)    0           Embedding-Norm[0][0]             \n",
      "                                                                 Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 206)    412         Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward (Feed (None, None, 206)    26638       Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward-Add ( (None, None, 206)    0           Transformer-0-MultiHeadSelfAttent\n",
      "                                                                 Transformer-0-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward-Norm  (None, None, 206)    412         Transformer-0-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 206)    168914      Transformer-0-FeedForward-Norm[0]\n",
      "                                                                 Transformer-0-FeedForward-Norm[0]\n",
      "                                                                 Transformer-0-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 206)    0           Transformer-0-FeedForward-Norm[0]\n",
      "                                                                 Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 206)    412         Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward (Feed (None, None, 206)    26638       Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward-Add ( (None, None, 206)    0           Transformer-1-MultiHeadSelfAttent\n",
      "                                                                 Transformer-1-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward-Norm  (None, None, 206)    412         Transformer-1-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-MultiHeadSelfAtte (None, None, 206)    168914      Transformer-1-FeedForward-Norm[0]\n",
      "                                                                 Transformer-1-FeedForward-Norm[0]\n",
      "                                                                 Transformer-1-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-MultiHeadSelfAtte (None, None, 206)    0           Transformer-1-FeedForward-Norm[0]\n",
      "                                                                 Transformer-2-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-MultiHeadSelfAtte (None, None, 206)    412         Transformer-2-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-FeedForward (Feed (None, None, 206)    26638       Transformer-2-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-FeedForward-Add ( (None, None, 206)    0           Transformer-2-MultiHeadSelfAttent\n",
      "                                                                 Transformer-2-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-FeedForward-Norm  (None, None, 206)    412         Transformer-2-FeedForward-Add[0][\n",
      "==================================================================================================\n",
      "Total params: 6,875,424\n",
      "Trainable params: 6,875,424\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_roberta.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_roberta = model_roberta.get_layer('Transformer-2-FeedForward-Norm').output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_218/add:0' shape=(None, None, 6) dtype=float32>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_fin = Dense(6)(output_roberta)\n",
    "output_fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_16\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input-Token (InputLayer)        (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Segment (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token (Embedding)     (None, None, 128)    3840000     Input-Token[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Segment (Embedding)   (None, None, 128)    256         Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token-Segment (Add)   (None, None, 128)    0           Embedding-Token[0][0]            \n",
      "                                                                 Embedding-Segment[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Position (PositionEmb (None, None, 128)    65536       Embedding-Token-Segment[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Norm (LayerNormalizat (None, None, 128)    256         Embedding-Position[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 128)    66048       Embedding-Norm[0][0]             \n",
      "                                                                 Embedding-Norm[0][0]             \n",
      "                                                                 Embedding-Norm[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 128)    0           Embedding-Norm[0][0]             \n",
      "                                                                 Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 128)    256         Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward (Feed (None, None, 128)    33024       Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward-Add ( (None, None, 128)    0           Transformer-0-MultiHeadSelfAttent\n",
      "                                                                 Transformer-0-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward-Norm  (None, None, 128)    256         Transformer-0-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 128)    66048       Transformer-0-FeedForward-Norm[0]\n",
      "                                                                 Transformer-0-FeedForward-Norm[0]\n",
      "                                                                 Transformer-0-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 128)    0           Transformer-0-FeedForward-Norm[0]\n",
      "                                                                 Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 128)    256         Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward (Feed (None, None, 128)    33024       Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward-Add ( (None, None, 128)    0           Transformer-1-MultiHeadSelfAttent\n",
      "                                                                 Transformer-1-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward-Norm  (None, None, 128)    256         Transformer-1-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-MultiHeadSelfAtte (None, None, 128)    66048       Transformer-1-FeedForward-Norm[0]\n",
      "                                                                 Transformer-1-FeedForward-Norm[0]\n",
      "                                                                 Transformer-1-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-MultiHeadSelfAtte (None, None, 128)    0           Transformer-1-FeedForward-Norm[0]\n",
      "                                                                 Transformer-2-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-MultiHeadSelfAtte (None, None, 128)    256         Transformer-2-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-FeedForward (Feed (None, None, 128)    33024       Transformer-2-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-FeedForward-Add ( (None, None, 128)    0           Transformer-2-MultiHeadSelfAttent\n",
      "                                                                 Transformer-2-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-FeedForward-Norm  (None, None, 128)    256         Transformer-2-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "dense_274 (Dense)               (None, None, 12)     1548        Transformer-2-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "conditional_random_field_1 (Con (None, None, 12)     144         dense_274[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,206,492\n",
      "Trainable params: 4,206,492\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert_layers = 3\n",
    "num_labels = 12\n",
    "lr_multiplier=1000\n",
    "model = build_transformer_model(\n",
    "    vocab_size=30000,\n",
    "    hidden_size=128,\n",
    "    num_hidden_layers=3,\n",
    "    num_attention_heads=2,\n",
    "    intermediate_size=128,\n",
    "    hidden_act=\"gelu\"\n",
    "    )\n",
    "\n",
    "output_layer = 'Transformer-%s-FeedForward-Norm' % (bert_layers - 1)\n",
    "output = model.get_layer(output_layer).output\n",
    "output = Dense(num_labels)(output)\n",
    "CRF = ConditionalRandomField(lr_multiplier=lr_multiplier)\n",
    "output = CRF(output)\n",
    "\n",
    "model = Model(model.input, output)\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    loss=CRF.sparse_loss,\n",
    "    optimizer=Adam(0.01),\n",
    "    metrics=[CRF.sparse_accuracy]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.input?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 改进"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "begin: 0000000010000000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "end:   0000000000100000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cat:   0000000033300000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = build_transformer_model(\n",
    "    vocab_size=30000,\n",
    "    hidden_size=128,\n",
    "    num_hidden_layers=3,\n",
    "    num_attention_heads=2,\n",
    "    intermediate_size=128,\n",
    "    hidden_act=\"gelu\"\n",
    "    )\n",
    "\n",
    "output_layer = 'Transformer-%s-FeedForward-Norm' % (bert_layers - 1)\n",
    "output = model2.get_layer(output_layer).output\n",
    "#两个不同学习率的CRF\n",
    "output1 = Dense(10)(output)\n",
    "CRF1 = ConditionalRandomField(lr_multiplier=100)\n",
    "output1 = CRF1(output1)\n",
    "\n",
    "output2 = Dense(9)(output)\n",
    "CRF2 = ConditionalRandomField(lr_multiplier=1)\n",
    "output2 = CRF2(output2)\n",
    "model2 = Model(model2.input, [output1,output2])\n",
    "\n",
    "model2.compile(\n",
    "    loss=[CRF1.sparse_loss,CRF2.sparse_loss],\n",
    "    optimizer=Adam(0.01),\n",
    "    metrics=[CRF1.sparse_accuracy,CRF2.sparse_accuracy]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_22\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input-Token (InputLayer)        (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Segment (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token (Embedding)     (None, None, 128)    3840000     Input-Token[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Segment (Embedding)   (None, None, 128)    256         Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token-Segment (Add)   (None, None, 128)    0           Embedding-Token[0][0]            \n",
      "                                                                 Embedding-Segment[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Position (PositionEmb (None, None, 128)    65536       Embedding-Token-Segment[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Norm (LayerNormalizat (None, None, 128)    256         Embedding-Position[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 128)    66048       Embedding-Norm[0][0]             \n",
      "                                                                 Embedding-Norm[0][0]             \n",
      "                                                                 Embedding-Norm[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 128)    0           Embedding-Norm[0][0]             \n",
      "                                                                 Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 128)    256         Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward (Feed (None, None, 128)    33024       Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward-Add ( (None, None, 128)    0           Transformer-0-MultiHeadSelfAttent\n",
      "                                                                 Transformer-0-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward-Norm  (None, None, 128)    256         Transformer-0-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 128)    66048       Transformer-0-FeedForward-Norm[0]\n",
      "                                                                 Transformer-0-FeedForward-Norm[0]\n",
      "                                                                 Transformer-0-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 128)    0           Transformer-0-FeedForward-Norm[0]\n",
      "                                                                 Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 128)    256         Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward (Feed (None, None, 128)    33024       Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward-Add ( (None, None, 128)    0           Transformer-1-MultiHeadSelfAttent\n",
      "                                                                 Transformer-1-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward-Norm  (None, None, 128)    256         Transformer-1-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-MultiHeadSelfAtte (None, None, 128)    66048       Transformer-1-FeedForward-Norm[0]\n",
      "                                                                 Transformer-1-FeedForward-Norm[0]\n",
      "                                                                 Transformer-1-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-MultiHeadSelfAtte (None, None, 128)    0           Transformer-1-FeedForward-Norm[0]\n",
      "                                                                 Transformer-2-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-MultiHeadSelfAtte (None, None, 128)    256         Transformer-2-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-FeedForward (Feed (None, None, 128)    33024       Transformer-2-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-FeedForward-Add ( (None, None, 128)    0           Transformer-2-MultiHeadSelfAttent\n",
      "                                                                 Transformer-2-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-FeedForward-Norm  (None, None, 128)    256         Transformer-2-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "dense_333 (Dense)               (None, None, 10)     1290        Transformer-2-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "dense_334 (Dense)               (None, None, 9)      1161        Transformer-2-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "conditional_random_field_6 (Con (None, None, 10)     100         dense_333[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conditional_random_field_7 (Con (None, None, 9)      81          dense_334[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,207,432\n",
      "Trainable params: 4,207,432\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRF2.sparse_loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 改进三"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = build_transformer_model(\n",
    "    vocab_size=30000,\n",
    "    hidden_size=128,\n",
    "    num_hidden_layers=3,\n",
    "    num_attention_heads=2,\n",
    "    intermediate_size=128,\n",
    "    hidden_act=\"gelu\"\n",
    "    )\n",
    "\n",
    "output_layer = 'Transformer-%s-FeedForward-Norm' % (bert_layers - 1)\n",
    "output = model3.get_layer(output_layer).output\n",
    "#CRF 和Dense softmax层融合\n",
    "output1 = Dense(10)(output)\n",
    "CRF1 = ConditionalRandomField(lr_multiplier=1)\n",
    "output1 = CRF1(output1)\n",
    "\n",
    "output2 = Dense(units=9,activation='softmax')(output)\n",
    "model3 = Model(model3.input, [output1,output2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input-Token (InputLayer)        (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Segment (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token (Embedding)     (None, None, 128)    3840000     Input-Token[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Segment (Embedding)   (None, None, 128)    256         Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token-Segment (Add)   (None, None, 128)    0           Embedding-Token[0][0]            \n",
      "                                                                 Embedding-Segment[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Position (PositionEmb (None, None, 128)    65536       Embedding-Token-Segment[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Norm (LayerNormalizat (None, None, 128)    256         Embedding-Position[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 128)    66048       Embedding-Norm[0][0]             \n",
      "                                                                 Embedding-Norm[0][0]             \n",
      "                                                                 Embedding-Norm[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 128)    0           Embedding-Norm[0][0]             \n",
      "                                                                 Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 128)    256         Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward (Feed (None, None, 128)    33024       Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward-Add ( (None, None, 128)    0           Transformer-0-MultiHeadSelfAttent\n",
      "                                                                 Transformer-0-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward-Norm  (None, None, 128)    256         Transformer-0-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 128)    66048       Transformer-0-FeedForward-Norm[0]\n",
      "                                                                 Transformer-0-FeedForward-Norm[0]\n",
      "                                                                 Transformer-0-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 128)    0           Transformer-0-FeedForward-Norm[0]\n",
      "                                                                 Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 128)    256         Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward (Feed (None, None, 128)    33024       Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward-Add ( (None, None, 128)    0           Transformer-1-MultiHeadSelfAttent\n",
      "                                                                 Transformer-1-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward-Norm  (None, None, 128)    256         Transformer-1-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-MultiHeadSelfAtte (None, None, 128)    66048       Transformer-1-FeedForward-Norm[0]\n",
      "                                                                 Transformer-1-FeedForward-Norm[0]\n",
      "                                                                 Transformer-1-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-MultiHeadSelfAtte (None, None, 128)    0           Transformer-1-FeedForward-Norm[0]\n",
      "                                                                 Transformer-2-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-MultiHeadSelfAtte (None, None, 128)    256         Transformer-2-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-FeedForward (Feed (None, None, 128)    33024       Transformer-2-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-FeedForward-Add ( (None, None, 128)    0           Transformer-2-MultiHeadSelfAttent\n",
      "                                                                 Transformer-2-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-FeedForward-Norm  (None, None, 128)    256         Transformer-2-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, None, 10)     1290        Transformer-2-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "conditional_random_field_1 (Con (None, None, 10)     100         dense_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_38 (Dense)                (None, None, 9)      1161        Transformer-2-FeedForward-Norm[0]\n",
      "==================================================================================================\n",
      "Total params: 4,207,351\n",
      "Trainable params: 4,207,351\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3.summary()\n",
    "model3.compile(\n",
    "    loss=[CRF1.sparse_loss,'sparse_categorical_crossentropy'],\n",
    "    optimizer=Adam(0.01)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 改进四"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Lambda\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat(dense_list):\n",
    "    dense = tf.concat(dense_list, axis=-1)\n",
    "    return dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = build_transformer_model(\n",
    "    vocab_size=30000,\n",
    "    hidden_size=128,\n",
    "    num_hidden_layers=3,\n",
    "    num_attention_heads=2,\n",
    "    intermediate_size=128,\n",
    "    hidden_act=\"gelu\"\n",
    "    )\n",
    "\n",
    "output_layer = 'Transformer-%s-FeedForward-Norm' % (bert_layers - 1)\n",
    "output = model4.get_layer(output_layer).output\n",
    "\n",
    "#两分支【bertoutput->CRF1_output】【 output,CRF1_output ->output2_crf】\n",
    "\n",
    "# 辅助的label\n",
    "output1 = Dense(10)(output)\n",
    "CRF1 = ConditionalRandomField(lr_multiplier=1)\n",
    "output1_crf = CRF1(output1)\n",
    "\n",
    "# 主要的label\n",
    "concat_output1_output = Lambda(concat)([output,output1])\n",
    "CRF2 = ConditionalRandomField(lr_multiplier=1)\n",
    "output2_crf = CRF2(concat_output1_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = Model(model4.input, [output1_crf,output2_crf])\n",
    "model4.compile(\n",
    "    loss=[CRF1.sparse_loss,CRF2.sparse_loss],\n",
    "    optimizer=Adam(0.01),\n",
    "    metrics=[CRF1.sparse_accuracy,CRF2.sparse_accuracy]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_25\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input-Token (InputLayer)        (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Segment (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token (Embedding)     (None, None, 128)    3840000     Input-Token[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Segment (Embedding)   (None, None, 128)    256         Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token-Segment (Add)   (None, None, 128)    0           Embedding-Token[0][0]            \n",
      "                                                                 Embedding-Segment[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Position (PositionEmb (None, None, 128)    65536       Embedding-Token-Segment[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Norm (LayerNormalizat (None, None, 128)    256         Embedding-Position[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 128)    66048       Embedding-Norm[0][0]             \n",
      "                                                                 Embedding-Norm[0][0]             \n",
      "                                                                 Embedding-Norm[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 128)    0           Embedding-Norm[0][0]             \n",
      "                                                                 Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 128)    256         Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward (Feed (None, None, 128)    33024       Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward-Add ( (None, None, 128)    0           Transformer-0-MultiHeadSelfAttent\n",
      "                                                                 Transformer-0-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward-Norm  (None, None, 128)    256         Transformer-0-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 128)    66048       Transformer-0-FeedForward-Norm[0]\n",
      "                                                                 Transformer-0-FeedForward-Norm[0]\n",
      "                                                                 Transformer-0-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 128)    0           Transformer-0-FeedForward-Norm[0]\n",
      "                                                                 Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 128)    256         Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward (Feed (None, None, 128)    33024       Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward-Add ( (None, None, 128)    0           Transformer-1-MultiHeadSelfAttent\n",
      "                                                                 Transformer-1-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward-Norm  (None, None, 128)    256         Transformer-1-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-MultiHeadSelfAtte (None, None, 128)    66048       Transformer-1-FeedForward-Norm[0]\n",
      "                                                                 Transformer-1-FeedForward-Norm[0]\n",
      "                                                                 Transformer-1-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-MultiHeadSelfAtte (None, None, 128)    0           Transformer-1-FeedForward-Norm[0]\n",
      "                                                                 Transformer-2-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-MultiHeadSelfAtte (None, None, 128)    256         Transformer-2-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-FeedForward (Feed (None, None, 128)    33024       Transformer-2-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-FeedForward-Add ( (None, None, 128)    0           Transformer-2-MultiHeadSelfAttent\n",
      "                                                                 Transformer-2-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-FeedForward-Norm  (None, None, 128)    256         Transformer-2-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "dense_372 (Dense)               (None, None, 10)     1290        Transformer-2-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None, 138)    0           Transformer-2-FeedForward-Norm[0]\n",
      "                                                                 dense_372[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conditional_random_field_9 (Con (None, None, 10)     100         dense_372[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conditional_random_field_10 (Co (None, None, 138)    19044       lambda_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 4,225,234\n",
      "Trainable params: 4,225,234\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
